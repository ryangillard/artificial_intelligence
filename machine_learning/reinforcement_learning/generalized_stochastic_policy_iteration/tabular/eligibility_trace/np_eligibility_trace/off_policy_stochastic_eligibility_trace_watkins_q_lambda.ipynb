{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)\n",
    "number_of_actions_per_non_terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_state_action_successor_states = np.ones(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "number_of_state_action_successor_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1],\n",
       "        [ 0],\n",
       "        [14],\n",
       "        [ 4]],\n",
       "\n",
       "       [[ 2],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [ 5]],\n",
       "\n",
       "       [[ 2],\n",
       "        [ 2],\n",
       "        [ 1],\n",
       "        [ 6]],\n",
       "\n",
       "       [[ 4],\n",
       "        [14],\n",
       "        [ 3],\n",
       "        [ 7]],\n",
       "\n",
       "       [[ 5],\n",
       "        [ 0],\n",
       "        [ 3],\n",
       "        [ 8]],\n",
       "\n",
       "       [[ 6],\n",
       "        [ 1],\n",
       "        [ 4],\n",
       "        [ 9]],\n",
       "\n",
       "       [[ 6],\n",
       "        [ 2],\n",
       "        [ 5],\n",
       "        [10]],\n",
       "\n",
       "       [[ 8],\n",
       "        [ 3],\n",
       "        [ 7],\n",
       "        [11]],\n",
       "\n",
       "       [[ 9],\n",
       "        [ 4],\n",
       "        [ 7],\n",
       "        [12]],\n",
       "\n",
       "       [[10],\n",
       "        [ 5],\n",
       "        [ 8],\n",
       "        [13]],\n",
       "\n",
       "       [[10],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [15]],\n",
       "\n",
       "       [[12],\n",
       "        [ 7],\n",
       "        [11],\n",
       "        [11]],\n",
       "\n",
       "       [[13],\n",
       "        [ 8],\n",
       "        [11],\n",
       "        [12]],\n",
       "\n",
       "       [[15],\n",
       "        [ 9],\n",
       "        [12],\n",
       "        [13]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_successor_state_indices = np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64)\n",
    "state_action_successor_state_indices = np.reshape(a = state_action_successor_state_indices, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_successor_state_transition_probabilities = np.ones(shape = [number_of_non_terminal_states, max_number_of_actions, 1], dtype = np.float64)\n",
    "state_action_successor_state_transition_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_successor_state_rewards = np.ones(shape = [number_of_non_terminal_states, max_number_of_actions, 1], dtype = np.float64) * -1\n",
    "state_action_successor_state_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes\n",
    "number_of_episodes = 10000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 200\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.1\n",
    "# Set discounting factor gamma\n",
    "discounting_factor_gamma = 1.0\n",
    "# Set trace decay parameter lambda\n",
    "trace_decay_lambda = 0.9\n",
    "# Set the trace update type, 0 = accumulating, 1 = replacing\n",
    "trace_update_type = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_value_function = np.zeros(shape = [number_of_states, max_number_of_actions], dtype = np.float64)\n",
    "state_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.repeat(a = 1.0 / max_number_of_actions, repeats = number_of_non_terminal_states * max_number_of_actions)\n",
    "policy = np.reshape(a = policy, newshape = (number_of_non_terminal_states, max_number_of_actions))\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligibility_trace = np.zeros(shape = [number_of_states, max_number_of_actions], dtype = np.float64)\n",
    "eligibility_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(number_of_non_terminal_states, max_number_of_actions, state_action_value_function, epsilon, policy, eligibility_trace):\n",
    "    # Reset eligibility traces for new episode\n",
    "    eligibility_trace = np.zeros_like(a = eligibility_trace)\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state_index = np.random.randint(low = 0, high = number_of_non_terminal_states, dtype = np.int64) # randomly choose an initial state from all non-terminal states\n",
    "\n",
    "    # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "    policy = epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, initial_state_index, policy)\n",
    "\n",
    "    # Get initial action\n",
    "    initial_action_index = np.random.choice(a = max_number_of_actions, p = policy[initial_state_index, :])\n",
    "\n",
    "    return initial_state_index, initial_action_index, policy, eligibility_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy epsilon-greedily from the state-action-value function\n",
    "def epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy):\n",
    "    # Save max state-action value and find the number of actions that have the same max state-action value\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function[state_index, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if max_action_count == max_number_of_actions:\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remaining_apportioned_probability_per_action = epsilon / (max_number_of_actions - max_action_count)\n",
    "\n",
    "    policy[state_index, :] = np.where(state_action_value_function[state_index, :] == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, eligibility_trace, alpha, epsilon, discounting_factor_gamma, trace_decay_lambda, trace_update_type, maximum_episode_length, state_index, action_index):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Get reward\n",
    "        successor_state_transition_index = np.random.choice(a = number_of_state_action_successor_states[state_index, action_index], p = state_action_successor_state_transition_probabilities[state_index, action_index, :])\n",
    "\n",
    "        # Get reward from state and action\n",
    "        reward = state_action_successor_state_rewards[state_index, action_index, successor_state_transition_index]\n",
    "\n",
    "        # Get next state\n",
    "        next_state_index = state_action_successor_state_indices[state_index, action_index, successor_state_transition_index]\n",
    "\n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            # Calculate TD error delta\n",
    "            delta = reward - state_action_value_function[state_index, action_index]\n",
    "\n",
    "            # Update eligibility traces and state action value function with TD error\n",
    "            eligibility_trace, state_action_value_function = update_eligibility_trace_and_state_action_value_function(state_index, action_index, 0, 0, delta, number_of_non_terminal_states, max_number_of_actions, alpha, discounting_factor_gamma, trace_decay_lambda, trace_update_type, state_action_value_function, eligibility_trace)\n",
    "\n",
    "            break; # episode terminated since we ended up in a terminal state\n",
    "        else:\n",
    "            # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "            policy = epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, next_state_index, policy)\n",
    "\n",
    "            # Get next action\n",
    "            next_action_index = np.random.choice(a = max_number_of_actions, p = policy[next_state_index, :])\n",
    "    \n",
    "            # Get next action, max action of next state\n",
    "            max_action_value = np.max(a = state_action_value_function[next_state_index, :])\n",
    "            state_action_value_function_max_tie_stack = np.extract(\n",
    "                condition = state_action_value_function[next_state_index, :] == max_action_value, \n",
    "                arr = np.arange(max_number_of_actions))\n",
    "            max_action_count = state_action_value_function_max_tie_stack.size\n",
    "\n",
    "            if state_action_value_function[next_state_index, next_action_index] == max_action_value:\n",
    "                max_action_index = next_action_index\n",
    "            else:\n",
    "                max_action_index = state_action_value_function_max_tie_stack[np.random.randint(max_action_count)]\n",
    "            \n",
    "            # Calculate TD error delta\n",
    "            delta = reward + discounting_factor_gamma * state_action_value_function[next_state_index, next_action_index] - state_action_value_function[state_index, action_index]\n",
    "\n",
    "            # Update eligibility traces and state action value function with TD error\n",
    "            eligibility_trace, state_action_value_function = update_eligibility_trace_and_state_action_value_function(state_index, action_index, next_action_index, max_action_index, delta, number_of_non_terminal_states, max_number_of_actions, alpha, discounting_factor_gamma, trace_decay_lambda, trace_update_type, state_action_value_function, eligibility_trace)\n",
    "\n",
    "            # Update state and action to next state and action\n",
    "            state_index = next_state_index\n",
    "            action_index = next_action_index\n",
    "\n",
    "    return state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the eligibility trace and state-action-value function\n",
    "def update_eligibility_trace_and_state_action_value_function(state_index, action_index, next_action_index, max_action_index, delta, number_of_non_terminal_states, max_number_of_actions, alpha, discounting_factor_gamma, trace_decay_lambda, trace_update_type, state_action_value_function, eligibility_trace):\n",
    "    # Update eligibility trace\n",
    "    if trace_update_type == 1: # replacing\n",
    "        eligibility_trace[state_index, action_index] = 1.0\n",
    "    else: # accumulating or unknown\n",
    "        eligibility_trace[state_index, action_index] += 1.0\n",
    "\n",
    "    # Update state-action-value function\n",
    "    state_action_value_function += alpha * delta * eligibility_trace\n",
    "    if next_action_index == max_action_index:\n",
    "        eligibility_trace *= discounting_factor_gamma * trace_decay_lambda\n",
    "    else:\n",
    "        eligibility_trace = np.zeros_like(a = eligibility_trace)\n",
    "\n",
    "    return eligibility_trace, state_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_stochastic_eligibility_trace_watkins_q_lambda(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, eligibility_trace, alpha, epsilon, discounting_factor_gamma, maximum_episode_length):\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        # Initialize episode to get initial state and action\n",
    "        initial_state_index, initial_action_index, policy, eligibility_trace = initialize_epsiode(number_of_non_terminal_states, max_number_of_actions, state_action_value_function, epsilon, policy, eligibility_trace)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        state_action_value_function, policy = loop_through_episode(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, eligibility_trace, alpha, epsilon, discounting_factor_gamma, trace_decay_lambda, trace_update_type, maximum_episode_length, initial_state_index, initial_action_index)\n",
    "    \n",
    "    return state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3.19370048 -2.13162436 -1.         -3.38141896]\n",
      " [-4.38395052 -3.33476397 -2.38134827 -4.5229024 ]\n",
      " [-4.28703659 -4.30601635 -3.46509669 -3.60659776]\n",
      " [-3.19243428 -1.         -2.23270918 -3.35790288]\n",
      " [-4.623028   -2.46719813 -2.14596559 -4.2397604 ]\n",
      " [-3.57051791 -3.68744377 -3.63886548 -3.38296475]\n",
      " [-3.45319322 -4.42056987 -4.48584522 -2.08774738]\n",
      " [-4.1461218  -2.36655874 -3.1126213  -4.41436266]\n",
      " [-3.74436177 -3.64808791 -3.35591569 -3.79740881]\n",
      " [-2.63136986 -4.43561424 -4.67314619 -2.30242319]\n",
      " [-2.16179896 -3.64684068 -3.60480662 -1.        ]\n",
      " [-3.35979638 -3.55882338 -4.28853357 -4.44678675]\n",
      " [-2.11282607 -4.2518263  -4.11094703 -3.209986  ]\n",
      " [-1.         -3.35090441 -3.0432591  -2.07841672]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Final policy\n",
      "[[0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run on policy temporal difference sarsa\n",
    "state_action_value_function, policy = off_policy_stochastic_eligibility_trace_watkins_q_lambda(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, eligibility_trace, alpha, epsilon, discounting_factor_gamma, maximum_episode_length)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
