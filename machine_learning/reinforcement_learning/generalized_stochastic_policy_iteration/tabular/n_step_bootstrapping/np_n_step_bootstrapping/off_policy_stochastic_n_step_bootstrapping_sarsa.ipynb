{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-Step Bootstrapping: Off-policy Sarsa, Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_states():\n",
    "    \"\"\"Creates environment states.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_terminal_states: int, number of terminal states.\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    num_terminal_states = 2\n",
    "    num_non_terminal_states = num_states - num_terminal_states\n",
    "\n",
    "    return num_states, num_terminal_states, num_non_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_actions(num_non_terminal_states):\n",
    "    \"\"\"Creates environment actions.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "\n",
    "    Returns:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_terminal_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "    \"\"\"\n",
    "    max_num_actions = 4\n",
    "\n",
    "    num_actions_per_non_terminal_state = np.repeat(\n",
    "        a=max_num_actions, repeats=num_non_terminal_states)\n",
    "\n",
    "    return max_num_actions, num_actions_per_non_terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_successor_counts(num_states, max_num_actions):\n",
    "    \"\"\"Creates environment successor counts.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        num_sp: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "    \"\"\"\n",
    "    num_sp = np.repeat(\n",
    "        a=1, repeats=num_states * max_num_actions)\n",
    "\n",
    "    num_sp = np.reshape(\n",
    "        a=num_sp,\n",
    "        newshape=(num_states, max_num_actions))\n",
    "\n",
    "    return num_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_successor_arrays(\n",
    "        num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates environment successor arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    sp_idx = np.array(\n",
    "        object=[1, 0, 14, 4,\n",
    "                2, 1, 0, 5,\n",
    "                2, 2, 1, 6,\n",
    "                4, 14, 3, 7,\n",
    "                5, 0, 3, 8,\n",
    "                6, 1, 4, 9,\n",
    "                6, 2, 5, 10,\n",
    "                8, 3, 7, 11,\n",
    "                9, 4, 7, 12,\n",
    "                10, 5, 8, 13,\n",
    "                10, 6, 9, 15,\n",
    "                12, 7, 11, 11,\n",
    "                13, 8, 11, 12,\n",
    "                15, 9, 12, 13],\n",
    "        dtype=np.int64)\n",
    "\n",
    "    p = np.repeat(\n",
    "        a=1.0, repeats=num_non_terminal_states * max_num_actions * 1)\n",
    "\n",
    "    r = np.repeat(\n",
    "        a=-1.0, repeats=num_non_terminal_states * max_num_actions * 1)\n",
    "\n",
    "    sp_idx = np.reshape(\n",
    "        a=sp_idx,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "    p = np.reshape(\n",
    "        a=p,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "    r = np.reshape(\n",
    "        a=r,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "\n",
    "    return sp_idx, p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    \"\"\"Creates environment.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_terminal_states: int, number of terminal states.\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_terminal_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "        num_sp: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    (num_states,\n",
    "     num_terminal_states,\n",
    "     num_non_terminal_states) = create_environment_states()\n",
    "\n",
    "    (max_num_actions,\n",
    "     num_actions_per_non_terminal_state) = create_environment_actions(\n",
    "        num_non_terminal_states)\n",
    "\n",
    "    num_sp = create_environment_successor_counts(\n",
    "        num_states, max_num_actions)\n",
    "\n",
    "    (sp_idx,\n",
    "     p,\n",
    "     r) = create_environment_successor_arrays(\n",
    "        num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    return (num_states,\n",
    "            num_terminal_states,\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_actions_per_non_terminal_state,\n",
    "            num_sp,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        n_steps: int, number of timesteps within value update.\n",
    "        num_episodes: int, number of episodes to train over.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "    \"\"\"\n",
    "    n_steps = 4\n",
    "    num_episodes = 1000000\n",
    "    maximum_episode_length = 2000\n",
    "    alpha = 0.0001\n",
    "    gamma = 1.0\n",
    "\n",
    "    return n_steps, num_episodes, maximum_episode_length, alpha, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episode_log(maximum_episode_length):\n",
    "    \"\"\"Creates episode log.\n",
    "\n",
    "    Args:\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "\n",
    "    Returns:\n",
    "        episode_log: dict, dictionary of state, action, and reward timestep\n",
    "            history arrays for episode.\n",
    "    \"\"\"\n",
    "    episode_log = {\n",
    "        \"s_idx\": np.repeat(a=-1, repeats=maximum_episode_length),\n",
    "        \"a_idx\": np.repeat(a=-1, repeats=maximum_episode_length),\n",
    "        \"reward\": np.repeat(a=0.0, repeats=maximum_episode_length)\n",
    "    }\n",
    "\n",
    "    return episode_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_function_arrays(num_states, max_num_actions):\n",
    "    \"\"\"Creates value function arrays.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "    \"\"\"\n",
    "    q = np.repeat(a=0.0, repeats=num_states * max_num_actions)\n",
    "    q = np.reshape(a=q, newshape=(num_states, max_num_actions))\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        target_policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        behavior_policy: array[float], unlearned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    target_policy = np.repeat(\n",
    "        a=1.0 / max_num_actions,\n",
    "        repeats=num_non_terminal_states * max_num_actions)\n",
    "    target_policy = np.reshape(\n",
    "        a=target_policy, newshape=(num_non_terminal_states, max_num_actions))\n",
    "\n",
    "    behavior_policy = np.repeat(\n",
    "        a=1.0 / max_num_actions,\n",
    "        repeats=num_non_terminal_states * max_num_actions)\n",
    "    behavior_policy = np.reshape(\n",
    "        a=behavior_policy, newshape=(num_non_terminal_states, max_num_actions))\n",
    "\n",
    "    return target_policy, behavior_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_epsiode(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        behavior_policy,\n",
    "        episode_log):\n",
    "    \"\"\"Initializes epsiode with initial state and initial action.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        behavior_policy: array[float], unlearned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        episode_log: dict, dictionary of state, action, and reward timestep\n",
    "            history arrays for episode.\n",
    "    Returns:\n",
    "        episode_log: dict, dictionary of state, action, and reward timestep\n",
    "            history arrays for episode.\n",
    "    \"\"\"\n",
    "    # Randomly choose an initial state from all non-terminal states\n",
    "    episode_log[\"s_idx\"][0] = np.random.randint(\n",
    "        low=0, high=num_non_terminal_states, dtype=np.int64)\n",
    "\n",
    "    # Get initial action\n",
    "    episode_log[\"a_idx\"][0] = np.random.choice(\n",
    "        a=max_num_actions,\n",
    "        p=behavior_policy[episode_log[\"s_idx\"][0], :])\n",
    "\n",
    "    return episode_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_state_action_function(q, s_idx, policy):\n",
    "    \"\"\"Create greedy policy from state-action value function.\n",
    "\n",
    "    Args:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        s_idx: int, current state index.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "    \"\"\"\n",
    "    # Save max state-action value and find the number of actions that have the\n",
    "    # same max state-action value\n",
    "    max_action_value = np.max(a=q[s_idx, :])\n",
    "    max_action_count = np.count_nonzero(a=q[s_idx, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    max_policy_prob_per_action = 1.0 / max_action_count\n",
    "    policy[s_idx, :] = np.where(\n",
    "        q[s_idx, :] == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        0.0)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_episode(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_sp,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        q,\n",
    "        target_policy,\n",
    "        behavior_policy,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        maximum_episode_length,\n",
    "        episode_log,\n",
    "        n_steps):\n",
    "    \"\"\"Loops through episode to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_sp: array[int], number of successor states s' that can be reached\n",
    "            from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        target_policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        behavior_policy: array[float], unlearned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        episode_log: dict, dictionary of state, action, and reward timestep\n",
    "            history arrays for episode.\n",
    "        n_steps: int, number of timesteps within value update.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        target_policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    max_timestep = maximum_episode_length\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Spend a little memory to save computation time\n",
    "        t_mod_n_plus_1 = t % (n_steps + 1)\n",
    "        t_plus_1_mod_n_plus_1 = (t + 1) % (n_steps + 1)\n",
    "\n",
    "        s_idx = episode_log[\"s_idx\"][t_mod_n_plus_1]\n",
    "        a_idx = episode_log[\"a_idx\"][t_mod_n_plus_1]\n",
    "\n",
    "        if t < max_timestep:\n",
    "            # Get reward\n",
    "            sst_idx = np.random.choice(\n",
    "                a=num_sp[s_idx, a_idx],\n",
    "                p=p[s_idx, a_idx, :])\n",
    "\n",
    "            reward = r[s_idx, a_idx, sst_idx]\n",
    "            episode_log[\"reward\"][t_plus_1_mod_n_plus_1] = reward\n",
    "\n",
    "            # Get next state\n",
    "            next_s_idx = sp_idx[s_idx, a_idx, sst_idx]\n",
    "            episode_log[\"s_idx\"][t_plus_1_mod_n_plus_1] = next_s_idx\n",
    "\n",
    "            # Check to see if we actioned into a terminal state\n",
    "            if next_s_idx >= num_non_terminal_states:\n",
    "                max_timestep = t + 1\n",
    "            else:\n",
    "                # Get next action\n",
    "                episode_log[\"a_idx\"][t_plus_1_mod_n_plus_1] = np.random.choice(\n",
    "                    a=max_num_actions,\n",
    "                    p=behavior_policy[next_s_idx, :])\n",
    "\n",
    "        # tau is the time whose estimate is being updated\n",
    "        tau = t - n_steps + 1\n",
    "\n",
    "        if tau >= 0:\n",
    "            # Calculate importance sampling ratio due to using behavior policy\n",
    "            upper = np.min([tau + n_steps, max_timestep - 1]) + 1\n",
    "\n",
    "            ratio = []\n",
    "            for i in range(tau + 1, upper):\n",
    "                n_s_idx = episode_log[\"s_idx\"][i % (n_steps + 1)]\n",
    "                n_a_idx = episode_log[\"a_idx\"][i % (n_steps + 1)]\n",
    "\n",
    "                t_policy = target_policy[n_s_idx, n_a_idx]\n",
    "                b_policy = behavior_policy[n_s_idx, n_a_idx]\n",
    "\n",
    "                ratio.append(t_policy / b_policy)\n",
    "\n",
    "            importance_sampling_ratio = np.prod(a=ratio)\n",
    "\n",
    "            # Calculate expected return\n",
    "            divisor = n_steps + 1\n",
    "            upper = np.min([tau + n_steps, max_timestep]) + 1\n",
    "\n",
    "            expected_return = np.sum(\n",
    "                a=[gamma ** (i - tau - 1) * episode_log[\"reward\"][i % divisor]\n",
    "                   for i in range(tau + 1, upper)])\n",
    "\n",
    "            if tau + n_steps < max_timestep:\n",
    "                # Spend a little memory to save computation time\n",
    "                tau_plus_n_mod_n_plus_1 = (tau + n_steps) % (n_steps + 1)\n",
    "                tau_s_idx = episode_log[\"s_idx\"][tau_plus_n_mod_n_plus_1]\n",
    "                tau_a_idx = episode_log[\"a_idx\"][tau_plus_n_mod_n_plus_1]\n",
    "\n",
    "                expected_return += gamma ** n_steps * q[tau_s_idx, tau_a_idx]\n",
    "\n",
    "            # Spend a little memory to save computation time\n",
    "            tau_mod_n_plus_1 = tau % (n_steps + 1)\n",
    "            tau_s_idx = episode_log[\"s_idx\"][tau_mod_n_plus_1]\n",
    "            tau_a_idx = episode_log[\"a_idx\"][tau_mod_n_plus_1]\n",
    "\n",
    "            # Calculate state-action-function at tau timestep\n",
    "            delta = expected_return - q[tau_s_idx, tau_a_idx]\n",
    "            update = alpha * importance_sampling_ratio * delta\n",
    "            q[tau_s_idx, tau_a_idx] += update\n",
    "\n",
    "            # Choose policy for chosen state by epsilon-greedy choosing from\n",
    "            # the state-action-value function\n",
    "            target_policy = greedy_policy_from_state_action_function(\n",
    "                q, tau_s_idx, target_policy)\n",
    "\n",
    "        if tau == max_timestep - 1:\n",
    "            break  # break episode step loop, move on to next episode\n",
    "\n",
    "    return q, target_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_n_step_bootstrapping_sarsa(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_sp,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        q,\n",
    "        target_policy,\n",
    "        behavior_policy,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        num_episodes,\n",
    "        maximum_episode_length,\n",
    "        episode_log,\n",
    "        n_steps):\n",
    "    \"\"\"Loops through episodes to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_sp: array[int], number of successor states s' that can be reached\n",
    "            from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        target_policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        behavior_policy: array[float], unlearned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        num_episodes: int, number of episodes to train over.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        episode_log: dict, dictionary of state, action, and reward timestep\n",
    "            history arrays for episode.\n",
    "        n_steps: int, number of timesteps within value update.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        target_policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    for episode in range(0, num_episodes):\n",
    "        # Initialize episode to get initial state and action\n",
    "        episode_log = initialize_epsiode(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            behavior_policy,\n",
    "            episode_log)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        q, target_policy = loop_through_episode(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_sp,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r,\n",
    "            q,\n",
    "            target_policy,\n",
    "            behavior_policy,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            maximum_episode_length,\n",
    "            episode_log,\n",
    "            n_steps)\n",
    "\n",
    "    return q, target_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\"\"\"\n",
    "    (num_states,\n",
    "     num_terminal_states,\n",
    "     num_non_terminal_states,\n",
    "     max_num_actions,\n",
    "     num_actions_per_non_terminal_state,\n",
    "     num_sp,\n",
    "     sp_idx,\n",
    "     p,\n",
    "     r) = create_environment()\n",
    "\n",
    "    (n_steps,\n",
    "     num_episodes,\n",
    "     maximum_episode_length,\n",
    "     alpha,\n",
    "     gamma) = set_hyperparameters()\n",
    "\n",
    "    episode_log = create_episode_log(maximum_episode_length)\n",
    "\n",
    "    q = create_value_function_arrays(num_states, max_num_actions)\n",
    "\n",
    "    target_policy, behavior_policy = create_policy_arrays(\n",
    "        num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    # Print initial arrays\n",
    "    print(\"\\nInitial state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nInitial target policy\")\n",
    "    print(target_policy)\n",
    "\n",
    "    # Run off policy n-step bootstrapping sarsa\n",
    "    q, target_policy = off_policy_n_step_bootstrapping_sarsa(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_sp,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        q,\n",
    "        target_policy,\n",
    "        behavior_policy,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        num_episodes,\n",
    "        maximum_episode_length,\n",
    "        episode_log,\n",
    "        n_steps)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nFinal target policy\")\n",
    "    print(target_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial target policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-4. -3. -2. -4.]\n",
      " [-4. -4. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -4. -4. -2.]\n",
      " [-4. -2. -3. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -4. -4.]\n",
      " [-2. -4. -4. -3.]\n",
      " [-1. -3. -3. -2.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "Final target policy\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "run_algorithm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
