{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bandits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1957272 , -1.51936052,  0.40767963, -0.45513443, -0.48142763,\n",
       "       -1.24410407,  2.60391307, -1.93834966, -2.15886687, -1.15053664])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_bandit_mean_mean = 0.0\n",
    "global_bandit_mean_variance = 1.0\n",
    "\n",
    "bandit_mean = np.random.normal(loc = global_bandit_mean_mean, scale = np.sqrt(global_bandit_mean_variance), size = number_of_bandits)\n",
    "bandit_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_bandit_variance_mean = 1.0\n",
    "global_bandit_variance_variance = 0.0\n",
    "\n",
    "bandit_variance = np.random.normal(loc = global_bandit_variance_mean, scale = np.sqrt(global_bandit_variance_variance), size = number_of_bandits)\n",
    "bandit_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 201, 201, 201, 201, 201, 201, 201, 201, 201])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_stochastic_change_frequencies = np.repeat(a = 201, repeats = number_of_bandits)\n",
    "bandit_stochastic_change_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_stochastic_change_counter = np.zeros(shape = [number_of_bandits], dtype = np.int64)\n",
    "bandit_stochastic_change_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of iterations\n",
    "number_of_iterations = 2000\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Get average reward update type (sample-average, constant step-size)\n",
    "average_reward_update_type = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_preference = np.zeros(shape = [number_of_bandits], dtype = np.float64)\n",
    "action_preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.repeat(a = 1.0 / number_of_bandits, repeats = number_of_bandits)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through iterations and updates the policy\n",
    "def loop_through_iterations(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_preference, policy, alpha, average_reward_update_type):\n",
    "    average_reward = 0.0\n",
    "    # Loop through iterations until termination\n",
    "    for t in range(0, number_of_iterations):\n",
    "        # Choose policy by epsilon-greedy choosing from the action-value function\n",
    "        policy = update_policy_from_action_preference(number_of_bandits, action_preference, policy)\n",
    "\n",
    "        # Get action\n",
    "        action_index = np.random.choice(a = number_of_bandits, p = policy)\n",
    "\n",
    "        # Get reward from action\n",
    "        reward = np.random.normal(loc = bandit_mean[action_index], scale = np.sqrt(bandit_variance[action_index]))\n",
    "\n",
    "        # Update average reward\n",
    "        if average_reward_update_type == 0: # sample-average method\n",
    "            average_reward += 1.0 / (t + 1) * (reward - average_reward);\n",
    "        elif average_reward_update_type == 1: # constant step-size\n",
    "            average_reward += alpha * (reward - average_reward);\n",
    "            \n",
    "        # Update action preference\n",
    "        action_preference = np.where(\n",
    "            np.arange(number_of_bandits) == action_index, \n",
    "            action_preference + alpha * (reward - average_reward) * (1.0 - policy), \n",
    "            action_preference - alpha * (reward - average_reward) * policy)\n",
    "\n",
    "        # Mutate bandit statistics\n",
    "        for i in range(number_of_bandits):\n",
    "            if bandit_stochastic_change_frequencies[i] > 0:\n",
    "                bandit_stochastic_change_counter[i] += 1\n",
    "\n",
    "                if bandit_stochastic_change_counter[i] == bandit_stochastic_change_frequencies[i]:\n",
    "                    bandit_mean[i] = np.random.normal(loc = global_bandit_mean_mean, scale = np.sqrt(global_bandit_mean_variance))\n",
    "                    bandit_variance[i] = np.random.normal(loc = global_bandit_variance_mean, scale = np.sqrt(global_bandit_variance_variance))\n",
    "\n",
    "                    bandit_stochastic_change_counter[i] = 0\n",
    "\n",
    "    return bandit_mean, bandit_variance, action_preference, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates policy based on action preference\n",
    "def update_policy_from_action_preference(number_of_bandits, action_preference, policy):\n",
    "    # Calculate probabilities by taking softmax of action preferences\n",
    "    policy = apply_softmax_function(number_of_bandits, action_preference, policy)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies the softmax function\n",
    "def apply_softmax_function(number_of_bandits, action_preference, policy):\n",
    "    # f(xi) = e^(xi - max(x)) / sum(e^(xj - max(x)), j, 0, n - 1)\n",
    "\n",
    "    max_logit = np.max(a = action_preference)\n",
    "\n",
    "    # Shift logits by the max logit to make numerically stable\n",
    "    policy = np.exp(action_preference - max_logit)\n",
    "    policy /= np.sum(a = policy)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_multi_armed_gradient_bandits(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_preference, policy, alpha, average_reward_update_type):\n",
    "    # Loop through iterations and update the policy\n",
    "    bandit_mean, bandit_variance, action_preference, policy = loop_through_iterations(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_preference, policy, alpha, average_reward_update_type)\n",
    "    \n",
    "    return bandit_mean, bandit_variance, action_preference, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial bandit mean\n",
      "[ 0.1957272  -1.51936052  0.40767963 -0.45513443 -0.48142763 -1.24410407\n",
      "  2.60391307 -1.93834966 -2.15886687 -1.15053664]\n",
      "\n",
      "Initial bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Initial action preference function\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Initial policy\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "\n",
      "Final bandit mean\n",
      "[ 0.01430227 -1.07617476 -1.07668153 -0.81885676 -0.64743395 -0.23869974\n",
      "  0.23653173  0.01623212  1.25593101 -0.67111337]\n",
      "\n",
      "Final bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Final action preference function\n",
      "[-0.18786209 -0.89910764 -1.53930444 -0.55416574 -1.1369539  -1.44614132\n",
      " -0.05565221 -0.82714036  5.95762718  0.68870053]\n",
      "\n",
      "Final policy\n",
      "[2.11096262e-03 1.03654008e-03 5.46449636e-04 1.46350504e-03\n",
      " 8.17127591e-04 5.99805701e-04 2.40934943e-03 1.11388770e-03\n",
      " 9.84830374e-01 5.07199772e-03]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial bandit mean\")\n",
    "print(bandit_mean)\n",
    "\n",
    "print(\"\\nInitial bandit variance\")\n",
    "print(bandit_variance)\n",
    "\n",
    "print(\"\\nInitial action preference function\")\n",
    "print(action_preference)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run on policy temporal difference sarsa\n",
    "bandit_mean, bandit_variance, action_preference, policy = stochastic_multi_armed_gradient_bandits(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_preference, policy, alpha, average_reward_update_type)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal bandit mean\")\n",
    "print(bandit_mean)\n",
    "\n",
    "print(\"\\nFinal bandit variance\")\n",
    "print(bandit_variance)\n",
    "\n",
    "print(\"\\nFinal action preference function\")\n",
    "print(action_preference)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
