{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Gradient Bandits: Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_num_bandits():\n",
    "    \"\"\"Creates environment number of bandits.\n",
    "\n",
    "    Returns:\n",
    "        num_bandits: int, number of bandits.\n",
    "    \"\"\"\n",
    "    num_bandits = 10\n",
    "\n",
    "    return num_bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_means(num_bandits):\n",
    "    \"\"\"Creates environment bandit means.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "    \"\"\"\n",
    "    global_bandit_mean_mean = 0.0\n",
    "    global_bandit_mean_variance = 1.0\n",
    "\n",
    "    bandit_mean = np.random.normal(\n",
    "        loc=global_bandit_mean_mean,\n",
    "        scale=np.sqrt(global_bandit_mean_variance),\n",
    "        size=num_bandits)\n",
    "\n",
    "    return global_bandit_mean_mean, global_bandit_mean_variance, bandit_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_variances(num_bandits):\n",
    "    \"\"\"Creates environment bandit variances.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "    \"\"\"\n",
    "    global_bandit_variance_mean = 1.0\n",
    "    global_bandit_variance_variance = 0.0\n",
    "\n",
    "    bandit_variance = np.random.normal(\n",
    "        loc=global_bandit_variance_mean,\n",
    "        scale=np.sqrt(global_bandit_variance_variance),\n",
    "        size=num_bandits)\n",
    "\n",
    "    return (global_bandit_variance_mean,\n",
    "            global_bandit_variance_variance,\n",
    "            bandit_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_change_arrays(num_bandits):\n",
    "    \"\"\"Creates environment bandit change arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "    \"\"\"\n",
    "    bandit_change_frequencies = np.repeat(\n",
    "        a=201, repeats=num_bandits)\n",
    "\n",
    "    bandit_change_counter = np.zeros(\n",
    "        shape=[num_bandits], dtype=np.int64)\n",
    "\n",
    "    return (bandit_change_frequencies,\n",
    "            bandit_change_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    \"\"\"Creates environment.\n",
    "\n",
    "    Returns:\n",
    "        num_bandits: int, number of bandits.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "    \"\"\"\n",
    "    num_bandits = create_environment_num_bandits()\n",
    "\n",
    "    (global_bandit_mean_mean,\n",
    "     global_bandit_mean_variance,\n",
    "     bandit_mean) = create_environment_bandit_means(num_bandits)\n",
    "\n",
    "    (global_bandit_variance_mean,\n",
    "     global_bandit_variance_variance,\n",
    "     bandit_variance) = create_environment_bandit_variances(num_bandits)\n",
    "\n",
    "    (bandit_change_frequencies,\n",
    "     bandit_change_counter) = create_environment_bandit_change_arrays(\n",
    "        num_bandits)\n",
    "\n",
    "    return (num_bandits,\n",
    "            global_bandit_mean_mean,\n",
    "            global_bandit_mean_variance,\n",
    "            bandit_mean,\n",
    "            global_bandit_variance_mean,\n",
    "            global_bandit_variance_variance,\n",
    "            bandit_variance,\n",
    "            bandit_change_frequencies,\n",
    "            bandit_change_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        num_iterations: int, number of iterations.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        average_reward_update_type: int, average reward update type (\n",
    "            sample-average, constant step-size).\n",
    "    \"\"\"\n",
    "    num_iterations = 2000\n",
    "    alpha = 0.1\n",
    "    average_reward_update_type = 1\n",
    "\n",
    "    return (num_iterations,\n",
    "            alpha,\n",
    "            average_reward_update_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_arrays(num_bandits):\n",
    "    \"\"\"Creates action arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "    \"\"\"\n",
    "    action_preference = np.zeros(shape=[num_bandits], dtype=np.float64)\n",
    "\n",
    "    return action_preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_bandits):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    policy = np.repeat(a=1.0 / num_bandits, repeats=num_bandits)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_iterations(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_preference,\n",
    "        policy,\n",
    "        alpha,\n",
    "        average_reward_update_type):\n",
    "    \"\"\"Loops through iterations to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: int, number of iterations.\n",
    "        num_bandits: int, number of bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        average_reward_update_type: int, average reward update type (\n",
    "            sample-average, constant step-size).\n",
    "    Returns:\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    average_reward = 0.0\n",
    "    # Loop through iterations until termination\n",
    "    for t in range(0, num_iterations):\n",
    "        # Choose policy by epsilon-greedy choosing from action-value function\n",
    "        policy = update_policy_from_action_preference(\n",
    "            num_bandits, action_preference, policy)\n",
    "\n",
    "        # Get action\n",
    "        a_idx = np.random.choice(a=num_bandits, p=policy)\n",
    "\n",
    "        # Get reward from action\n",
    "        reward = np.random.normal(\n",
    "            loc=bandit_mean[a_idx], scale=np.sqrt(bandit_variance[a_idx]))\n",
    "\n",
    "        # Update average reward\n",
    "        if average_reward_update_type == 0:  # sample-average method\n",
    "            average_reward += 1.0 / (t + 1) * (reward - average_reward)\n",
    "        elif average_reward_update_type == 1:  # constant step-size\n",
    "            average_reward += alpha * (reward - average_reward)\n",
    "\n",
    "        # Update action preference\n",
    "        reward_diff = reward - average_reward\n",
    "        action_preference = np.where(\n",
    "            np.arange(num_bandits) == a_idx,\n",
    "            action_preference + alpha * reward_diff * (1.0 - policy),\n",
    "            action_preference - alpha * reward_diff * policy)\n",
    "\n",
    "        # Mutate bandit statistics\n",
    "        for i in range(num_bandits):\n",
    "            if bandit_change_frequencies[i] > 0:\n",
    "                bandit_change_counter[i] += 1\n",
    "\n",
    "                if bandit_change_counter[i] == bandit_change_frequencies[i]:\n",
    "                    bandit_mean[i] = np.random.normal(\n",
    "                        loc=global_bandit_mean_mean,\n",
    "                        scale=np.sqrt(global_bandit_mean_variance))\n",
    "                    bandit_variance[i] = np.random.normal(\n",
    "                        loc=global_bandit_variance_mean,\n",
    "                        scale=np.sqrt(global_bandit_variance_variance))\n",
    "\n",
    "                    bandit_change_counter[i] = 0\n",
    "\n",
    "    return bandit_mean, bandit_variance, action_preference, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_from_action_preference(\n",
    "        num_bandits, action_preference, policy):\n",
    "    \"\"\"Updates policy based on action preference.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    # Calculate probabilities by taking softmax of action preferences\n",
    "    policy = apply_softmax_function(num_bandits, action_preference, policy)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_softmax_function(num_bandits, action_preference, policy):\n",
    "    \"\"\"Applies the softmax function to action preferences to update policy.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    # f(xi) = e^(xi - max(x)) / sum(e^(xj - max(x)), j, 0, n - 1)\n",
    "\n",
    "    max_logit = np.max(a=action_preference)\n",
    "\n",
    "    # Shift logits by the max logit to make numerically stable\n",
    "    policy = np.exp(action_preference - max_logit)\n",
    "    policy /= np.sum(a=policy)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_multi_armed_gradient_bandits(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_preference,\n",
    "        policy,\n",
    "        alpha,\n",
    "        average_reward_update_type):\n",
    "    \"\"\"Loops through iterations to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: int, number of iterations.\n",
    "        num_bandits: int, number of bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        average_reward_update_type: int, average reward update type (\n",
    "            sample-average, constant step-size).\n",
    "    Returns:\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        action_preference: array[float], keeps track of the preference\n",
    "            of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    # Loop through iterations and update the policy\n",
    "    (bandit_mean,\n",
    "     bandit_variance,\n",
    "     action_preference,\n",
    "     policy) = loop_through_iterations(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_preference,\n",
    "        policy,\n",
    "        alpha,\n",
    "        average_reward_update_type)\n",
    "\n",
    "    return bandit_mean, bandit_variance, action_preference, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\"\"\"\n",
    "    (num_bandits,\n",
    "     global_bandit_mean_mean,\n",
    "     global_bandit_mean_variance,\n",
    "     bandit_mean,\n",
    "     global_bandit_variance_mean,\n",
    "     global_bandit_variance_variance,\n",
    "     bandit_variance,\n",
    "     bandit_change_frequencies,\n",
    "     bandit_change_counter) = create_environment()\n",
    "\n",
    "    num_iterations, alpha, average_reward_update_type = set_hyperparameters()\n",
    "\n",
    "    action_preference = create_action_arrays(num_bandits)\n",
    "\n",
    "    policy = create_policy_arrays(num_bandits)\n",
    "\n",
    "    # Print initial arrays\n",
    "    print(\"\\nInitial bandit mean\")\n",
    "    print(bandit_mean)\n",
    "\n",
    "    print(\"\\nInitial bandit variance\")\n",
    "    print(bandit_variance)\n",
    "\n",
    "    print(\"\\nInitial action preference function\")\n",
    "    print(action_preference)\n",
    "\n",
    "    print(\"\\nInitial policy\")\n",
    "    print(policy)\n",
    "\n",
    "    # Run on policy temporal difference sarsa\n",
    "    (bandit_mean,\n",
    "     bandit_variance,\n",
    "     action_preference,\n",
    "     policy) = stochastic_multi_armed_gradient_bandits(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_preference,\n",
    "        policy,\n",
    "        alpha,\n",
    "        average_reward_update_type)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal bandit mean\")\n",
    "    print(bandit_mean)\n",
    "\n",
    "    print(\"\\nFinal bandit variance\")\n",
    "    print(bandit_variance)\n",
    "\n",
    "    print(\"\\nFinal action preference function\")\n",
    "    print(action_preference)\n",
    "\n",
    "    print(\"\\nFinal policy\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial bandit mean\n",
      "[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n",
      "\n",
      "Initial bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Initial action preference function\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Initial policy\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "\n",
      "Final bandit mean\n",
      "[-1.07668153 -0.81885676 -0.64743395 -0.23869974  0.23653173  0.01623212\n",
      "  1.25593101 -0.67111337  0.92955301  0.31983224]\n",
      "\n",
      "Final bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Final action preference function\n",
      "[-0.67786663 -1.22810691  0.54660341  4.8841117  -1.57988109 -0.29028634\n",
      " -0.82674735 -0.50718411 -0.83868299  0.51804032]\n",
      "\n",
      "Final policy\n",
      "[0.00365876 0.00211039 0.01244912 0.95220016 0.00148453 0.00539092\n",
      " 0.00315264 0.00433972 0.00311523 0.01209854]\n"
     ]
    }
   ],
   "source": [
    "run_algorithm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
