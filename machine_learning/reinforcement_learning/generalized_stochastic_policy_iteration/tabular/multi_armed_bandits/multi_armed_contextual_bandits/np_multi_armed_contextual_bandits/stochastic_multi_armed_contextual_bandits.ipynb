{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Contextual Bandits: Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_num_states():\n",
    "    \"\"\"Creates environment number of states.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "    \"\"\"\n",
    "    num_states = 8\n",
    "\n",
    "    return num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_num_bandits():\n",
    "    \"\"\"Creates environment number of bandits.\n",
    "\n",
    "    Returns:\n",
    "        num_bandits: int, number of bandits.\n",
    "    \"\"\"\n",
    "    num_bandits = 10\n",
    "\n",
    "    return num_bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_means(num_states, num_bandits):\n",
    "    \"\"\"Creates environment bandit means.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "    \"\"\"\n",
    "    global_bandit_mean_mean = 0.0\n",
    "    global_bandit_mean_variance = 1.0\n",
    "    \n",
    "    bandit_mean = np.random.normal(\n",
    "        loc=global_bandit_mean_mean,\n",
    "        scale=np.sqrt(global_bandit_mean_variance),\n",
    "        size=num_states * num_bandits)\n",
    "\n",
    "    bandit_mean = bandit_mean.reshape(num_states, num_bandits)\n",
    "\n",
    "    return global_bandit_mean_mean, global_bandit_mean_variance, bandit_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_variances(num_states, num_bandits):\n",
    "    \"\"\"Creates environment bandit variances.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "    \"\"\"\n",
    "    global_bandit_variance_mean = 1.0\n",
    "    global_bandit_variance_variance = 0.0\n",
    "\n",
    "    bandit_variance = np.random.normal(\n",
    "        loc=global_bandit_variance_mean,\n",
    "        scale=np.sqrt(global_bandit_variance_variance),\n",
    "        size=num_states * num_bandits)\n",
    "\n",
    "    bandit_variance = bandit_variance.reshape(num_states, num_bandits)\n",
    "\n",
    "    return (global_bandit_variance_mean,\n",
    "            global_bandit_variance_variance,\n",
    "            bandit_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_change_arrays(num_bandits):\n",
    "    \"\"\"Creates environment bandit change arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "    \"\"\"\n",
    "    bandit_change_frequencies = np.repeat(\n",
    "        a=201, repeats=num_bandits)\n",
    "\n",
    "    bandit_change_counter = np.zeros(\n",
    "        shape=[num_bandits], dtype=np.int64)\n",
    "\n",
    "    return (bandit_change_frequencies,\n",
    "            bandit_change_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    \"\"\"Creates environment.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_bandits: int, number of bandits.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "    \"\"\"\n",
    "    num_states = create_environment_num_states()\n",
    "\n",
    "    num_bandits = create_environment_num_bandits()\n",
    "\n",
    "    (global_bandit_mean_mean,\n",
    "     global_bandit_mean_variance,\n",
    "     bandit_mean) = create_environment_bandit_means(num_states, num_bandits)\n",
    "\n",
    "    (global_bandit_variance_mean,\n",
    "     global_bandit_variance_variance,\n",
    "     bandit_variance) = create_environment_bandit_variances(\n",
    "        num_states, num_bandits)\n",
    "\n",
    "    (bandit_change_frequencies,\n",
    "     bandit_change_counter) = create_environment_bandit_change_arrays(\n",
    "        num_bandits)\n",
    "\n",
    "    return (num_states,\n",
    "            num_bandits,\n",
    "            global_bandit_mean_mean,\n",
    "            global_bandit_mean_variance,\n",
    "            bandit_mean,\n",
    "            global_bandit_variance_mean,\n",
    "            global_bandit_variance_variance,\n",
    "            bandit_variance,\n",
    "            bandit_change_frequencies,\n",
    "            bandit_change_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        num_iterations: int, number of iterations.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        action_value_update_type: int, action value update type (\n",
    "            sample-average, biased constant step-size, unbiased constant\n",
    "            step-size).\n",
    "    \"\"\"\n",
    "    num_iterations = 2000\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.1\n",
    "    action_selection_type = 1\n",
    "    action_value_update_type = 2\n",
    "\n",
    "    return (num_iterations,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            action_selection_type,\n",
    "            action_value_update_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_arrays(num_states, num_bandits):\n",
    "    \"\"\"Creates action arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each bandit in\n",
    "            each state, Q(s, b).\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        action_trace: array[float], keeps track of the reward trace for each\n",
    "            bandit.\n",
    "    \"\"\"\n",
    "    q = np.zeros(shape=[num_states, num_bandits], dtype=np.float64)\n",
    "\n",
    "    action_count = np.zeros(shape=[num_states, num_bandits], dtype=np.int64)\n",
    "\n",
    "    action_trace = np.zeros(shape=[num_states, num_bandits], dtype=np.float64)\n",
    "\n",
    "    return q, action_count, action_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_states, num_bandits):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "    \"\"\"\n",
    "    policy = np.repeat(a=1.0 / num_bandits, repeats=num_states * num_bandits)\n",
    "    policy = policy.reshape(num_states, num_bandits)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_iterations(\n",
    "        num_iterations,\n",
    "        num_states,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        q,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type):\n",
    "    \"\"\"Loops through iterations to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: int, number of iterations.\n",
    "        num_states: int, number of states.\n",
    "        num_bandits: int, number of bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        q: array[float], keeps track of the estimated value of each bandit in\n",
    "            each state, Q(s, b).\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        action_trace: array[float], keeps track of the reward trace for each\n",
    "            bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        action_value_update_type: int, action value update type (\n",
    "            sample-average, biased constant step-size, unbiased constant\n",
    "            step-size).\n",
    "    Returns:\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        q: array[float], keeps track of the estimated value of each bandit in\n",
    "            each state, Q(s, b).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "    \"\"\"\n",
    "    # Loop through iterations until termination\n",
    "    for t in range(0, num_iterations):\n",
    "        # Get random state\n",
    "        s_idx = np.random.randint(num_states)\n",
    "\n",
    "        # Choose policy by epsilon-greedy choosing from state-action-value\n",
    "        # function\n",
    "        policy = update_policy_from_q(\n",
    "            s_idx,\n",
    "            num_bandits,\n",
    "            q,\n",
    "            action_count,\n",
    "            t + 1,\n",
    "            epsilon,\n",
    "            action_selection_type,\n",
    "            policy)\n",
    "\n",
    "        # Get action\n",
    "        a_idx = np.random.choice(\n",
    "            a=num_bandits, p=policy[s_idx, :])\n",
    "\n",
    "        # Get reward from action\n",
    "        reward = np.random.normal(\n",
    "            loc=bandit_mean[s_idx, a_idx],\n",
    "            scale=np.sqrt(bandit_variance[s_idx, a_idx]))\n",
    "\n",
    "        # Update action count\n",
    "        action_count[s_idx, a_idx] += 1\n",
    "\n",
    "        # Update action-value function\n",
    "        delta = reward - q[s_idx, a_idx]\n",
    "        if action_value_update_type == 0:  # sample-average method\n",
    "            learning_rate = 1.0 / action_count[s_idx, a_idx]\n",
    "            q[s_idx, a_idx] += learning_rate * delta\n",
    "        elif action_value_update_type == 1:  # biased constant step-size\n",
    "            q[s_idx, a_idx] += alpha * delta\n",
    "        elif action_value_update_type == 2:  # unbiased constant step-size\n",
    "            # Update action trace\n",
    "            trace_diff = 1.0 - action_trace[s_idx, a_idx]\n",
    "            action_trace[s_idx, a_idx] += alpha * trace_diff\n",
    "\n",
    "            learning_rate = alpha / action_trace[s_idx, a_idx]\n",
    "            q[s_idx, a_idx] += learning_rate * delta\n",
    "\n",
    "        # Mutate bandit statistics\n",
    "        for i in range(num_bandits):\n",
    "            if bandit_change_frequencies[i] > 0:\n",
    "                bandit_change_counter[i] += 1\n",
    "\n",
    "                if bandit_change_counter[i] == bandit_change_frequencies[i]:\n",
    "                    bandit_mean[:, i] = np.random.normal(\n",
    "                        loc=global_bandit_mean_mean,\n",
    "                        scale=np.sqrt(global_bandit_mean_variance),\n",
    "                        size=num_states)\n",
    "                    bandit_variance[:, i] = np.random.normal(\n",
    "                        loc=global_bandit_variance_mean,\n",
    "                        scale=np.sqrt(global_bandit_variance_variance),\n",
    "                        size=num_states)\n",
    "\n",
    "                    bandit_change_counter[i] = 0\n",
    "\n",
    "    return bandit_mean, bandit_variance, q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_from_q(\n",
    "        s_idx,\n",
    "        num_bandits,\n",
    "        q,\n",
    "        action_count,\n",
    "        iteration_count,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        policy):\n",
    "    \"\"\"Updates policy epilson-greedily from state-action-value function.\n",
    "\n",
    "    Args:\n",
    "        s_idx: int, current state index.\n",
    "        num_bandits: int, number of bandits.\n",
    "        q: array[float], keeps track of the estimated value of each bandit in\n",
    "            each state, Q(s, b).\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        iteration_count: int, current loop iteration count.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "    \"\"\"\n",
    "    # Calculate action value depending on action selection type\n",
    "    if action_selection_type == 0 or action_selection_type == 1:\n",
    "        # Greedy or epsilon-greedy\n",
    "        action_value = q[s_idx, :]\n",
    "    elif action_selection_type == 2:\n",
    "        # Upper-confidence-bound\n",
    "        min_count_idx = np.argmin(a=q[s_idx, :])\n",
    "        if min_count_idx == 0:\n",
    "            policy[s_idx, :] = np.where(\n",
    "                np.arange(num_bandits) == min_count_idx, 1.0, 0.0)\n",
    "            return policy\n",
    "        else:\n",
    "            action_value = q[s_idx, :] + epsilon * np.sqrt(\n",
    "                np.log(iteration_count) / action_count)\n",
    "\n",
    "    # Save max action value and find the number of actions that have the same\n",
    "    # max action value\n",
    "    max_action_value = np.max(a=q[s_idx, :])\n",
    "    max_action_count = np.count_nonzero(a=q[s_idx, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    if action_selection_type == 1:\n",
    "        # Epsilon-greedy\n",
    "        if max_action_count == num_bandits:\n",
    "            max_policy_prob_per_action = 1.0 / max_action_count\n",
    "            remain_prob_per_action = 0.0\n",
    "        else:\n",
    "            max_policy_prob_per_action = (1.0 - epsilon) / max_action_count\n",
    "            remain_prob_per_action = epsilon / (num_bandits - max_action_count)\n",
    "    elif action_selection_type == 0 or action_selection_type == 2:\n",
    "        # Greedy or upper-confidence-bound\n",
    "        max_policy_prob_per_action = 1.0 / max_action_count\n",
    "        remain_prob_per_action = 0.0\n",
    "\n",
    "    policy[s_idx, :] = np.where(\n",
    "        action_value == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        remain_prob_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_multi_armed_contextual_bandits(\n",
    "        num_iterations,\n",
    "        num_states,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        q,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type):\n",
    "    \"\"\"Loops through iterations to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: int, number of iterations.\n",
    "        num_states: int, number of states.\n",
    "        num_bandits: int, number of bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        q: array[float], keeps track of the estimated value of each bandit in\n",
    "            each state, Q(s, b).\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        action_trace: array[float], keeps track of the reward trace for each\n",
    "            bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        action_value_update_type: int, action value update type (\n",
    "            sample-average, biased constant step-size, unbiased constant\n",
    "            step-size).\n",
    "    Returns:\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        q: array[float], keeps track of the estimated value of each bandit in\n",
    "            each state, Q(s, b).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action given a state.\n",
    "    \"\"\"\n",
    "    # Loop through iterations and update the policy\n",
    "    bandit_mean, bandit_variance, q, policy = loop_through_iterations(\n",
    "        num_iterations,\n",
    "        num_states,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        q,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type)\n",
    "\n",
    "    return bandit_mean, bandit_variance, q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\"\"\"\n",
    "    (num_states,\n",
    "     num_bandits,\n",
    "     global_bandit_mean_mean,\n",
    "     global_bandit_mean_variance,\n",
    "     bandit_mean,\n",
    "     global_bandit_variance_mean,\n",
    "     global_bandit_variance_variance,\n",
    "     bandit_variance,\n",
    "     bandit_change_frequencies,\n",
    "     bandit_change_counter) = create_environment()\n",
    "\n",
    "    (num_iterations,\n",
    "     alpha,\n",
    "     epsilon,\n",
    "     action_selection_type,\n",
    "     action_value_update_type) = set_hyperparameters()\n",
    "\n",
    "    q, action_count, action_trace = create_action_arrays(\n",
    "        num_states, num_bandits)\n",
    "\n",
    "    policy = create_policy_arrays(num_states, num_bandits)\n",
    "\n",
    "    # Print initial arrays\n",
    "    print(\"\\nInitial bandit mean\")\n",
    "    print(bandit_mean)\n",
    "\n",
    "    print(\"\\nInitial bandit variance\")\n",
    "    print(bandit_variance)\n",
    "\n",
    "    print(\"\\nInitial state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nInitial policy\")\n",
    "    print(policy)\n",
    "\n",
    "    # Run on policy temporal difference sarsa\n",
    "    (bandit_mean,\n",
    "     bandit_variance,\n",
    "     q,\n",
    "     policy) = stochastic_multi_armed_contextual_bandits(\n",
    "        num_iterations,\n",
    "        num_states,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        q,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal bandit mean\")\n",
    "    print(bandit_mean)\n",
    "\n",
    "    print(\"\\nFinal bandit variance\")\n",
    "    print(bandit_variance)\n",
    "\n",
    "    print(\"\\nFinal state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nFinal policy\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial bandit mean\n",
      "[[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "   0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n",
      " [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323  0.33367433\n",
      "   1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462 -1.45436567\n",
      "   0.04575852 -0.18718385  1.53277921  1.46935877]\n",
      " [ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215  0.15634897\n",
      "   1.23029068  1.20237985 -0.38732682 -0.30230275]\n",
      " [-1.04855297 -1.42001794 -1.70627019  1.9507754  -0.50965218 -0.4380743\n",
      "  -1.25279536  0.77749036 -1.61389785 -0.21274028]\n",
      " [-0.89546656  0.3869025  -0.51080514 -1.18063218 -0.02818223  0.42833187\n",
      "   0.06651722  0.3024719  -0.63432209 -0.36274117]\n",
      " [-0.67246045 -0.35955316 -0.81314628 -1.7262826   0.17742614 -0.40178094\n",
      "  -1.63019835  0.46278226 -0.90729836  0.0519454 ]\n",
      " [ 0.72909056  0.12898291  1.13940068 -1.23482582  0.40234164 -0.68481009\n",
      "  -0.87079715 -0.57884966 -0.31155253  0.05616534]]\n",
      "\n",
      "Initial bandit variance\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
      "\n",
      "Final bandit mean\n",
      "[[-1.40589108  1.83913723 -1.36672695  0.32918202  1.21839055  0.83574959\n",
      "   2.03616429  0.50702432 -1.40333144  0.12625352]\n",
      " [ 0.39986639  0.1521146   0.01870451 -0.14589035 -1.45714129  1.0530042\n",
      "  -0.81086173 -1.23985531 -0.40915673  0.25593894]\n",
      " [ 0.2232724   0.87068751 -0.31229445  0.7077315   2.13881312  0.5087983\n",
      "  -0.94550194  1.09546951 -2.20836457  0.95054718]\n",
      " [ 0.44938339 -1.41174638 -0.7739106   0.91599466 -0.66619324  0.11906977\n",
      "   0.05515039  1.18063137  1.11692313 -0.64763911]\n",
      " [ 1.19375768  0.31006721  0.59360572  0.43541812 -0.44876515 -2.02813952\n",
      "  -0.4698713  -1.25848375  0.05664064 -0.15159892]\n",
      " [ 0.33657785  0.14199526  1.22483172  0.61668411  2.25186866  0.08763878\n",
      "  -1.33529084 -0.06040796 -0.99163381 -0.48756218]\n",
      " [ 0.80880105 -0.21935062  1.15635599 -0.58053307 -0.76365363  1.9292509\n",
      "   0.15887735 -0.21164971  1.08225318  1.64651705]\n",
      " [-0.43686519 -2.56597308  1.60054151 -0.62531678 -0.24480799  0.63349768\n",
      "  -0.48417372  0.80530041  0.55923541  0.68442069]]\n",
      "\n",
      "Final bandit variance\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Final state-action value function\n",
      "[[ 0.0662906  -0.06278537 -0.23806457  0.07047282  1.07157521 -0.04429175\n",
      "  -0.36541823 -0.69068174 -0.23701176 -0.30071712]\n",
      " [-0.139542   -0.04907388 -0.00411942 -0.02066082 -0.11600349  0.72559358\n",
      "  -0.24724475 -0.18164385 -0.05215079  0.15685743]\n",
      " [-1.58909197  0.85577266  0.18577058  0.19990415  3.16925872  0.21386067\n",
      "  -0.14068832  0.27625869 -1.62899242 -2.0017622 ]\n",
      " [-0.10931344 -0.41204063 -0.23129923 -0.06727593 -0.13676935 -0.37184063\n",
      "  -0.30484839 -0.18721819  1.151939   -0.59371918]\n",
      " [-0.57234136 -0.18289856 -0.07285829  0.73566261 -0.327141   -1.25147886\n",
      "  -0.37910262 -0.65201015 -0.24302982 -0.6315339 ]\n",
      " [-0.45802997 -0.16537532 -0.88464145 -0.78520204  1.64228869 -1.20201321\n",
      "  -1.08304605 -0.42437238 -0.78212763 -0.44349861]\n",
      " [-1.06369339 -0.33017157 -0.04107948 -1.69297442 -0.82956348 -1.0592818\n",
      "  -0.74832483  0.05384388 -0.35615548  1.13530465]\n",
      " [ 0.04830132 -0.33928519  0.33901146  0.21647644 -0.01932378  0.55045067\n",
      "  -0.80598585  1.11633234  0.26824665  0.42639972]]\n",
      "\n",
      "Final policy\n",
      "[[0.01111111 0.01111111 0.01111111 0.01111111 0.9        0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.9\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.9        0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.9        0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.9        0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.9        0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.9       ]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.9        0.01111111 0.01111111]]\n"
     ]
    }
   ],
   "source": [
    "run_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
