{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bandits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97769822, -0.95743347,  1.23151506, -0.61025123,  0.49222511,\n",
       "        -1.03686477, -0.10894172,  0.71237699, -0.13493995,  0.65384896],\n",
       "       [ 1.20738252, -0.45827285,  0.67056061, -2.12437064, -0.85994336,\n",
       "        -1.12569576,  2.15337041,  1.06479789,  1.6433249 ,  1.00446276],\n",
       "       [ 0.86270704, -0.64426033,  1.98798016, -0.82591445, -0.24048806,\n",
       "        -0.10600849,  1.49507224, -0.62041518,  0.1753965 , -1.35948718],\n",
       "       [-0.87054276, -0.16610892, -0.72700421,  1.25727616,  0.69112453,\n",
       "        -0.72581363, -0.34268559, -0.08943292, -1.07181379, -0.88667207],\n",
       "       [ 0.84000859, -1.34853052,  0.18606458, -0.15302866, -0.43935007,\n",
       "         1.2407873 , -0.45009704,  0.75279305,  0.14540684, -1.70121639],\n",
       "       [-0.11609372,  0.38242081, -0.4006153 ,  1.5709845 ,  0.61559163,\n",
       "         1.05404329, -0.64900393, -1.99388633,  0.07495446, -0.2431281 ],\n",
       "       [ 0.18599669,  0.7386674 , -0.61888697,  0.58111699,  0.47827546,\n",
       "        -0.03025682, -0.56950803, -0.01848738, -0.0463772 , -0.07039535],\n",
       "       [-1.0230011 , -0.72050413, -1.32661279, -0.03016345,  0.60687215,\n",
       "        -1.136144  , -1.78557353,  0.58456567, -0.32864403,  0.56322594]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_bandit_mean_mean = 0.0\n",
    "global_bandit_mean_variance = 1.0\n",
    "\n",
    "bandit_mean = np.random.normal(loc = global_bandit_mean_mean, scale = np.sqrt(global_bandit_mean_variance), size = number_of_states * number_of_bandits)\n",
    "bandit_mean = bandit_mean.reshape(number_of_states, number_of_bandits)\n",
    "bandit_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_bandit_variance_mean = 1.0\n",
    "global_bandit_variance_variance = 0.0\n",
    "\n",
    "bandit_variance = np.random.normal(loc = global_bandit_variance_mean, scale = np.sqrt(global_bandit_variance_variance), size = number_of_states * number_of_bandits)\n",
    "bandit_variance = bandit_variance.reshape(number_of_states, number_of_bandits)\n",
    "bandit_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 201, 201, 201, 201, 201, 201, 201, 201, 201])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_stochastic_change_frequencies = np.repeat(a = 201, repeats = number_of_bandits)\n",
    "bandit_stochastic_change_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_stochastic_change_counter = np.zeros(shape = [number_of_bandits], dtype = np.int64)\n",
    "bandit_stochastic_change_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of iterations\n",
    "number_of_iterations = 2000\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.1\n",
    "# Set action selection type (greedy, epsilon-greedy, upper-confidence-bound)\n",
    "action_selection_type = 1\n",
    "# Set action value update type (sample-average, biased constant step-size, unbiased constant step-size)\n",
    "action_value_update_type = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_value_function = np.zeros(shape = [number_of_states, number_of_bandits], dtype = np.float64)\n",
    "state_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_count = np.zeros(shape = [number_of_states, number_of_bandits], dtype = np.int64)\n",
    "action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_trace = np.zeros(shape = [number_of_states, number_of_bandits], dtype = np.float64)\n",
    "action_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.repeat(a = 1.0 / number_of_bandits, repeats = number_of_states * number_of_bandits)\n",
    "policy = policy.reshape(number_of_states, number_of_bandits)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through iterations and updates the policy\n",
    "def loop_through_iterations(number_of_iterations, number_of_states, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, state_action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type):\n",
    "    # Loop through iterations until termination\n",
    "    for t in range(0, number_of_iterations):\n",
    "        # Get random state\n",
    "        state_index = np.random.randint(number_of_states)\n",
    "    \n",
    "        # Choose policy by epsilon-greedy choosing from the action-value function\n",
    "        policy = update_policy_from_state_action_value_function(state_index, number_of_bandits, state_action_value_function, action_count, t + 1, epsilon, action_selection_type, policy);\n",
    "\n",
    "        # Get action\n",
    "        action_index = np.random.choice(a = number_of_bandits, p = policy[state_index, :])\n",
    "\n",
    "        # Get reward from action\n",
    "        reward = np.random.normal(loc = bandit_mean[state_index, action_index], scale = np.sqrt(bandit_variance[state_index, action_index]))\n",
    "\n",
    "        # Update action count\n",
    "        action_count[state_index, action_index] += 1\n",
    "\n",
    "        # Update action-value function\n",
    "        if action_value_update_type == 0: # sample-average method\n",
    "            state_action_value_function[state_index, action_index] += (1.0 / action_count[state_index, action_index]) * (reward - state_action_value_function[state_index, action_index]);\n",
    "        elif action_value_update_type == 1: # biased constant step-size\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward - state_action_value_function[state_index, action_index]);\n",
    "        elif action_value_update_type == 2: # unbiased constant step-size\n",
    "            # Update action trace\n",
    "            action_trace[state_index, action_index] += alpha * (1.0 - action_trace[state_index, action_index]);\n",
    "\n",
    "            state_action_value_function[state_index, action_index] += (alpha / action_trace[state_index, action_index]) * (reward - state_action_value_function[state_index, action_index]);\n",
    "\n",
    "        # Mutate bandit statistics\n",
    "        for i in range(number_of_bandits):\n",
    "            if bandit_stochastic_change_frequencies[i] > 0:\n",
    "                bandit_stochastic_change_counter[i] += 1\n",
    "\n",
    "                if bandit_stochastic_change_counter[i] == bandit_stochastic_change_frequencies[i]:\n",
    "                    bandit_mean[:, i] = np.random.normal(loc = global_bandit_mean_mean, scale = np.sqrt(global_bandit_mean_variance), size = number_of_states)\n",
    "                    bandit_variance[:, i] = np.random.normal(loc = global_bandit_variance_mean, scale = np.sqrt(global_bandit_variance_variance), size = number_of_states)\n",
    "\n",
    "                    bandit_stochastic_change_counter[i] = 0\n",
    "\n",
    "    return bandit_mean, bandit_variance, state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates policy as some function of action-value function\n",
    "def update_policy_from_state_action_value_function(state_index, number_of_bandits, state_action_value_function, action_count, iteration_count, epsilon, action_selection_type, policy):\n",
    "    # Calculate action value depending on action selection type\n",
    "    if action_selection_type == 0 or action_selection_type == 1: # greedy or epsilon-greedy\n",
    "        action_value = state_action_value_function[state_index, :]\n",
    "    elif action_selection_type == 2: # upper-confidence-bound\n",
    "        min_count_idx = np.argmin(a = state_action_value_function[state_index, :])\n",
    "        if min_count_idx == 0:\n",
    "            policy[state_index, :] = np.where(np.arange(number_of_bandits) == min_count_idx, 1.0, 0.0)\n",
    "            return policy\n",
    "        else:\n",
    "            action_value = state_action_value_function[state_index, :] + epsilon * np.sqrt(np.log(iteration_count) / action_count)\n",
    "    \n",
    "    # Save max action value and find the number of actions that have the same max action value\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function[state_index, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if action_selection_type == 1: # epsilon-greedy\n",
    "        if max_action_count == number_of_bandits:\n",
    "            max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "            remaining_apportioned_probability_per_action = 0.0\n",
    "        else:\n",
    "            max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "            remaining_apportioned_probability_per_action = epsilon / (number_of_bandits - max_action_count)\n",
    "    elif action_selection_type == 0 or action_selection_type == 2: # greedy or upper-confidence-bound\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "\n",
    "    policy[state_index, :] = np.where(action_value == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_multi_armed_contextual_bandits(number_of_iterations, number_of_states, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, state_action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type):\n",
    "    # Loop through iterations and update the policy\n",
    "    bandit_mean, bandit_variance, state_action_value_function, policy = loop_through_iterations(number_of_iterations, number_of_states, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, state_action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type)\n",
    "    \n",
    "    return bandit_mean, bandit_variance, state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial bandit mean\n",
      "[[ 0.97769822 -0.95743347  1.23151506 -0.61025123  0.49222511 -1.03686477\n",
      "  -0.10894172  0.71237699 -0.13493995  0.65384896]\n",
      " [ 1.20738252 -0.45827285  0.67056061 -2.12437064 -0.85994336 -1.12569576\n",
      "   2.15337041  1.06479789  1.6433249   1.00446276]\n",
      " [ 0.86270704 -0.64426033  1.98798016 -0.82591445 -0.24048806 -0.10600849\n",
      "   1.49507224 -0.62041518  0.1753965  -1.35948718]\n",
      " [-0.87054276 -0.16610892 -0.72700421  1.25727616  0.69112453 -0.72581363\n",
      "  -0.34268559 -0.08943292 -1.07181379 -0.88667207]\n",
      " [ 0.84000859 -1.34853052  0.18606458 -0.15302866 -0.43935007  1.2407873\n",
      "  -0.45009704  0.75279305  0.14540684 -1.70121639]\n",
      " [-0.11609372  0.38242081 -0.4006153   1.5709845   0.61559163  1.05404329\n",
      "  -0.64900393 -1.99388633  0.07495446 -0.2431281 ]\n",
      " [ 0.18599669  0.7386674  -0.61888697  0.58111699  0.47827546 -0.03025682\n",
      "  -0.56950803 -0.01848738 -0.0463772  -0.07039535]\n",
      " [-1.0230011  -0.72050413 -1.32661279 -0.03016345  0.60687215 -1.136144\n",
      "  -1.78557353  0.58456567 -0.32864403  0.56322594]]\n",
      "\n",
      "Initial bandit variance\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
      "\n",
      "Final bandit mean\n",
      "[[ 0.60426908  0.71013536  1.67843318 -0.38063677  1.60770027 -2.77907294\n",
      "  -0.67419525 -0.50793038  0.04206418  1.1574027 ]\n",
      " [-0.311497    0.83545909 -0.25031081 -0.21860929 -1.74620777 -0.39625977\n",
      "   0.45771381 -0.55242915  1.80593312  0.99527269]\n",
      " [-1.24931576  0.35919432 -0.20835868  0.31742521  0.15769163 -1.05248348\n",
      "  -0.81054594  0.90407078  0.01857262 -0.07778636]\n",
      " [-0.27133349 -1.20111166 -0.1620291  -1.04846876 -0.67975252  0.09537101\n",
      "   0.47807026  0.29692736 -0.48500345  0.04520906]\n",
      " [-0.14225304 -2.10884678 -2.57765606 -0.28477088  0.01299312 -0.6906667\n",
      "   1.49004639  0.16037487  1.51419611  1.47920818]\n",
      " [-0.49331428 -0.09994647  0.0713594  -0.31587316 -0.0596153   1.06495733\n",
      "  -1.51274414  0.48529402 -0.76439375 -0.22032743]\n",
      " [ 1.02713018 -0.18335589 -0.83597598 -0.45903789 -1.01971395  0.34361427\n",
      "   0.70229572  0.35269642 -0.31710731 -0.4475855 ]\n",
      " [ 2.65015343 -0.93811692  0.21518725 -0.35801509  0.57595017 -1.28085811\n",
      "   0.72660096 -0.28635116  0.30811148  0.33119161]]\n",
      "\n",
      "Final bandit variance\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Final state-action value function\n",
      "[[ 0.67046327 -0.07479404 -0.09020653 -0.42590122 -0.11183075 -0.15540686\n",
      "  -0.27864911 -0.24761815 -0.07816563  0.05618519]\n",
      " [-0.32529948  0.37396582 -0.87143743 -0.11535624 -1.75682873 -2.46920425\n",
      "  -0.18686463 -0.17971857 -0.10375208  0.65068395]\n",
      " [-0.37458815 -1.40690601 -0.27677719 -0.68884227  0.21519284 -0.22421717\n",
      "  -0.34391899 -1.1212805  -1.32735571 -0.87012501]\n",
      " [-0.65213515 -0.68581792  0.07138197 -0.22043023 -0.6593647   0.0638535\n",
      "  -0.23283582 -0.93148028 -0.30723287 -0.03703339]\n",
      " [-0.3339466  -0.56409268 -0.14268075 -0.8367032  -2.20685683 -0.23763395\n",
      "  -0.24099352 -2.04774015  1.26475709 -1.60178388]\n",
      " [-0.03251924 -0.08099515 -0.90752518 -0.00861148  0.18498423  0.64946708\n",
      "   0.04039473 -0.06064795 -0.1241865   0.        ]\n",
      " [-0.96184403 -0.45258117 -0.81174822 -0.40088737 -0.79286912 -1.03052252\n",
      "  -0.43869146 -0.72607757 -0.25413508 -0.59667178]\n",
      " [-0.33361575 -0.3309338  -0.37553206  0.03561694 -0.06521602  0.05438908\n",
      "  -1.1052917   0.15878808  0.05168006 -0.11137928]]\n",
      "\n",
      "Final policy\n",
      "[[0.9        0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.9       ]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.9        0.01111111\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.9        0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.9        0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.9\n",
      "  0.01111111 0.01111111 0.01111111 0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.01111111 0.9        0.01111111]\n",
      " [0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      "  0.01111111 0.9        0.01111111 0.01111111]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial bandit mean\")\n",
    "print(bandit_mean)\n",
    "\n",
    "print(\"\\nInitial bandit variance\")\n",
    "print(bandit_variance)\n",
    "\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run on policy temporal difference sarsa\n",
    "bandit_mean, bandit_variance, state_action_value_function, policy = stochastic_multi_armed_contextual_bandits(number_of_iterations, number_of_states, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, state_action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal bandit mean\")\n",
    "print(bandit_mean)\n",
    "\n",
    "print(\"\\nFinal bandit variance\")\n",
    "print(bandit_variance)\n",
    "\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
