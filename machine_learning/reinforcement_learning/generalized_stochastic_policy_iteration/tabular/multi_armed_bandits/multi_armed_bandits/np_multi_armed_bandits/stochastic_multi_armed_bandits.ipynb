{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bandits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00007121, -1.29275786,  0.42196963,  1.55721517, -0.67232445,\n",
       "       -0.11010087,  2.54621684, -0.23408509,  0.0900666 ,  0.22306781])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_bandit_mean_mean = 0.0\n",
    "global_bandit_mean_variance = 1.0\n",
    "\n",
    "bandit_mean = np.random.normal(loc = global_bandit_mean_mean, scale = np.sqrt(global_bandit_mean_variance), size = number_of_bandits)\n",
    "bandit_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_bandit_variance_mean = 1.0\n",
    "global_bandit_variance_variance = 0.0\n",
    "\n",
    "bandit_variance = np.random.normal(loc = global_bandit_variance_mean, scale = np.sqrt(global_bandit_variance_variance), size = number_of_bandits)\n",
    "bandit_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 201, 201, 201, 201, 201, 201, 201, 201, 201])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_stochastic_change_frequencies = np.repeat(a = 201, repeats = number_of_bandits)\n",
    "bandit_stochastic_change_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_stochastic_change_counter = np.zeros(shape = [number_of_bandits], dtype = np.int64)\n",
    "bandit_stochastic_change_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of iterations\n",
    "number_of_iterations = 2000\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.1\n",
    "# Set action selection type (greedy, epsilon-greedy, upper-confidence-bound)\n",
    "action_selection_type = 1\n",
    "# Set action value update type (sample-average, biased constant step-size, unbiased constant step-size)\n",
    "action_value_update_type = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_value_function = np.zeros(shape = [number_of_bandits], dtype = np.float64)\n",
    "action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_count = np.zeros(shape = [number_of_bandits], dtype = np.int64)\n",
    "action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_trace = np.zeros(shape = [number_of_bandits], dtype = np.float64)\n",
    "action_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.repeat(a = 1.0 / number_of_bandits, repeats = number_of_bandits)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through iterations and updates the policy\n",
    "def loop_through_iterations(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type):\n",
    "    # Loop through iterations until termination\n",
    "    for t in range(0, number_of_iterations):\n",
    "        # Choose policy by epsilon-greedy choosing from the action-value function\n",
    "        policy = update_policy_from_action_value_function(number_of_bandits, action_value_function, action_count, t + 1, epsilon, action_selection_type, policy);\n",
    "\n",
    "        # Get action\n",
    "        action_index = np.random.choice(a = number_of_bandits, p = policy)\n",
    "\n",
    "        # Get reward from action\n",
    "        reward = np.random.normal(loc = bandit_mean[action_index], scale = np.sqrt(bandit_variance[action_index]))\n",
    "\n",
    "        # Update action count\n",
    "        action_count[action_index] += 1\n",
    "\n",
    "        # Update action-value function\n",
    "        if action_value_update_type == 0: # sample-average method\n",
    "            action_value_function[action_index] += (1.0 / action_count[action_index]) * (reward - action_value_function[action_index]);\n",
    "        elif action_value_update_type == 1: # biased constant step-size\n",
    "            action_value_function[action_index] += alpha * (reward - action_value_function[action_index]);\n",
    "        elif action_value_update_type == 2: # unbiased constant step-size\n",
    "            # Update action trace\n",
    "            action_trace[action_index] += alpha * (1.0 - action_trace[action_index]);\n",
    "\n",
    "            action_value_function[action_index] += (alpha / action_trace[action_index]) * (reward - action_value_function[action_index]);\n",
    "\n",
    "        # Mutate bandit statistics\n",
    "        for i in range(number_of_bandits):\n",
    "            if bandit_stochastic_change_frequencies[i] > 0:\n",
    "                bandit_stochastic_change_counter[i] += 1\n",
    "\n",
    "                if bandit_stochastic_change_counter[i] == bandit_stochastic_change_frequencies[i]:\n",
    "                    bandit_mean[i] = np.random.normal(loc = global_bandit_mean_mean, scale = np.sqrt(global_bandit_mean_variance))\n",
    "                    bandit_variance[i] = np.random.normal(loc = global_bandit_variance_mean, scale = np.sqrt(global_bandit_variance_variance))\n",
    "\n",
    "                    bandit_stochastic_change_counter[i] = 0\n",
    "\n",
    "    return bandit_mean, bandit_variance, action_value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates policy as some function of action-value function\n",
    "def update_policy_from_action_value_function(number_of_bandits, action_value_function, action_count, iteration_count, epsilon, action_selection_type, policy):\n",
    "    # Calculate action value depending on action selection type\n",
    "    if action_selection_type == 0 or action_selection_type == 1: # greedy or epsilon-greedy\n",
    "        action_value = action_value_function[:]\n",
    "    elif action_selection_type == 2: # upper-confidence-bound\n",
    "        min_count_idx = np.argmin(a = action_value_function)\n",
    "        if min_count_idx == 0:\n",
    "            policy = np.where(np.arange(number_of_bandits) == min_count_idx, 1.0, 0.0)\n",
    "            return policy\n",
    "        else:\n",
    "            action_value = action_value_function + epsilon * np.sqrt(np.log(iteration_count) / action_count)\n",
    "    \n",
    "    # Save max action value and find the number of actions that have the same max action value\n",
    "    max_action_value = np.max(a = action_value_function)\n",
    "    max_action_count = np.count_nonzero(a = action_value_function == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if action_selection_type == 1: # epsilon-greedy\n",
    "        if max_action_count == number_of_bandits:\n",
    "            max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "            remaining_apportioned_probability_per_action = 0.0\n",
    "        else:\n",
    "            max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "            remaining_apportioned_probability_per_action = epsilon / (number_of_bandits - max_action_count)\n",
    "    elif action_selection_type == 0 or action_selection_type == 2: # greedy or upper-confidence-bound\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "\n",
    "    policy = np.where(action_value == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_multi_armed_banditsloop_through_iterations(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type):\n",
    "    # Loop through iterations and update the policy\n",
    "    bandit_mean, bandit_variance, action_value_function, policy = loop_through_iterations(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type)\n",
    "    \n",
    "    return bandit_mean, bandit_variance, action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial bandit mean\n",
      "[-1.00007121 -1.29275786  0.42196963  1.55721517 -0.67232445 -0.11010087\n",
      "  2.54621684 -0.23408509  0.0900666   0.22306781]\n",
      "\n",
      "Initial bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Initial action value function\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Initial policy\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "\n",
      "Final bandit mean\n",
      "[ 0.01430227 -1.07617476 -1.07668153 -0.81885676 -0.64743395 -0.23869974\n",
      "  0.23653173  0.01623212  1.25593101 -0.67111337]\n",
      "\n",
      "Final bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Final action value function\n",
      "[-0.30564655 -0.66879949 -0.50445807 -0.3821192  -0.30001687 -0.24570694\n",
      "  0.31200227 -0.24441839 -0.31581366 -0.13650139]\n",
      "\n",
      "Final policy\n",
      "[0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      " 0.9        0.01111111 0.01111111 0.01111111]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial bandit mean\")\n",
    "print(bandit_mean)\n",
    "\n",
    "print(\"\\nInitial bandit variance\")\n",
    "print(bandit_variance)\n",
    "\n",
    "print(\"\\nInitial action value function\")\n",
    "print(action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run on policy temporal difference sarsa\n",
    "bandit_mean, bandit_variance, action_value_function, policy = stochastic_multi_armed_banditsloop_through_iterations(number_of_iterations, number_of_bandits, bandit_mean, bandit_variance, bandit_stochastic_change_frequencies, bandit_stochastic_change_counter, global_bandit_mean_mean, global_bandit_mean_variance, global_bandit_variance_mean, global_bandit_variance_variance, action_value_function, action_count, action_trace, policy, alpha, epsilon, action_selection_type, action_value_update_type)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal bandit mean\")\n",
    "print(bandit_mean)\n",
    "\n",
    "print(\"\\nFinal bandit variance\")\n",
    "print(bandit_variance)\n",
    "\n",
    "print(\"\\nFinal action value function\")\n",
    "print(action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
