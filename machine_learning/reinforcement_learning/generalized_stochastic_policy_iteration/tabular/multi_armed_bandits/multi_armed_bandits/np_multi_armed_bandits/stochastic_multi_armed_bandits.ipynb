{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits: Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_num_bandits():\n",
    "    \"\"\"Creates environment number of bandits.\n",
    "\n",
    "    Returns:\n",
    "        num_bandits: int, number of bandits.\n",
    "    \"\"\"\n",
    "    num_bandits = 10\n",
    "\n",
    "    return num_bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_means(num_bandits):\n",
    "    \"\"\"Creates environment bandit means.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "    \"\"\"\n",
    "    global_bandit_mean_mean = 0.0\n",
    "    global_bandit_mean_variance = 1.0\n",
    "\n",
    "    bandit_mean = np.random.normal(\n",
    "        loc=global_bandit_mean_mean,\n",
    "        scale=np.sqrt(global_bandit_mean_variance),\n",
    "        size=num_bandits)\n",
    "\n",
    "    return global_bandit_mean_mean, global_bandit_mean_variance, bandit_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_variances(num_bandits):\n",
    "    \"\"\"Creates environment bandit variances.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "    \"\"\"\n",
    "    global_bandit_variance_mean = 1.0\n",
    "    global_bandit_variance_variance = 0.0\n",
    "\n",
    "    bandit_variance = np.random.normal(\n",
    "        loc=global_bandit_variance_mean,\n",
    "        scale=np.sqrt(global_bandit_variance_variance),\n",
    "        size=num_bandits)\n",
    "\n",
    "    return (global_bandit_variance_mean,\n",
    "            global_bandit_variance_variance,\n",
    "            bandit_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_bandit_change_arrays(num_bandits):\n",
    "    \"\"\"Creates environment bandit change arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "    \"\"\"\n",
    "    bandit_change_frequencies = np.repeat(\n",
    "        a=201, repeats=num_bandits)\n",
    "\n",
    "    bandit_change_counter = np.zeros(\n",
    "        shape=[num_bandits], dtype=np.int64)\n",
    "\n",
    "    return (bandit_change_frequencies,\n",
    "            bandit_change_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    \"\"\"Creates environment.\n",
    "\n",
    "    Returns:\n",
    "        num_bandits: int, number of bandits.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "    \"\"\"\n",
    "    num_bandits = create_environment_num_bandits()\n",
    "\n",
    "    (global_bandit_mean_mean,\n",
    "     global_bandit_mean_variance,\n",
    "     bandit_mean) = create_environment_bandit_means(num_bandits)\n",
    "\n",
    "    (global_bandit_variance_mean,\n",
    "     global_bandit_variance_variance,\n",
    "     bandit_variance) = create_environment_bandit_variances(num_bandits)\n",
    "\n",
    "    (bandit_change_frequencies,\n",
    "     bandit_change_counter) = create_environment_bandit_change_arrays(\n",
    "        num_bandits)\n",
    "\n",
    "    return (num_bandits,\n",
    "            global_bandit_mean_mean,\n",
    "            global_bandit_mean_variance,\n",
    "            bandit_mean,\n",
    "            global_bandit_variance_mean,\n",
    "            global_bandit_variance_variance,\n",
    "            bandit_variance,\n",
    "            bandit_change_frequencies,\n",
    "            bandit_change_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        num_iterations: int, number of iterations.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        action_value_update_type: int, action value update type (\n",
    "            sample-average, biased constant step-size, unbiased constant\n",
    "            step-size).\n",
    "    \"\"\"\n",
    "    num_iterations = 2000\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.1\n",
    "    action_selection_type = 1\n",
    "    action_value_update_type = 2\n",
    "\n",
    "    return (num_iterations,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            action_selection_type,\n",
    "            action_value_update_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_arrays(num_bandits):\n",
    "    \"\"\"Creates action arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        action_value_function: array[float], keeps track of the estimated\n",
    "            value of each bandit.\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        action_trace: array[float], keeps track of the reward trace for each\n",
    "            bandit.\n",
    "    \"\"\"\n",
    "    action_value_function = np.zeros(shape=[num_bandits], dtype=np.float64)\n",
    "\n",
    "    action_count = np.zeros(shape=[num_bandits], dtype=np.int64)\n",
    "\n",
    "    action_trace = np.zeros(shape=[num_bandits], dtype=np.float64)\n",
    "\n",
    "    return action_value_function, action_count, action_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_bandits):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    policy = np.repeat(a=1.0 / num_bandits, repeats=num_bandits)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_iterations(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_value_function,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type):\n",
    "    \"\"\"Loops through iterations to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: int, number of iterations.\n",
    "        num_bandits: int, number of bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        action_value_function: array[float], keeps track of the estimated\n",
    "            value of each bandit.\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        action_trace: array[float], keeps track of the reward trace for each\n",
    "            bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        action_value_update_type: int, action value update type (\n",
    "            sample-average, biased constant step-size, unbiased constant\n",
    "            step-size).\n",
    "    Returns:\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        action_value_function: array[float], keeps track of the estimated\n",
    "            value of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    # Loop through iterations until termination\n",
    "    for t in range(0, num_iterations):\n",
    "        # Choose policy by epsilon-greedy choosing from action-value function\n",
    "        policy = update_policy_from_action_value_function(\n",
    "            num_bandits,\n",
    "            action_value_function,\n",
    "            action_count,\n",
    "            t + 1,\n",
    "            epsilon,\n",
    "            action_selection_type,\n",
    "            policy)\n",
    "\n",
    "        # Get action\n",
    "        a_idx = np.random.choice(a=num_bandits, p=policy)\n",
    "\n",
    "        # Get reward from action\n",
    "        reward = np.random.normal(\n",
    "            loc=bandit_mean[a_idx],\n",
    "            scale=np.sqrt(bandit_variance[a_idx]))\n",
    "\n",
    "        # Update action count\n",
    "        action_count[a_idx] += 1\n",
    "\n",
    "        # Update action-value function\n",
    "        if action_value_update_type == 0:  # sample-average method\n",
    "            learning_rate = 1.0 / action_count[a_idx]\n",
    "            delta = reward - action_value_function[a_idx]\n",
    "            action_value_function[a_idx] += learning_rate * delta\n",
    "        elif action_value_update_type == 1:  # biased constant step-size\n",
    "            delta = reward - action_value_function[a_idx]\n",
    "            action_value_function[a_idx] += alpha * delta\n",
    "        elif action_value_update_type == 2:  # unbiased constant step-size\n",
    "            # Update action trace\n",
    "            update = 1.0 - action_trace[a_idx]\n",
    "            action_trace[a_idx] += alpha * update\n",
    "            learning_rate = alpha / action_trace[a_idx]\n",
    "            delta = reward - action_value_function[a_idx]\n",
    "            action_value_function[a_idx] += learning_rate * delta\n",
    "\n",
    "        # Mutate bandit statistics\n",
    "        for i in range(num_bandits):\n",
    "            if bandit_change_frequencies[i] > 0:\n",
    "                bandit_change_counter[i] += 1\n",
    "\n",
    "                if bandit_change_counter[i] == bandit_change_frequencies[i]:\n",
    "                    bandit_mean[i] = np.random.normal(\n",
    "                        loc=global_bandit_mean_mean,\n",
    "                        scale=np.sqrt(global_bandit_mean_variance))\n",
    "                    bandit_variance[i] = np.random.normal(\n",
    "                        loc=global_bandit_variance_mean,\n",
    "                        scale=np.sqrt(global_bandit_variance_variance))\n",
    "\n",
    "                    bandit_change_counter[i] = 0\n",
    "\n",
    "    return bandit_mean, bandit_variance, action_value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_from_action_value_function(\n",
    "        num_bandits,\n",
    "        action_value_function,\n",
    "        action_count,\n",
    "        iteration_count,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        policy):\n",
    "    \"\"\"Updates policy as some function of action-value function.\n",
    "\n",
    "    Args:\n",
    "        num_bandits: int, number of bandits.\n",
    "        action_value_function: array[float], keeps track of the estimated\n",
    "            value of each bandit.\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        iteration_count: int, current loop iteration count.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    # Calculate action value depending on action selection type\n",
    "    if action_selection_type == 0 or action_selection_type == 1:\n",
    "        # Greedy or epsilon-greedy\n",
    "        action_value = action_value_function[:]\n",
    "    elif action_selection_type == 2:\n",
    "        # Upper-confidence-bound\n",
    "        min_count_idx = np.argmin(a=action_value_function)\n",
    "        if min_count_idx == 0:\n",
    "            policy = np.where(\n",
    "                np.arange(num_bandits) == min_count_idx, 1.0, 0.0)\n",
    "            return policy\n",
    "        else:\n",
    "            action_value = action_value_function + epsilon * np.sqrt(\n",
    "                np.log(iteration_count) / action_count)\n",
    "\n",
    "    # Save max action value and find the number of actions that have the same\n",
    "    # max action value\n",
    "    max_action_value = np.max(a=action_value_function)\n",
    "    max_action_count = np.count_nonzero(\n",
    "        a=action_value_function == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    if action_selection_type == 1:\n",
    "        # Epsilon-greedy\n",
    "        if max_action_count == num_bandits:\n",
    "            max_policy_prob_per_action = 1.0 / max_action_count\n",
    "            remain_prob_per_action = 0.0\n",
    "        else:\n",
    "            max_policy_prob_per_action = (1.0 - epsilon) / max_action_count\n",
    "            remain_prob_per_action = epsilon / (num_bandits - max_action_count)\n",
    "    elif action_selection_type == 0 or action_selection_type == 2:\n",
    "        # Greedy or upper-confidence-bound\n",
    "        max_policy_prob_per_action = 1.0 / max_action_count\n",
    "        remain_prob_per_action = 0.0\n",
    "\n",
    "    policy = np.where(\n",
    "        action_value == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        remain_prob_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_multi_armed_bandits(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_value_function,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type):\n",
    "    \"\"\"Loops through iterations to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: int, number of iterations.\n",
    "        num_bandits: int, number of bandits.\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        bandit_change_frequencies: array[int], how often each\n",
    "            bandit's statistics changes.\n",
    "        bandit_change_counter: array[int], the change\n",
    "            counter of each bandit.\n",
    "        global_bandit_mean_mean: float, the global mean of means across all\n",
    "            bandits.\n",
    "        global_bandit_mean_variance: float, the global variance of means\n",
    "            across all bandits.\n",
    "        global_bandit_variance_mean: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        global_bandit_variance_variance: float, the global variance of variances\n",
    "            across all bandits.\n",
    "        action_value_function: array[float], keeps track of the estimated\n",
    "            value of each bandit.\n",
    "        action_count: array[int], counts the number of times each bandit was\n",
    "            actioned.\n",
    "        action_trace: array[float], keeps track of the reward trace for each\n",
    "            bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        action_selection_type: int, action selection type (greedy,\n",
    "            epsilon-greedy, upper-confidence-bound).\n",
    "        action_value_update_type: int, action value update type (\n",
    "            sample-average, biased constant step-size, unbiased constant\n",
    "            step-size).\n",
    "    Returns:\n",
    "        bandit_mean: array[float], the means of each bandit.\n",
    "        bandit_variance: array[float], the variances of each bandit.\n",
    "        action_value_function: array[float], keeps track of the estimated\n",
    "            value of each bandit.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            bandit to action.\n",
    "    \"\"\"\n",
    "    (bandit_mean,\n",
    "     bandit_variance,\n",
    "     action_value_function,\n",
    "     policy) = loop_through_iterations(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_value_function,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type)\n",
    "\n",
    "    return bandit_mean, bandit_variance, action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\"\"\"\n",
    "    (num_bandits,\n",
    "     global_bandit_mean_mean,\n",
    "     global_bandit_mean_variance,\n",
    "     bandit_mean,\n",
    "     global_bandit_variance_mean,\n",
    "     global_bandit_variance_variance,\n",
    "     bandit_variance,\n",
    "     bandit_change_frequencies,\n",
    "     bandit_change_counter) = create_environment()\n",
    "\n",
    "    (num_iterations,\n",
    "     alpha,\n",
    "     epsilon,\n",
    "     action_selection_type,\n",
    "     action_value_update_type) = set_hyperparameters()\n",
    "\n",
    "    (action_value_function,\n",
    "     action_count,\n",
    "     action_trace) = create_action_arrays(num_bandits)\n",
    "\n",
    "    policy = create_policy_arrays(num_bandits)\n",
    "\n",
    "    # Print initial arrays\n",
    "    print(\"\\nInitial bandit mean\")\n",
    "    print(bandit_mean)\n",
    "\n",
    "    print(\"\\nInitial bandit variance\")\n",
    "    print(bandit_variance)\n",
    "\n",
    "    print(\"\\nInitial action value function\")\n",
    "    print(action_value_function)\n",
    "\n",
    "    print(\"\\nInitial policy\")\n",
    "    print(policy)\n",
    "\n",
    "    # Run on policy temporal difference sarsa\n",
    "    (bandit_mean,\n",
    "     bandit_variance,\n",
    "     action_value_function,\n",
    "     policy) = stochastic_multi_armed_bandits(\n",
    "        num_iterations,\n",
    "        num_bandits,\n",
    "        bandit_mean,\n",
    "        bandit_variance,\n",
    "        bandit_change_frequencies,\n",
    "        bandit_change_counter,\n",
    "        global_bandit_mean_mean,\n",
    "        global_bandit_mean_variance,\n",
    "        global_bandit_variance_mean,\n",
    "        global_bandit_variance_variance,\n",
    "        action_value_function,\n",
    "        action_count,\n",
    "        action_trace,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        action_selection_type,\n",
    "        action_value_update_type)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal bandit mean\")\n",
    "    print(bandit_mean)\n",
    "\n",
    "    print(\"\\nFinal bandit variance\")\n",
    "    print(bandit_variance)\n",
    "\n",
    "    print(\"\\nFinal action value function\")\n",
    "    print(action_value_function)\n",
    "\n",
    "    print(\"\\nFinal policy\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial bandit mean\n",
      "[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n",
      "\n",
      "Initial bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Initial action value function\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Initial policy\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "\n",
      "Final bandit mean\n",
      "[-1.07668153 -0.81885676 -0.64743395 -0.23869974  0.23653173  0.01623212\n",
      "  1.25593101 -0.67111337  0.92955301  0.31983224]\n",
      "\n",
      "Final bandit variance\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Final action value function\n",
      "[-0.67775581 -0.7097237  -0.16982074 -0.26023497 -0.28153331 -0.35250613\n",
      " -0.64989934 -0.41892227  1.1678523  -0.04797865]\n",
      "\n",
      "Final policy\n",
      "[0.01111111 0.01111111 0.01111111 0.01111111 0.01111111 0.01111111\n",
      " 0.01111111 0.01111111 0.9        0.01111111]\n"
     ]
    }
   ],
   "source": [
    "run_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
