{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 16\n",
    "num_terminal_states = 2\n",
    "num_non_terminal_states = num_states - num_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions_per_non_terminal_state = np.repeat(\n",
    "    a=max_num_actions, repeats=num_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state_action_successor_states = np.repeat(\n",
    "    a=1, repeats=num_states * max_num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state_action_successor_states = np.reshape(\n",
    "    a=num_state_action_successor_states,\n",
    "    newshape=(num_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_idx = np.array(\n",
    "    object=[1, 0, 14, 4,\n",
    "            2, 1, 0, 5,\n",
    "            2, 2, 1, 6,\n",
    "            4, 14, 3, 7,\n",
    "            5, 0, 3, 8,\n",
    "            6, 1, 4, 9,\n",
    "            6, 2, 5, 10,\n",
    "            8, 3, 7, 11,\n",
    "            9, 4, 7, 12,\n",
    "            10, 5, 8, 13,\n",
    "            10, 6, 9, 15,\n",
    "            12, 7, 11, 11,\n",
    "            13, 8, 11, 12,\n",
    "            15, 9, 12, 13],\n",
    "    dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.repeat(\n",
    "    a=1.0, repeats=num_non_terminal_states * max_num_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.repeat(\n",
    "    a=-1.0, repeats=num_non_terminal_states * max_num_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_idx = np.reshape(\n",
    "    a=sp_idx,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "p = np.reshape(\n",
    "    a=p,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "r = np.reshape(\n",
    "    a=r,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.0\n",
    "convergence_threshold = 0.001\n",
    "maximum_num_value_estimations = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function estimates the value functions\n",
    "def value_estimation(\n",
    "    num_non_terminal_states,\n",
    "    sp_idx_tensor,\n",
    "    p,\n",
    "    r_tensor,\n",
    "    convergence_threshold,\n",
    "    gamma,\n",
    "    maximum_num_value_estimations,\n",
    "    v_tensor,\n",
    "    q_tensor):\n",
    "    delta = np.finfo(np.float64).max\n",
    "    num_value_estimations = 0\n",
    "\n",
    "    def while_loop_condition(\n",
    "        delta,\n",
    "        num_value_estimations,\n",
    "        v_tensor,\n",
    "        q_tensor):\n",
    "        return tf.logical_and(\n",
    "            x=tf.greater_equal(x=delta, y=convergence_threshold),\n",
    "            y=tf.less(\n",
    "                x=num_value_estimations,\n",
    "                y=maximum_num_value_estimations))\n",
    "\n",
    "    def while_loop_body(\n",
    "        delta,\n",
    "        num_value_estimations,\n",
    "        v_tensor,\n",
    "        q_tensor):\n",
    "        def value_non_terminal_state_for_loop(\n",
    "            state_index,\n",
    "            delta,\n",
    "            num_value_estimations,\n",
    "            v_tensor,\n",
    "            q_tensor):\n",
    "            # Cache state-value function for state state_index\n",
    "            temp_v = tf.gather(\n",
    "                params=v_tensor, indices=state_index)\n",
    "\n",
    "            # Gather state action successor state slices\n",
    "            sp_idx_tensor_slice = tf.gather(\n",
    "                params=sp_idx_tensor,\n",
    "                indices=state_index)\n",
    "            p_tensor_slice = tf.gather(\n",
    "                params=p_tensor,\n",
    "                indices=state_index)\n",
    "            r_tensor_slice = tf.gather(\n",
    "                params=r_tensor,\n",
    "                indices=\n",
    "                state_index)\n",
    "\n",
    "            # Update state-action value function based on\n",
    "            # successor states, transition probabilities, and rewards\n",
    "            x = p_tensor_slice * (r_tensor_slice + gamma * temp_v)\n",
    "            y = p_tensor_slice * (r_tensor_slice + gamma * tf.gather(\n",
    "                params=v_tensor, indices=sp_idx_tensor_slice))\n",
    "\n",
    "            q_tensor_updated = tf.squeeze(\n",
    "                input=tf.where(\n",
    "                    condition=sp_idx_tensor_slice == state_index,\n",
    "                    x=x, y=y),\n",
    "                axis = 1)\n",
    "\n",
    "            # Update state-value function\n",
    "            v_tensor_updated = tf.reduce_max(\n",
    "                input_tensor=q_tensor_updated)\n",
    "\n",
    "            # Update delta for convergence criteria to\n",
    "            # break while loop and update policy\n",
    "            delta = tf.reduce_max(\n",
    "                input_tensor=(delta,\n",
    "                              tf.abs(x=temp_v - v_tensor_updated)))\n",
    "\n",
    "            return (v_tensor_updated,\n",
    "                    q_tensor_updated)\n",
    "\n",
    "        # Replace non-terminal state for loop with map function\n",
    "        (v_tensor,\n",
    "         q_tensor) = tf.map_fn(\n",
    "            fn=lambda x: value_non_terminal_state_for_loop(\n",
    "                x,\n",
    "                delta,\n",
    "                num_value_estimations,\n",
    "                v_tensor,\n",
    "                q_tensor), \n",
    "            elems=tf.range(num_non_terminal_states), \n",
    "            dtype=(tf.float64, tf.float64))\n",
    "\n",
    "        # Concat terminal state values back onto state value function\n",
    "        v_tensor = tf.concat(\n",
    "            values=[v_tensor,\n",
    "                    tf.constant(\n",
    "                        value=0.0,\n",
    "                        shape=[num_terminal_states],\n",
    "                        dtype=tf.float64)],\n",
    "            axis=0)\n",
    "\n",
    "        num_value_estimations += 1\n",
    "\n",
    "        return (delta,\n",
    "                num_value_estimations,\n",
    "                v_tensor,\n",
    "                q_tensor)\n",
    "\n",
    "    (delta,\n",
    "     num_value_estimations,\n",
    "     v_tensor,\n",
    "     q_tensor) = tf.while_loop(\n",
    "        cond=while_loop_condition,\n",
    "        body=while_loop_body,\n",
    "        loop_vars=[delta,\n",
    "                   num_value_estimations,\n",
    "                   v_tensor,\n",
    "                   q_tensor])\n",
    "\n",
    "    return v_tensor, q_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function greedily selects the policy based on the current value function\n",
    "def greedy_policy_selection(\n",
    "    sp_idx_tensor,\n",
    "    p_tensor,\n",
    "    r_tensor,\n",
    "    policy_tensor,\n",
    "    gamma,\n",
    "    v_tensor):\n",
    "    def policy_non_terminal_state_for_loop(state_index, policy_tensor):\n",
    "        # Gather state action successor state slices\n",
    "        sp_idx_tensor_slice = tf.gather(\n",
    "            params=sp_idx_tensor,\n",
    "            indices=state_index)\n",
    "        p_tensor_slice = tf.gather(\n",
    "            params=p_tensor,\n",
    "            indices=state_index)\n",
    "        r_tensor_slice = tf.gather(\n",
    "            params=r_tensor,\n",
    "            indices=state_index)\n",
    "\n",
    "        # Update policy greedily from state-value function\n",
    "        policy_tensor_updated = tf.squeeze(\n",
    "            input=p_tensor_slice * (r_tensor_slice + gamma * tf.gather(\n",
    "                params=v_tensor, indices=sp_idx_tensor_slice)),\n",
    "            axis=1)\n",
    "\n",
    "        # Save max policy value and find the number of actions that have the\n",
    "        # same max policy value\n",
    "        max_policy_value = tf.reduce_max(input_tensor=policy_tensor_updated)\n",
    "        max_policy_count = tf.count_nonzero(\n",
    "            input_tensor=tf.equal(x=policy_tensor_updated, y=max_policy_value))\n",
    "\n",
    "        # Apportion policy probability across ties equally for state-action\n",
    "        # pairs that have the same value and zero otherwise\n",
    "        x = tf.fill(\n",
    "            dims=[max_num_actions],\n",
    "            value=1.0 / tf.cast(max_policy_count, dtype=tf.float64))\n",
    "        y = tf.cast(\n",
    "            tf.fill(\n",
    "                dims=[max_num_actions], value=0.0),\n",
    "            dtype=tf.float64)\n",
    "\n",
    "        policy_tensor_updated = tf.where(\n",
    "            condition=tf.equal(\n",
    "                x=policy_tensor_updated, y=max_policy_value), x=x, y=y)\n",
    "\n",
    "        return policy_tensor_updated\n",
    "\n",
    "    # Replace non-terminal state for loop with map function\n",
    "    policy_tensor = tf.map_fn(\n",
    "        fn=lambda x: policy_non_terminal_state_for_loop(x, policy_tensor),\n",
    "        elems=tf.range(num_non_terminal_states),\n",
    "        dtype=tf.float64)\n",
    "\n",
    "    return policy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    num_non_terminal_states,\n",
    "    sp_idx_tensor,\n",
    "    p_tensor,\n",
    "    r_tensor,\n",
    "    policy_tensor,\n",
    "    convergence_threshold,\n",
    "    gamma,\n",
    "    maximum_num_value_iterations,\n",
    "    v_tensor,\n",
    "    q_tensor):\n",
    "    # Value estimation\n",
    "    v_tensor, q_tensor = value_estimation(\n",
    "        num_non_terminal_states,\n",
    "        sp_idx_tensor,\n",
    "        p_tensor,\n",
    "        r,\n",
    "        convergence_threshold,\n",
    "        gamma,\n",
    "        maximum_num_value_iterations,\n",
    "        v_tensor,\n",
    "        q_tensor)\n",
    "\n",
    "    # Greedy policy selection\n",
    "    policy_tensor = greedy_policy_selection(\n",
    "        sp_idx_tensor,\n",
    "        p_tensor,\n",
    "        r_tensor,\n",
    "        policy_tensor,\n",
    "        gamma,\n",
    "        v_tensor)\n",
    "\n",
    "    return (v_tensor,\n",
    "            q_tensor,\n",
    "            policy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final state value function\n",
      "[-1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.  0.]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-4. -3. -2. -4.]\n",
      " [-4. -4. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -4. -4. -2.]\n",
      " [-4. -2. -3. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -4. -4.]\n",
      " [-2. -4. -4. -3.]\n",
      " [-1. -3. -3. -2.]]\n",
      "\n",
      "Final policy\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Read in environment\n",
    "    sp_idx_tensor = tf.placeholder(\n",
    "        dtype=tf.int64,\n",
    "        shape=[num_non_terminal_states,\n",
    "               max_num_actions,\n",
    "               max_num_state_action_successor_states])\n",
    "    p_tensor = tf.placeholder(\n",
    "        dtype=tf.float64,\n",
    "        shape=[num_non_terminal_states,\n",
    "               max_num_actions,\n",
    "               max_num_state_action_successor_states])\n",
    "    r_tensor = tf.placeholder(\n",
    "        dtype=tf.float64,\n",
    "        shape=[num_non_terminal_states,\n",
    "               max_num_actions,\n",
    "               max_num_state_action_successor_states])\n",
    "\n",
    "    # Create value functions\n",
    "    v_tensor = tf.zeros(\n",
    "        shape=num_states, dtype=tf.float64)\n",
    "    q_tensor = tf.zeros(\n",
    "        shape=[num_non_terminal_states, max_num_actions],\n",
    "        dtype = tf.float64)\n",
    "\n",
    "    # Create policy\n",
    "    policy_tensor = tf.tile(\n",
    "        input=[tf.constant(\n",
    "            value = 1.0 / max_num_actions, dtype = tf.float64)],\n",
    "        multiples=[num_non_terminal_states * max_num_actions])\n",
    "    policy_tensor = tf.reshape(\n",
    "        tensor=policy_tensor,\n",
    "        shape=[num_non_terminal_states, max_num_actions])\n",
    "\n",
    "    # Create algorithm\n",
    "    algorithm = value_iteration(\n",
    "        num_non_terminal_states,\n",
    "        sp_idx_tensor,\n",
    "        p_tensor,\n",
    "        r_tensor,\n",
    "        policy_tensor,\n",
    "        convergence_threshold,\n",
    "        gamma,\n",
    "        maximum_num_value_estimations,\n",
    "        v_tensor,\n",
    "        q_tensor)\n",
    "\n",
    "    # Run graph\n",
    "    (v,\n",
    "     q,\n",
    "     policy) = sess.run(\n",
    "        fetches=algorithm,\n",
    "        feed_dict={\n",
    "            sp_idx_tensor: sp_idx, \n",
    "            p_tensor: p, \n",
    "            r_tensor: r\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\nFinal state value function\")\n",
    "print(v)\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(q)\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
