{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming: Policy Iteration, Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_states():\n",
    "    \"\"\"Creates environment states.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_terminal_states: int, number of terminal states.\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    num_terminal_states = 2\n",
    "    num_non_terminal_states = num_states - num_terminal_states\n",
    "\n",
    "    return num_states, num_terminal_states, num_non_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_actions(num_non_terminal_states):\n",
    "    \"\"\"Creates environment actions.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "\n",
    "    Returns:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_terminal_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "    \"\"\"\n",
    "    max_num_actions = 4\n",
    "\n",
    "    num_actions_per_non_terminal_state = np.repeat(\n",
    "        a=max_num_actions, repeats=num_non_terminal_states)\n",
    "\n",
    "    return max_num_actions, num_actions_per_non_terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_successor_counts(num_states, max_num_actions):\n",
    "    \"\"\"Creates environment successor counts.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        num_state_action_successor_states: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "    \"\"\"\n",
    "    num_state_action_successor_states = np.repeat(\n",
    "        a=1, repeats=num_states * max_num_actions)\n",
    "\n",
    "    num_state_action_successor_states = np.reshape(\n",
    "        a=num_state_action_successor_states,\n",
    "        newshape=(num_states, max_num_actions))\n",
    "\n",
    "    return num_state_action_successor_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_successor_arrays(\n",
    "        num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates environment successor arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    sp_idx = np.array(\n",
    "        object=[1, 0, 14, 4,\n",
    "                2, 1, 0, 5,\n",
    "                2, 2, 1, 6,\n",
    "                4, 14, 3, 7,\n",
    "                5, 0, 3, 8,\n",
    "                6, 1, 4, 9,\n",
    "                6, 2, 5, 10,\n",
    "                8, 3, 7, 11,\n",
    "                9, 4, 7, 12,\n",
    "                10, 5, 8, 13,\n",
    "                10, 6, 9, 15,\n",
    "                12, 7, 11, 11,\n",
    "                13, 8, 11, 12,\n",
    "                15, 9, 12, 13],\n",
    "        dtype=np.int64)\n",
    "\n",
    "    p = np.repeat(\n",
    "        a=1.0, repeats=num_non_terminal_states * max_num_actions * 1)\n",
    "\n",
    "    r = np.repeat(\n",
    "        a=-1.0, repeats=num_non_terminal_states * max_num_actions * 1)\n",
    "\n",
    "    sp_idx = np.reshape(\n",
    "        a=sp_idx,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "    p = np.reshape(\n",
    "        a=p,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "    r = np.reshape(\n",
    "        a=r,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "\n",
    "    return sp_idx, p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    \"\"\"Creates environment.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_terminal_states: int, number of terminal states.\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_terminal_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "        num_state_action_successor_states: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    (num_states,\n",
    "     num_terminal_states,\n",
    "     num_non_terminal_states) = create_environment_states()\n",
    "\n",
    "    (max_num_actions,\n",
    "     num_actions_per_non_terminal_state) = create_environment_actions(\n",
    "        num_non_terminal_states)\n",
    "\n",
    "    num_state_action_successor_states = create_environment_successor_counts(\n",
    "        num_states, max_num_actions)\n",
    "\n",
    "    (sp_idx,\n",
    "     p,\n",
    "     r) = create_environment_successor_arrays(\n",
    "        num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    return (num_states,\n",
    "            num_terminal_states,\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_actions_per_non_terminal_state,\n",
    "            num_state_action_successor_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        convergence_threshold: float, minimum maximum change across all value\n",
    "            function updates.\n",
    "        maximum_num_sweeps: int, max number of outer loop sweeps.\n",
    "        maximum_num_policy_evaluations: int, max number of iterations.\n",
    "    \"\"\"\n",
    "    gamma = 1.0\n",
    "    convergence_threshold = 0.001\n",
    "    maximum_num_sweeps = 30\n",
    "    maximum_num_policy_evaluations = 20\n",
    "\n",
    "    return (gamma,\n",
    "            convergence_threshold,\n",
    "            maximum_num_sweeps,\n",
    "            maximum_num_policy_evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_function_arrays(num_states, num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates value function arrays.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        num_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        v: array, estimate of state value function V(s).\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "    \"\"\"\n",
    "    v = np.zeros(shape=num_states, dtype=np.float64)\n",
    "    q = np.zeros(\n",
    "        shape=(num_non_terminal_states, max_num_actions),\n",
    "        dtype=np.float64)\n",
    "\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        old_policy: array[float], copy of policy to be used for comparison\n",
    "            with learned policy, tracking changes over time.\n",
    "    \"\"\"\n",
    "    policy = np.repeat(\n",
    "        a=1.0 / max_num_actions,\n",
    "        repeats=num_non_terminal_states * max_num_actions)\n",
    "\n",
    "    policy = np.reshape(\n",
    "        a=policy,\n",
    "        newshape=(num_non_terminal_states, max_num_actions))\n",
    "    \n",
    "    old_policy = np.copy(a=policy)\n",
    "\n",
    "    return policy, old_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "        num_non_terminal_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        policy,\n",
    "        convergence_threshold,\n",
    "        gamma,\n",
    "        maximum_num_policy_evaluations,\n",
    "        v,\n",
    "        q):\n",
    "    \"\"\"Evaluates current policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "        convergence_threshold: float, minimum maximum change across all value\n",
    "            function updates.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        maximum_num_policy_evaluations: int, max number of iterations.\n",
    "        v: array[float], keeps track of the estimated value of each state V(s).\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "    Returns:\n",
    "        v: array, estimate of state value function V(s).\n",
    "        q: array, estimate of state-action value function Q(s, a).\n",
    "    \"\"\"\n",
    "    delta = np.finfo(np.float64).max\n",
    "    num_policy_evaluations = 0\n",
    "\n",
    "    while (delta >= convergence_threshold and\n",
    "           num_policy_evaluations < maximum_num_policy_evaluations):\n",
    "        for i in range(0, num_non_terminal_states):\n",
    "            # Cache state-value function for state i\n",
    "            temp_v = v[i]\n",
    "\n",
    "            # Update state-action value function based on successor states,\n",
    "            # transition probabilities, and r\n",
    "            q[i, :] = np.squeeze(\n",
    "                a=np.where(\n",
    "                    sp_idx[i, :, :] == i,\n",
    "                    p[i, :, :] * (r[i, :, :] + gamma * temp_v),\n",
    "                    p[i, :, :] * (r[i, :, :] + gamma * v[sp_idx[i, :, :]])\n",
    "                ),\n",
    "                axis=1)\n",
    "\n",
    "            # Update state value function based on current policy\n",
    "            v[i] = np.sum(\n",
    "                a=policy[i, :] * q[i, :])\n",
    "\n",
    "            # Update delta for convergence criteria to break while loop and\n",
    "            # update policy\n",
    "            delta = np.max(\n",
    "                a=(delta,\n",
    "                   np.abs(temp_v - v[i])))\n",
    "\n",
    "        num_policy_evaluations += 1\n",
    "\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(\n",
    "        num_non_terminal_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        policy,\n",
    "        old_policy,\n",
    "        gamma,\n",
    "        v):\n",
    "    \"\"\"Improves policy greedily based on new value function estimates.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "        old_policy: array[float], previously learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        v: array[float], keeps track of the estimated value of each state V(s).\n",
    "    Returns:\n",
    "        policy_stable: bool, if policy is stable or not (hasn't changed).\n",
    "        policy: array, learned stochastic policy of which action a to take in\n",
    "            state s.\n",
    "    \"\"\"\n",
    "    for i in range(0, num_non_terminal_states):\n",
    "        # Cache policy for comparison later\n",
    "        old_policy = np.copy(a=policy[i, :])\n",
    "\n",
    "        # Update policy greedily from state-value function\n",
    "        policy[i, :] = np.squeeze(\n",
    "            a=p[i, :, :] * (r[i, :, :] + gamma * v[sp_idx[i, :, :]]),\n",
    "            axis=1)\n",
    "\n",
    "        # Save max policy value and find the number of actions that have\n",
    "        # the same max policy value\n",
    "        max_policy_value = np.max(a=policy[i, :])\n",
    "        max_policy_count = np.count_nonzero(\n",
    "            a=policy[i, :] == max_policy_value)\n",
    "\n",
    "        # Apportion policy probability across ties equally for state-action\n",
    "        # pairs that have the same value and zero otherwise\n",
    "        policy[i, :] = np.where(\n",
    "            policy[i, :] == max_policy_value,\n",
    "            1.0 / max_policy_count,\n",
    "            0.0)\n",
    "\n",
    "        # If policy has changed from old policy\n",
    "        policy_stable = np.array_equal(policy[i, :], old_policy)\n",
    "\n",
    "    return policy_stable, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "        num_non_terminal_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        policy,\n",
    "        old_policy,\n",
    "        convergence_threshold,\n",
    "        gamma,\n",
    "        maximum_num_policy_evaluations,\n",
    "        v,\n",
    "        q,\n",
    "        maximum_num_sweeps):\n",
    "    \"\"\"Iterates policy through evaluation and improvement stages.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "        old_policy: array[float], previously learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        convergence_threshold: float, minimum maximum change across all value\n",
    "            function updates.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        maximum_num_policy_evaluations: int, max number of iterations.\n",
    "        v: array[float], keeps track of the estimated value of each state V(s).\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        maximum_num_sweeps: int, max number of outer loop sweeps.\n",
    "    Returns:\n",
    "        v: array, estimate of state value function V(s).\n",
    "        q: array, estimate of state-action value function Q(s, a).\n",
    "        policy: array, learned stochastic policy of which action a to take in\n",
    "            state s.\n",
    "    \"\"\"\n",
    "    policy_stable = False\n",
    "    num_sweeps = 0\n",
    "\n",
    "    while (not policy_stable and num_sweeps < maximum_num_sweeps):\n",
    "        print(\"State value function before sweep {}\".format(num_sweeps))\n",
    "        print(v)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"State-action value function before sweep {}\".format(\n",
    "            num_sweeps))\n",
    "        print(q)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"Policy before sweep {}\".format(num_sweeps))\n",
    "        print(policy)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Policy evaluation\n",
    "        v, q = policy_evaluation(\n",
    "            num_non_terminal_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r,\n",
    "            policy,\n",
    "            convergence_threshold,\n",
    "            gamma,\n",
    "            maximum_num_policy_evaluations,\n",
    "            v,\n",
    "            q)\n",
    "\n",
    "        # Policy improvement\n",
    "        policy_stable, policy = policy_improvement(\n",
    "            num_non_terminal_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r,\n",
    "            policy,\n",
    "            old_policy,\n",
    "            gamma,\n",
    "            v)\n",
    "\n",
    "        print(\"policy_stable = {} at sweep {}\\n\".format(\n",
    "            policy_stable, num_sweeps))\n",
    "\n",
    "        num_sweeps += 1\n",
    "\n",
    "    return v, q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\"\"\"\n",
    "    (num_states,\n",
    "     num_terminal_states,\n",
    "     num_non_terminal_states,\n",
    "     max_num_actions,\n",
    "     num_actions_per_non_terminal_state,\n",
    "     num_state_action_successor_states,\n",
    "     sp_idx,\n",
    "     p,\n",
    "     r) = create_environment()\n",
    "\n",
    "    (gamma,\n",
    "     convergence_threshold,\n",
    "     maximum_num_sweeps,\n",
    "     maximum_num_policy_evaluations) = set_hyperparameters()\n",
    "\n",
    "    v, q = create_value_function_arrays(\n",
    "        num_states, num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    policy, old_policy = create_policy_arrays(\n",
    "        num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    # Run policy iteration\n",
    "    v, q, policy = policy_iteration(\n",
    "        num_non_terminal_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        policy,\n",
    "        old_policy,\n",
    "        convergence_threshold,\n",
    "        gamma,\n",
    "        maximum_num_policy_evaluations,\n",
    "        v,\n",
    "        q,\n",
    "        maximum_num_sweeps)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal state value function\")\n",
    "    print(v)\n",
    "    print(\"\\nFinal state-action value function\")\n",
    "    print(q)\n",
    "    print(\"\\nFinal policy\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State value function before sweep 0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "State-action value function before sweep 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "Policy before sweep 0\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "\n",
      "policy_stable = False at sweep 0\n",
      "\n",
      "State value function before sweep 1\n",
      "[-11.42591538 -16.29940807 -17.92681232 -11.42591538 -14.84125831\n",
      " -16.57033147 -16.60954761 -16.29940807 -16.57033147 -15.10598674\n",
      " -11.83929409 -17.92681232 -16.60954761 -11.83929409   0.\n",
      "   0.        ]\n",
      "\n",
      "\n",
      "State-action value function before sweep 1\n",
      "[[-16.96089737 -12.1904522   -1.         -15.55231196]\n",
      " [-18.55421829 -16.96089737 -12.42591538 -17.25660123]\n",
      " [-18.55421829 -18.55421829 -17.29940807 -17.29940463]\n",
      " [-15.55231196  -1.         -12.1904522  -16.96089737]\n",
      " [-17.25660123 -12.42591538 -12.42591538 -17.25660123]\n",
      " [-17.29940463 -17.29940807 -15.84125831 -15.84125487]\n",
      " [-17.29940463 -18.92681232 -17.57033147 -12.64164201]\n",
      " [-17.25660123 -12.42591538 -16.96089737 -18.55421829]\n",
      " [-15.84125487 -15.84125831 -17.29940807 -17.29940463]\n",
      " [-12.64164201 -17.57033147 -17.57033147 -12.64164201]\n",
      " [-12.64164201 -17.60954761 -16.10598674  -1.        ]\n",
      " [-17.29940463 -17.29940807 -18.55421829 -18.55421829]\n",
      " [-12.64164201 -17.57033147 -18.92681232 -17.29940463]\n",
      " [ -1.         -16.10598674 -17.60954761 -12.64164201]]\n",
      "\n",
      "\n",
      "Policy before sweep 1\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "policy_stable = True at sweep 1\n",
      "\n",
      "\n",
      "Final state value function\n",
      "[-1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.  0.]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-4. -3. -2. -4.]\n",
      " [-4. -4. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -4. -4. -2.]\n",
      " [-4. -2. -3. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -4. -4.]\n",
      " [-2. -4. -4. -3.]\n",
      " [-1. -3. -3. -2.]]\n",
      "\n",
      "Final policy\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "run_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
