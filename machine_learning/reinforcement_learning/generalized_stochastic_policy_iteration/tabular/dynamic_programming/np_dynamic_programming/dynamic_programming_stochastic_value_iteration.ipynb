{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(\n",
    "    a=max_number_of_actions, repeats=number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.repeat(\n",
    "    a=1, repeats=number_of_states * max_number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.reshape(\n",
    "    a=number_of_state_action_successor_states,\n",
    "    newshape=(number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.array(\n",
    "    object=[1, 0, 14, 4,\n",
    "            2, 1, 0, 5,\n",
    "            2, 2, 1, 6,\n",
    "            4, 14, 3, 7,\n",
    "            5, 0, 3, 8,\n",
    "            6, 1, 4, 9,\n",
    "            6, 2, 5, 10,\n",
    "            8, 3, 7, 11,\n",
    "            9, 4, 7, 12,\n",
    "            10, 5, 8, 13,\n",
    "            10, 6, 9, 15,\n",
    "            12, 7, 11, 11,\n",
    "            13, 8, 11, 12,\n",
    "            15, 9, 12, 13],\n",
    "    dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_transition_probabilities = np.repeat(\n",
    "    a=1.0, repeats=number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_rewards = np.repeat(\n",
    "    a=-1.0, repeats=number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.reshape(\n",
    "    a=state_action_successor_state_indices,\n",
    "    newshape=(number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_transition_probabilities = np.reshape(\n",
    "    a=state_action_successor_state_transition_probabilities,\n",
    "    newshape=(number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_rewards = np.reshape(\n",
    "    a=state_action_successor_state_rewards,\n",
    "    newshape=(number_of_non_terminal_states, max_number_of_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_function = np.zeros(shape=number_of_states, dtype=np.float64)\n",
    "state_action_value_function = np.zeros(\n",
    "    shape=(number_of_non_terminal_states, max_number_of_actions),\n",
    "    dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(\n",
    "    a=1.0 / max_number_of_actions,\n",
    "    repeats=number_of_non_terminal_states * max_number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.reshape(\n",
    "    a=policy,\n",
    "    newshape=(number_of_non_terminal_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounting_factor_gamma = 1.0\n",
    "convergence_threshold = 0.001\n",
    "maximum_number_of_value_estimations = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function estimates the value functions\n",
    "def value_estimation(\n",
    "    number_of_non_terminal_states,\n",
    "    state_action_successor_state_indices,\n",
    "    state_action_successor_state_transition_probabilities,\n",
    "    state_action_successor_state_rewards,\n",
    "    convergence_threshold,\n",
    "    discounting_factor_gamma,\n",
    "    maximum_number_of_value_estimations,\n",
    "    state_value_function,\n",
    "    state_action_value_function):\n",
    "    \"\"\"Estimates state-action value function.\n",
    "    \n",
    "    Args:\n",
    "        number_of_non_terminal_states: int, number of non terminal states.\n",
    "        state_action_successor_state_indices: array[int], state indices of new\n",
    "            state s' of taking action a from state s.\n",
    "        state_action_successor_state_transition_probabilities: array[float],\n",
    "            transition probability to go from state s to s' by taking action a.\n",
    "        state_action_successor_state_rewards: array[float], reward from new\n",
    "            state s' from state s by taking action a.\n",
    "        convergence_threshold: float, minimum maximum change across all value\n",
    "            function updates.\n",
    "        discounting_factor_gamma: float, 0 <= gamma <= 1, amount to discount\n",
    "            future rewards.\n",
    "        maximum_number_of_value_estimations: int, max number of iterations.\n",
    "        state_value_function: array[float], keeps track of the estimated\n",
    "            value of each state V(s).\n",
    "        state_action_value_function: array[float], keeps track of the estimated\n",
    "            value of each state-action pair Q(s, a).\n",
    "    Returns:\n",
    "        state_value_function: array, estimate of state value function V(s).\n",
    "        state_action_value_function: array, estimate of state-action value\n",
    "            function Q(s, a).\n",
    "    \"\"\"\n",
    "    delta = np.finfo(np.float64).max\n",
    "    number_of_value_estimations = 0\n",
    "\n",
    "    while (delta >= convergence_threshold and\n",
    "           number_of_value_estimations < maximum_number_of_value_estimations):\n",
    "        for i in range(0, number_of_non_terminal_states):\n",
    "            # Cache state-value function for state i\n",
    "            temp_state_value_function = state_value_function[i]\n",
    "\n",
    "            # Update state-action value function based on successor states,\n",
    "            # transition probabilities, and rewards\n",
    "            state_action_value_function[i, :] = np.squeeze(\n",
    "                a=np.where(\n",
    "                    state_action_successor_state_indices[i, :, :] == i,\n",
    "                    state_action_successor_state_transition_probabilities[i, :, :] * (state_action_successor_state_rewards[i, :, :] + discounting_factor_gamma * temp_state_value_function),\n",
    "                    state_action_successor_state_transition_probabilities[i, :, :] * (state_action_successor_state_rewards[i, :, :] + discounting_factor_gamma * state_value_function[state_action_successor_state_indices[i, :, :]])),\n",
    "                axis=1)\n",
    "\n",
    "            # Update state-value function\n",
    "            state_value_function[i] = np.max(\n",
    "                a=state_action_value_function[i, :])\n",
    "\n",
    "            # Update delta for convergence criteria to break while loop and\n",
    "            # update policy\n",
    "            delta = np.max(\n",
    "                (delta,\n",
    "                 np.abs(temp_state_value_function - state_value_function[i])))\n",
    "            \n",
    "        number_of_value_estimations += 1\n",
    "        \n",
    "    return state_value_function, state_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function greedily selects the policy based on the current value function\n",
    "def greedy_policy_selection(\n",
    "    state_action_successor_state_indices,\n",
    "    state_action_successor_state_transition_probabilities,\n",
    "    state_action_successor_state_rewards,\n",
    "    policy,\n",
    "    discounting_factor_gamma,\n",
    "    state_value_function):\n",
    "    \"\"\"Estimates state-action value function.\n",
    "    \n",
    "    Args:\n",
    "        state_action_successor_state_indices: array[int], state indices of new\n",
    "            state s' of taking action a from state s.\n",
    "        state_action_successor_state_transition_probabilities: array[float],\n",
    "            transition probability to go from state s to s' by taking action a.\n",
    "        state_action_successor_state_rewards: array[float], reward from new\n",
    "            state s' from state s by taking action a.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "        discounting_factor_gamma: float, 0 <= gamma <= 1, amount to discount\n",
    "            future rewards.\n",
    "        state_value_function: array[float], keeps track of the estimated\n",
    "            value of each state V(s).\n",
    "    Returns:\n",
    "        array, learned stochastic policy of which action a to take in state s.\n",
    "    \"\"\"\n",
    "    for i in range(0, number_of_non_terminal_states):\n",
    "        # Update policy greedily from state-value function\n",
    "        policy[i, :] = np.squeeze(\n",
    "            a=state_action_successor_state_transition_probabilities[i, :, :] * (state_action_successor_state_rewards[i, :, :] + discounting_factor_gamma * state_value_function[state_action_successor_state_indices[i, :, :]]),\n",
    "            axis=1)\n",
    "\n",
    "        # Save max policy value and find the number of actions that have the\n",
    "        # same max policy value\n",
    "        max_policy_value = np.max(a=policy[i, :])\n",
    "        max_policy_count = np.count_nonzero(\n",
    "            a=policy[i, :] == max_policy_value)\n",
    "            \n",
    "        # Apportion policy probability across ties equally for state-action\n",
    "        # pairs that have the same value and zero otherwise\n",
    "        policy[i, :] = np.where(\n",
    "            policy[i, :] == max_policy_value, 1.0 / max_policy_count, 0.0)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    number_of_non_terminal_states,\n",
    "    state_action_successor_state_indices,\n",
    "    state_action_successor_state_transition_probabilities,\n",
    "    state_action_successor_state_rewards,\n",
    "    policy,\n",
    "    convergence_threshold,\n",
    "    discounting_factor_gamma,\n",
    "    maximum_number_of_value_iterations,\n",
    "    state_value_function,\n",
    "    state_action_value_function):\n",
    "    \"\"\"Performs value iteration to learn optimal policy.\n",
    "    \n",
    "    Args:\n",
    "        number_of_non_terminal_states: int, number of non terminal states.\n",
    "        state_action_successor_state_indices: array[int], state indices of new\n",
    "            state s' of taking action a from state s.\n",
    "        state_action_successor_state_transition_probabilities: array[float],\n",
    "            transition probability to go from state s to s' by taking action a.\n",
    "        state_action_successor_state_rewards: array[float], reward from new\n",
    "            state s' from state s by taking action a.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "        convergence_threshold: float, minimum maximum change across all value\n",
    "            function updates.\n",
    "        discounting_factor_gamma: float, 0 <= gamma <= 1, amount to discount\n",
    "            future rewards.\n",
    "        maximum_number_of_value_estimations: int, max number of iterations.\n",
    "        state_value_function: array[float], keeps track of the estimated\n",
    "            value of each state V(s).\n",
    "        state_action_value_function: array[float], keeps track of the estimated\n",
    "            value of each state-action pair Q(s, a).\n",
    "    Returns:\n",
    "        state_value_function: array, estimate of state value function V(s).\n",
    "        state_action_value_function: array, estimate of state-action value\n",
    "            function Q(s, a).\n",
    "        policy: array, learned stochastic policy of which action a to take in\n",
    "            state s.\n",
    "    \"\"\"\n",
    "    # Value estimation\n",
    "    state_value_function, state_action_value_function = value_estimation(\n",
    "        number_of_non_terminal_states,\n",
    "        state_action_successor_state_indices,\n",
    "        state_action_successor_state_transition_probabilities,\n",
    "        state_action_successor_state_rewards,\n",
    "        convergence_threshold,\n",
    "        discounting_factor_gamma,\n",
    "        maximum_number_of_value_iterations,\n",
    "        state_value_function,\n",
    "        state_action_value_function)\n",
    "\n",
    "    # Greedy policy selection\n",
    "    policy = greedy_policy_selection(\n",
    "        state_action_successor_state_indices,\n",
    "        state_action_successor_state_transition_probabilities,\n",
    "        state_action_successor_state_rewards,\n",
    "        policy,\n",
    "        discounting_factor_gamma,\n",
    "        state_value_function)\n",
    "    \n",
    "    return state_value_function, state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state value function\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state value function\n",
      "[-1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.  0.]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-4. -3. -2. -4.]\n",
      " [-4. -4. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -4. -4. -2.]\n",
      " [-4. -2. -3. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -4. -4.]\n",
      " [-2. -4. -4. -3.]\n",
      " [-1. -3. -3. -2.]]\n",
      "\n",
      "Final policy\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitial state value function\")\n",
    "print(state_value_function)\n",
    "\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run value iteration\n",
    "state_value_function, state_action_value_function, policy = value_iteration(\n",
    "    number_of_non_terminal_states,\n",
    "    state_action_successor_state_indices,\n",
    "    state_action_successor_state_transition_probabilities,\n",
    "    state_action_successor_state_rewards,\n",
    "    policy,\n",
    "    convergence_threshold,\n",
    "    discounting_factor_gamma,\n",
    "    maximum_number_of_value_iterations,\n",
    "    state_value_function,\n",
    "    state_action_value_function)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state value function\")\n",
    "print(state_value_function)\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
