{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 16\n",
    "num_terminal_states = 2\n",
    "num_non_terminal_states = num_states - num_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions_per_non_terminal_state = np.repeat(\n",
    "    a=max_num_actions, repeats=num_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state_action_successor_states = np.repeat(\n",
    "    a=1, repeats=num_states * max_num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state_action_successor_states = np.reshape(\n",
    "    a=num_state_action_successor_states,\n",
    "    newshape=(num_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_idx = np.array(\n",
    "    object=[1, 0, 14, 4,\n",
    "            2, 1, 0, 5,\n",
    "            2, 2, 1, 6,\n",
    "            4, 14, 3, 7,\n",
    "            5, 0, 3, 8,\n",
    "            6, 1, 4, 9,\n",
    "            6, 2, 5, 10,\n",
    "            8, 3, 7, 11,\n",
    "            9, 4, 7, 12,\n",
    "            10, 5, 8, 13,\n",
    "            10, 6, 9, 15,\n",
    "            12, 7, 11, 11,\n",
    "            13, 8, 11, 12,\n",
    "            15, 9, 12, 13],\n",
    "    dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.repeat(a=1.0, repeats=num_non_terminal_states * max_num_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.repeat(a=-1.0, repeats=num_non_terminal_states * max_num_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_idx = np.reshape(\n",
    "    a=sp_idx,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "p = np.reshape(\n",
    "    a=p,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "r = np.reshape(\n",
    "    a=r,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "maximum_episode_length = 200\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epsiode log\n",
    "episode_log = {\n",
    "    \"s_idx\": np.repeat(a=-1, repeats=maximum_episode_length),\n",
    "    \"a_idx\": np.repeat(a=-1, repeats=maximum_episode_length),\n",
    "    \"reward\": np.repeat(a=0.0, repeats=maximum_episode_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minimum reward since GLIE MC can have some problems based on\n",
    "# value function intialization\n",
    "minimum_reward = np.min(r)\n",
    "\n",
    "if minimum_reward < 0:\n",
    "    q_initializer = 2.0 * minimum_reward\n",
    "else:\n",
    "    q_initializer = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.repeat(a=q_initializer, repeats=num_states * max_num_actions)\n",
    "q = np.reshape(a=q, newshape=(num_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_cum_sum = np.zeros(\n",
    "    shape=(num_states, max_num_actions), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_policy = np.repeat(\n",
    "    a=1.0 / max_num_actions, repeats=num_non_terminal_states * max_num_actions)\n",
    "target_policy = np.reshape(\n",
    "    a=target_policy, newshape=(num_non_terminal_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy = np.repeat(\n",
    "    a=1.0 / max_num_actions, repeats=num_non_terminal_states * max_num_actions)\n",
    "behavior_policy = np.reshape(\n",
    "    a=behavior_policy, newshape=(num_non_terminal_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates episodes\n",
    "def generate_epsiode(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_state_action_successor_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        maximum_episode_length,\n",
    "        behavior_policy,\n",
    "        episode_log):\n",
    "    step_count = 0\n",
    "\n",
    "    # Initial state\n",
    "    s_idx = np.random.randint(\n",
    "        low=0, high=num_non_terminal_states, dtype=np.int64)\n",
    "\n",
    "    # Now repeat\n",
    "    while step_count < maximum_episode_length:\n",
    "        # Get state\n",
    "        episode_log[\"s_idx\"][step_count] = s_idx\n",
    "\n",
    "        # Get action\n",
    "        a_idx = np.random.choice(\n",
    "            a=max_num_actions, p=behavior_policy[s_idx, :])\n",
    "        episode_log[\"a_idx\"][step_count] = a_idx\n",
    "\n",
    "        # Get reward\n",
    "        successor_state_transition_idx = np.random.choice(\n",
    "            a=num_state_action_successor_states[s_idx, a_idx],\n",
    "            p=p[s_idx, a_idx, :])\n",
    "\n",
    "        episode_log[\"reward\"][step_count] = r[s_idx,\n",
    "                                              a_idx,\n",
    "                                              successor_state_transition_idx]\n",
    "\n",
    "        # Get next state\n",
    "        s_idx = sp_idx[s_idx,\n",
    "                           a_idx,\n",
    "                           successor_state_transition_idx]\n",
    "\n",
    "        # Increment step count\n",
    "        step_count += 1\n",
    "\n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if s_idx >= num_non_terminal_states:\n",
    "            break  # episode terminated since we ended up in a terminal state\n",
    "\n",
    "    return step_count, episode_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def greedy_policy_from_state_action_function(q, s_idx, policy):\n",
    "    # Save max state-action value and find the number of actions that have the\n",
    "    # same max state-action value\n",
    "    max_action_value = np.max(a=q[s_idx, :])\n",
    "    max_action_count = np.count_nonzero(\n",
    "        a=q[s_idx, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    max_policy_prob_per_action = 1.0 / max_action_count\n",
    "    policy[s_idx, :] = np.where(\n",
    "        q[s_idx, :] == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        0.0)\n",
    "\n",
    "    return max_policy_prob_per_action, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes in reverse order and updates the target\n",
    "# policy\n",
    "def loop_through_episode_in_reverse(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        q,\n",
    "        weights_cum_sum,\n",
    "        target_policy,\n",
    "        behavior_policy,\n",
    "        gamma,\n",
    "        episode_log,\n",
    "        episode_length):\n",
    "    expected_return = 0.0\n",
    "    weight = 1.0\n",
    "\n",
    "    # Loop through episode steps in reverse order\n",
    "    for t in range(episode_length - 1, -1, -1):\n",
    "        s_idx = episode_log[\"s_idx\"][t]\n",
    "        a_idx = episode_log[\"a_idx\"][t]\n",
    "\n",
    "        # Calculate expected return\n",
    "        expected_return = gamma * expected_return + episode_log[\"reward\"][t]\n",
    "\n",
    "        # Keep track of weight so that we can incrementally calculate average\n",
    "        weights_cum_sum[s_idx, a_idx] += weight\n",
    "\n",
    "        # Update state-action value function\n",
    "        delta = expected_return - q[s_idx, a_idx]\n",
    "        weight_ratio = weight / weights_cum_sum[s_idx, a_idx]\n",
    "        q[s_idx, a_idx] += weight_ratio * delta\n",
    "\n",
    "        # Choose policy for chosen state by greedily choosing from the\n",
    "        # state-action-value function\n",
    "        (max_policy_prob_per_action,\n",
    "         target_policy) = greedy_policy_from_state_action_function(\n",
    "            q, s_idx, target_policy)\n",
    "\n",
    "        # Check to see if behavior action from episode is the same as\n",
    "        # target action\n",
    "        if target_policy[s_idx, a_idx] != max_policy_prob_per_action:\n",
    "            break  # break episode step loop, move on to next episode\n",
    "\n",
    "        # Update weight based on behavior policy\n",
    "        weight /= behavior_policy[s_idx, a_idx]\n",
    "\n",
    "    return q, target_policy, weights_cum_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_control(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        q,\n",
    "        weights_cum_sum,\n",
    "        target_policy,\n",
    "        behavior_policy,\n",
    "        gamma,\n",
    "        episode_log,\n",
    "        episode_length):\n",
    "    for episode in range(0, num_episodes):\n",
    "        # Generate episode and get the length\n",
    "        episode_length, episode_log = generate_epsiode(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_state_action_successor_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r,\n",
    "            maximum_episode_length,\n",
    "            behavior_policy,\n",
    "            episode_log)\n",
    "\n",
    "        # Loop through episode in reverse order and update the target policy\n",
    "        q, target_policy, weights_cum_sum = loop_through_episode_in_reverse(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            q,\n",
    "            weights_cum_sum,\n",
    "            target_policy,\n",
    "            behavior_policy,\n",
    "            gamma,\n",
    "            episode_log,\n",
    "            episode_length)\n",
    "\n",
    "    return q, target_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-2. -3. -2. -4.]\n",
      " [-2. -2. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -2. -4. -2.]\n",
      " [-4. -2. -3. -2.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -2. -2.]\n",
      " [-2. -4. -2. -3.]\n",
      " [-1. -3. -3. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]]\n",
      "\n",
      "Final policy\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.5  0.   0.5  0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.   0.5 ]\n",
      " [0.   0.5  0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.5  0.   0.5  0.  ]\n",
      " [1.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(q)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(target_policy)\n",
    "\n",
    "# Run off policy monte carlo control\n",
    "q, target_policy = off_policy_monte_carlo_control(\n",
    "    num_non_terminal_states,\n",
    "    max_num_actions,\n",
    "    q,\n",
    "    weights_cum_sum,\n",
    "    target_policy,\n",
    "    behavior_policy,\n",
    "    gamma,\n",
    "    episode_log,\n",
    "    episode_length)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(q)\n",
    "print(\"\\nFinal policy\")\n",
    "print(target_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
