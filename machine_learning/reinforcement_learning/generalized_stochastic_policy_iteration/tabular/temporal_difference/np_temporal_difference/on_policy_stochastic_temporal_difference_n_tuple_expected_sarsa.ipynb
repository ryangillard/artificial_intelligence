{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference: On-policy n-Tuple Expected Sarsa, Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_states():\n",
    "    \"\"\"Creates environment states.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_terminal_states: int, number of terminal states.\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    num_terminal_states = 2\n",
    "    num_non_terminal_states = num_states - num_terminal_states\n",
    "\n",
    "    return num_states, num_terminal_states, num_non_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_actions(num_non_terminal_states):\n",
    "    \"\"\"Creates environment actions.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "\n",
    "    Returns:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_terminal_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "    \"\"\"\n",
    "    max_num_actions = 4\n",
    "\n",
    "    num_actions_per_non_terminal_state = np.repeat(\n",
    "        a=max_num_actions, repeats=num_non_terminal_states)\n",
    "\n",
    "    return max_num_actions, num_actions_per_non_terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_successor_counts(num_states, max_num_actions):\n",
    "    \"\"\"Creates environment successor counts.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        num_state_action_successor_states: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "    \"\"\"\n",
    "    num_state_action_successor_states = np.repeat(\n",
    "        a=1, repeats=num_states * max_num_actions)\n",
    "\n",
    "    num_state_action_successor_states = np.reshape(\n",
    "        a=num_state_action_successor_states,\n",
    "        newshape=(num_states, max_num_actions))\n",
    "\n",
    "    return num_state_action_successor_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment_successor_arrays(\n",
    "        num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates environment successor arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    sp_idx = np.array(\n",
    "        object=[1, 0, 14, 4,\n",
    "                2, 1, 0, 5,\n",
    "                2, 2, 1, 6,\n",
    "                4, 14, 3, 7,\n",
    "                5, 0, 3, 8,\n",
    "                6, 1, 4, 9,\n",
    "                6, 2, 5, 10,\n",
    "                8, 3, 7, 11,\n",
    "                9, 4, 7, 12,\n",
    "                10, 5, 8, 13,\n",
    "                10, 6, 9, 15,\n",
    "                12, 7, 11, 11,\n",
    "                13, 8, 11, 12,\n",
    "                15, 9, 12, 13],\n",
    "        dtype=np.int64)\n",
    "\n",
    "    p = np.repeat(\n",
    "        a=1.0, repeats=num_non_terminal_states * max_num_actions * 1)\n",
    "\n",
    "    r = np.repeat(\n",
    "        a=-1.0, repeats=num_non_terminal_states * max_num_actions * 1)\n",
    "\n",
    "    sp_idx = np.reshape(\n",
    "        a=sp_idx,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "    p = np.reshape(\n",
    "        a=p,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "    r = np.reshape(\n",
    "        a=r,\n",
    "        newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "\n",
    "    return sp_idx, p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    \"\"\"Creates environment.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_terminal_states: int, number of terminal states.\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_terminal_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "        num_state_action_successor_states: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    (num_states,\n",
    "     num_terminal_states,\n",
    "     num_non_terminal_states) = create_environment_states()\n",
    "\n",
    "    (max_num_actions,\n",
    "     num_actions_per_non_terminal_state) = create_environment_actions(\n",
    "        num_non_terminal_states)\n",
    "\n",
    "    num_state_action_successor_states = create_environment_successor_counts(\n",
    "        num_states, max_num_actions)\n",
    "\n",
    "    (sp_idx,\n",
    "     p,\n",
    "     r) = create_environment_successor_arrays(\n",
    "        num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    return (num_states,\n",
    "            num_terminal_states,\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_actions_per_non_terminal_state,\n",
    "            num_state_action_successor_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        num_episodes: int, number of episodes to train over.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        num_qs: int, number of state-action-value functions Q_i(s, a).\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "    \"\"\"\n",
    "    num_episodes = 10000\n",
    "    maximum_episode_length = 200\n",
    "    num_qs = 3\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.1\n",
    "    gamma = 1.0\n",
    "\n",
    "    return num_episodes, maximum_episode_length, num_qs, alpha, epsilon, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_function_arrays(num_qs, num_states, max_num_actions):\n",
    "    \"\"\"Creates value function arrays.\n",
    "\n",
    "    Args:\n",
    "        num_qs: int, number of state-action-value functions Q_i(s, a).\n",
    "        num_states: int, number of states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "    \"\"\"\n",
    "    q = np.repeat(a=0.0, repeats=num_qs * num_states * max_num_actions)\n",
    "    q = np.reshape(a=q, newshape=(num_qs, num_states, max_num_actions))\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_non_terminal_states, max_num_actions):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    policy = np.repeat(\n",
    "        a=1.0 / max_num_actions,\n",
    "        repeats=num_non_terminal_states * max_num_actions)\n",
    "\n",
    "    policy = np.reshape(\n",
    "        a=policy,\n",
    "        newshape=(num_non_terminal_states, max_num_actions))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_epsiode(num_non_terminal_states):\n",
    "    \"\"\"Initializes epsiode with initial state.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "    Returns:\n",
    "        init_s_idx: int, initial state index from set of non terminal states.\n",
    "    \"\"\"\n",
    "    # Randomly choose an initial state from all non-terminal states\n",
    "    init_s_idx = np.random.randint(\n",
    "        low=0, high=num_non_terminal_states, dtype=np.int64)\n",
    "\n",
    "    return init_s_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_from_state_action_function(\n",
    "        max_num_actions, q, epsilon, s_idx, policy):\n",
    "    \"\"\"Create epsilon-greedy policy from state-action value function.\n",
    "\n",
    "    Args:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        s_idx: int, current state index.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "    \"\"\"\n",
    "    # Combine state-action value functions\n",
    "    q = np.sum(a=q[:, s_idx, :], axis=0)\n",
    "\n",
    "    # Save max state-action value and find the number of actions that have the\n",
    "    # same max state-action value\n",
    "    max_action_value = np.max(a=q)\n",
    "    max_action_count = np.count_nonzero(a=q == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    if max_action_count == max_num_actions:\n",
    "        max_policy_prob_per_action = 1.0 / max_action_count\n",
    "        remain_prob_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_prob_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remain_prob_per_action = epsilon / (max_num_actions - max_action_count)\n",
    "\n",
    "    policy[s_idx, :] = np.where(\n",
    "        q == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        remain_prob_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_episode(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_state_action_successor_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        num_qs,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        maximum_episode_length,\n",
    "        s_idx):\n",
    "    \"\"\"Loops through episode to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_state_action_successor_states: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        num_qs: int, number of state-action-value functions Q_i(s, a).\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        s_idx: int, current state index.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Choose policy for chosen state by epsilon-greedy choosing from the\n",
    "        # state-action-value function\n",
    "        policy = epsilon_greedy_policy_from_state_action_function(\n",
    "            max_num_actions, q, epsilon, s_idx, policy)\n",
    "\n",
    "        # Get epsilon-greedy action\n",
    "        a_idx = np.random.choice(\n",
    "            a=max_num_actions, p=policy[s_idx, :])\n",
    "\n",
    "        # Get reward\n",
    "        successor_state_transition_idx = np.random.choice(\n",
    "            a=num_state_action_successor_states[s_idx, a_idx],\n",
    "            p=p[s_idx, a_idx, :])\n",
    "\n",
    "        reward = r[s_idx, a_idx, successor_state_transition_idx]\n",
    "\n",
    "        # Get next state\n",
    "        next_s_idx = sp_idx[s_idx, a_idx, successor_state_transition_idx]\n",
    "\n",
    "        # Update state action value equally randomly selecting from the\n",
    "        # state-action-value functions\n",
    "        updating_q_idx = np.random.randint(low=0, high=num_qs, dtype=np.int64)\n",
    "\n",
    "        q, policy, s_idx = update_q(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            policy,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            gamma,\n",
    "            s_idx,\n",
    "            a_idx,\n",
    "            reward,\n",
    "            next_s_idx,\n",
    "            updating_q_idx,\n",
    "            num_qs,\n",
    "            q)\n",
    "\n",
    "        if next_s_idx >= num_non_terminal_states:\n",
    "            break  # episode terminated since we ended up in a terminal state\n",
    "\n",
    "    return q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        s_idx,\n",
    "        a_idx,\n",
    "        reward,\n",
    "        next_s_idx,\n",
    "        updating_q_idx,\n",
    "        num_qs,\n",
    "        q):\n",
    "    \"\"\"Updates state-action-value function using multiple estimates.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        s_idx: int, current state index.\n",
    "        a_idx: int, current action index.\n",
    "        reward: float, current reward from taking action a_idx in state s_idx.\n",
    "        next_s_idx: int, next state index.\n",
    "        updating_q_idx: int, index to which Q_i(s, a) we'll be updating.\n",
    "        num_qs: int, number of state-action-value functions Q_i(s, a).\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        s_idx: int, new current state index.\n",
    "    \"\"\"\n",
    "    # Check to see if we actioned into a terminal state\n",
    "    if next_s_idx >= num_non_terminal_states:\n",
    "        delta = reward - q[updating_q_idx, s_idx, a_idx]\n",
    "        q[updating_q_idx, s_idx, a_idx] += alpha * delta\n",
    "    else:\n",
    "        # Get next action, using expectation value\n",
    "        q_indices = np.arange(num_qs)\n",
    "\n",
    "        not_updating_q_idx = np.random.choice(\n",
    "            a=np.extract(condition=q_indices != updating_q_idx, arr=q_indices))\n",
    "\n",
    "        not_updating_v_expected_value_on_policy = np.sum(\n",
    "            a=policy[next_s_idx, :] * q[not_updating_q_idx, next_s_idx, :])\n",
    "\n",
    "        # Calculate state-action-function expectation\n",
    "        delta = gamma * not_updating_v_expected_value_on_policy\n",
    "        delta -= q[updating_q_idx, s_idx, a_idx]\n",
    "        q[updating_q_idx, s_idx, a_idx] += alpha * (reward + delta)\n",
    "\n",
    "        # Update state and action to next state and action\n",
    "        s_idx = next_s_idx\n",
    "\n",
    "    return q, policy, s_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_temporal_difference_n_tuple_expected_sarsa(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_state_action_successor_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        num_qs,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        maximum_episode_length,\n",
    "        num_episodes):\n",
    "    \"\"\"Loops through episodes to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_terminal_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_state_action_successor_states: array[int], number of successor\n",
    "            states s' that can be reached from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        num_qs: int, number of state-action-value functions Q_i(s, a).\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        num_episodes: int, number of episodes to train over.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q_i(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    for episode in range(0, num_episodes):\n",
    "        # Initialize episode to get initial state\n",
    "        init_s_idx = initialize_epsiode(num_non_terminal_states)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        q, policy = loop_through_episode(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_state_action_successor_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r,\n",
    "            num_qs,\n",
    "            q,\n",
    "            policy,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            gamma,\n",
    "            maximum_episode_length,\n",
    "            init_s_idx)\n",
    "\n",
    "    return q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\"\"\"\n",
    "    (num_states,\n",
    "     num_terminal_states,\n",
    "     num_non_terminal_states,\n",
    "     max_num_actions,\n",
    "     num_actions_per_non_terminal_state,\n",
    "     num_state_action_successor_states,\n",
    "     sp_idx,\n",
    "     p,\n",
    "     r) = create_environment()\n",
    "\n",
    "    (num_episodes,\n",
    "     maximum_episode_length,\n",
    "     num_qs,\n",
    "     alpha,\n",
    "     epsilon,\n",
    "     gamma) = set_hyperparameters()\n",
    "\n",
    "    q = create_value_function_arrays(num_qs, num_states, max_num_actions)\n",
    "\n",
    "    policy = create_policy_arrays(num_non_terminal_states, max_num_actions)\n",
    "\n",
    "    # Print initial arrays\n",
    "    print(\"\\nInitial state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nInitial policy\")\n",
    "    print(policy)\n",
    "\n",
    "    # Run on policy temporal difference n-tuple expected sarsa\n",
    "    q, policy = on_policy_temporal_difference_n_tuple_expected_sarsa(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_state_action_successor_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        num_qs,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        maximum_episode_length,\n",
    "        num_episodes)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nFinal policy\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[[-3.18459357 -2.1189881  -1.         -3.10386078]\n",
      "  [-3.69065031 -2.93357306 -2.17083139 -3.00905455]\n",
      "  [-3.4582224  -3.87651829 -3.27369192 -3.26981159]\n",
      "  [-3.17375873 -1.         -2.06455999 -3.17994081]\n",
      "  [-3.46430926 -2.17024661 -2.17241726 -3.89495096]\n",
      "  [-3.27273294 -3.26978286 -3.27018277 -3.26496145]\n",
      "  [-2.85864966 -3.75001552 -3.48932676 -2.17380553]\n",
      "  [-3.20703943 -2.17385205 -2.89098692 -3.53384376]\n",
      "  [-3.26356955 -3.2654695  -3.27746431 -3.26616593]\n",
      "  [-2.17380562 -3.55170642 -3.73326251 -2.17574334]\n",
      "  [-2.05584678 -3.23738139 -3.16788844 -1.        ]\n",
      "  [-3.2798313  -3.30059282 -3.60185838 -3.78331364]\n",
      "  [-2.17606571 -3.6820652  -3.8673286  -3.04126962]\n",
      "  [-1.         -3.10991344 -3.17834214 -2.08493804]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-3.09719566 -2.09355915 -1.         -3.17280604]\n",
      "  [-3.81023047 -2.82802918 -2.1683581  -3.93676108]\n",
      "  [-3.71710305 -3.35502541 -3.25776559 -3.2797274 ]\n",
      "  [-3.10566689 -1.         -2.12261441 -3.17462934]\n",
      "  [-3.86169309 -2.17097145 -2.17055401 -3.84386466]\n",
      "  [-3.25714141 -3.23697662 -3.25255805 -3.25130382]\n",
      "  [-2.61062372 -3.6736284  -3.14677797 -2.17894441]\n",
      "  [-3.81195217 -2.17078931 -2.94898301 -3.42961026]\n",
      "  [-3.26025189 -3.25793822 -3.25161855 -3.2763978 ]\n",
      "  [-2.17837232 -3.71718492 -3.48922426 -2.17602653]\n",
      "  [-2.04607089 -3.00235691 -3.12107929 -1.        ]\n",
      "  [-3.29455737 -3.28752939 -3.93958877 -3.75019912]\n",
      "  [-2.17715555 -3.7459044  -2.81736854 -2.95558678]\n",
      "  [-1.         -3.14987527 -3.05007983 -2.13159687]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-3.17209977 -2.14056374 -1.         -2.62843418]\n",
      "  [-2.69321839 -3.03761498 -2.17937144 -3.31654947]\n",
      "  [-3.68978066 -3.81646139 -3.27997682 -3.26550165]\n",
      "  [-2.85601264 -1.         -2.10336957 -3.02114584]\n",
      "  [-3.73114635 -2.17826991 -2.17775334 -3.81684283]\n",
      "  [-3.24954977 -3.27250604 -3.25884117 -3.26326278]\n",
      "  [-2.93173579 -3.71817716 -3.62762282 -2.17414903]\n",
      "  [-3.84890217 -2.17960585 -2.94018598 -3.89268929]\n",
      "  [-3.25928202 -3.26260591 -3.25409911 -3.24242014]\n",
      "  [-2.17618421 -3.34532238 -3.84623317 -2.17656078]\n",
      "  [-2.0560829  -3.14979414 -3.09895919 -1.        ]\n",
      "  [-3.29001335 -3.27336181 -3.54946497 -3.78655061]\n",
      "  [-2.17780826 -3.50589291 -3.83651347 -3.03618731]\n",
      "  [-1.         -3.15899574 -3.18075064 -1.97405219]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "\n",
      "Final policy\n",
      "[[0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]]\n"
     ]
    }
   ],
   "source": [
    "run_algorithm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
