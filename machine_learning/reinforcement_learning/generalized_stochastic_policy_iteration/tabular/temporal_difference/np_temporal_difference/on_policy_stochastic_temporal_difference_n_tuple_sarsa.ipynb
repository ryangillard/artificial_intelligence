{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 16\n",
    "num_terminal_states = 2\n",
    "num_non_terminal_states = num_states - num_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions_per_non_terminal_state = np.repeat(\n",
    "    a=max_num_actions, repeats=num_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state_action_successor_states = np.repeat(\n",
    "    a=1, repeats=num_states * max_num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state_action_successor_states = np.reshape(\n",
    "    a=num_state_action_successor_states,\n",
    "    newshape=(num_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_idx = np.array(\n",
    "    object=[1, 0, 14, 4,\n",
    "            2, 1, 0, 5,\n",
    "            2, 2, 1, 6,\n",
    "            4, 14, 3, 7,\n",
    "            5, 0, 3, 8,\n",
    "            6, 1, 4, 9,\n",
    "            6, 2, 5, 10,\n",
    "            8, 3, 7, 11,\n",
    "            9, 4, 7, 12,\n",
    "            10, 5, 8, 13,\n",
    "            10, 6, 9, 15,\n",
    "            12, 7, 11, 11,\n",
    "            13, 8, 11, 12,\n",
    "            15, 9, 12, 13],\n",
    "    dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.repeat(a=1.0, repeats=num_non_terminal_states * max_num_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.repeat(a=-1.0, repeats=num_non_terminal_states * max_num_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_idx = np.reshape(\n",
    "    a=sp_idx,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "p = np.reshape(\n",
    "    a=p,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))\n",
    "r = np.reshape(\n",
    "    a=r,\n",
    "    newshape=(num_non_terminal_states, max_num_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes\n",
    "num_episodes = 10000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 200\n",
    "# Set the number of state-action-value functions\n",
    "num_qs = 3\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.1\n",
    "# Set discounting factor gamma\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.repeat(a=0.0, repeats=num_qs * num_states * max_num_actions)\n",
    "q = np.reshape(a=q, newshape=(num_qs, num_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(\n",
    "    a=1.0 / max_num_actions, repeats=num_non_terminal_states * max_num_actions)\n",
    "policy = np.reshape(\n",
    "    a=policy, newshape=(num_non_terminal_states, max_num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(\n",
    "        num_non_terminal_states, max_num_actions, q, epsilon, policy):\n",
    "    # Initial state\n",
    "    # Randomly choose an initial state from all non-terminal states\n",
    "    init_s_idx = np.random.randint(\n",
    "        low=0, high=num_non_terminal_states, dtype=np.int64)\n",
    "\n",
    "    # Choose policy for chosen state by epsilon-greedy choosing from the\n",
    "    # state-action-value function\n",
    "    policy = epsilon_greedy_policy_from_state_action_function(\n",
    "        max_num_actions, q, epsilon, init_s_idx, policy)\n",
    "\n",
    "    # Get initial action\n",
    "    init_a_idx = np.random.choice(\n",
    "        a=max_num_actions,\n",
    "        p=policy[init_s_idx, :])\n",
    "\n",
    "    return init_s_idx, init_a_idx, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def epsilon_greedy_policy_from_state_action_function(\n",
    "        max_num_actions, q, epsilon, s_idx, policy):\n",
    "    # Combine state-action value functions\n",
    "    q = np.sum(a=q[:, s_idx, :], axis=0)\n",
    "\n",
    "    # Save max state-action value and find the number of actions that have the\n",
    "    # same max state-action value\n",
    "    max_action_value = np.max(a=q)\n",
    "    max_action_count = np.count_nonzero(a=q == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    if max_action_count == max_num_actions:\n",
    "        max_policy_prob_per_action = 1.0 / max_action_count\n",
    "        remain_prob_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_prob_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remain_prob_per_action = epsilon / (max_num_actions - max_action_count)\n",
    "\n",
    "    policy[s_idx, :] = np.where(\n",
    "        q == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        remain_prob_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_state_action_successor_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        num_qs,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        maximum_episode_length,\n",
    "        s_idx,\n",
    "        a_idx):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Get reward\n",
    "        successor_state_transition_idx = np.random.choice(\n",
    "            a=num_state_action_successor_states[s_idx, a_idx],\n",
    "            p=p[s_idx, a_idx, :])\n",
    "\n",
    "        reward = r[s_idx, a_idx, successor_state_transition_idx]\n",
    "\n",
    "        # Get next state\n",
    "        next_s_idx = sp_idx[s_idx, a_idx, successor_state_transition_idx]\n",
    "\n",
    "        # Update state action value equally randomly selecting from the\n",
    "        # state-action-value functions\n",
    "        updating_q_idx = np.random.randint(low=0, high=num_qs, dtype=np.int64)\n",
    "\n",
    "        q, policy, s_idx, a_idx = update_q(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            policy,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            gamma,\n",
    "            s_idx,\n",
    "            a_idx,\n",
    "            reward,\n",
    "            next_s_idx,\n",
    "            updating_q_idx,\n",
    "            num_qs,\n",
    "            q)\n",
    "\n",
    "        if next_s_idx >= num_non_terminal_states:\n",
    "            break  # episode terminated since we ended up in a terminal state\n",
    "\n",
    "    return q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the state-action-value function\n",
    "def update_q(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        s_idx,\n",
    "        a_idx,\n",
    "        reward,\n",
    "        next_s_idx,\n",
    "        updating_q_idx,\n",
    "        num_qs,\n",
    "        q):\n",
    "    # Check to see if we actioned into a terminal state\n",
    "    if next_s_idx >= num_non_terminal_states:\n",
    "        delta = reward - q[updating_q_idx, s_idx, a_idx]\n",
    "        q[updating_q_idx, s_idx, a_idx] += alpha * delta\n",
    "    else:\n",
    "        # Choose policy for chosen state by epsilon-greedy choosing from the\n",
    "        # state-action-value function\n",
    "        policy = epsilon_greedy_policy_from_state_action_function(\n",
    "            max_num_actions, q, epsilon, next_s_idx, policy)\n",
    "\n",
    "        # Get next action\n",
    "        next_a_idx = np.random.choice(\n",
    "            a=max_num_actions, p=policy[next_s_idx, :])\n",
    "\n",
    "        # Calculate state-action-function using quintuple SARSA\n",
    "        q_indices = np.arange(num_qs)\n",
    "\n",
    "        not_updating_q_idx = np.random.choice(\n",
    "            a=np.extract(condition=q_indices != updating_q_idx, arr=q_indices))\n",
    "\n",
    "        delta = gamma * q[not_updating_q_idx, next_s_idx, next_a_idx]\n",
    "        delta -= q[updating_q_idx, s_idx, a_idx]\n",
    "        q[updating_q_idx, s_idx, a_idx] += alpha * (reward + delta)\n",
    "\n",
    "        # Update state and action to next state and action\n",
    "        s_idx = next_s_idx\n",
    "        a_idx = next_a_idx\n",
    "\n",
    "    return q, policy, s_idx, a_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_temporal_difference_n_tuple_sarsa(\n",
    "        num_non_terminal_states,\n",
    "        max_num_actions,\n",
    "        num_state_action_successor_states,\n",
    "        sp_idx,\n",
    "        p,\n",
    "        r,\n",
    "        num_qs,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        maximum_episode_length):\n",
    "    for episode in range(0, num_episodes):\n",
    "        # Initialize episode to get initial state and action\n",
    "        init_s_idx, init_a_idx, policy = initialize_epsiode(\n",
    "            num_non_terminal_states, max_num_actions, q, epsilon, policy)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        q, policy = loop_through_episode(\n",
    "            num_non_terminal_states,\n",
    "            max_num_actions,\n",
    "            num_state_action_successor_states,\n",
    "            sp_idx,\n",
    "            p,\n",
    "            r,\n",
    "            num_qs,\n",
    "            q,\n",
    "            policy,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            gamma,\n",
    "            maximum_episode_length,\n",
    "            init_s_idx,\n",
    "            init_a_idx)\n",
    "\n",
    "    return q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[[-3.13713005 -2.03170358 -1.         -2.78873257]\n",
      "  [-3.7616367  -2.83993565 -2.05149502 -2.99083486]\n",
      "  [-3.70858611 -3.79192305 -3.28467468 -3.28992076]\n",
      "  [-2.9425024  -1.         -1.9293953  -2.92428379]\n",
      "  [-3.35500718 -2.33925877 -2.11554999 -3.46857146]\n",
      "  [-3.27622722 -3.1990644  -3.21837773 -3.16301782]\n",
      "  [-2.82863087 -3.98793502 -3.45190198 -2.25998382]\n",
      "  [-3.66219364 -2.03054132 -3.0558509  -3.46935458]\n",
      "  [-3.17979819 -3.321236   -3.25289649 -3.17060674]\n",
      "  [-2.23324783 -3.77561375 -3.61461143 -2.33139236]\n",
      "  [-2.04492266 -3.28602839 -3.02115685 -1.        ]\n",
      "  [-3.13971442 -3.37258045 -3.39004705 -3.8907769 ]\n",
      "  [-2.36410615 -3.62257475 -3.89385167 -2.58489664]\n",
      "  [-1.         -2.90636728 -3.20654647 -2.12290649]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-3.08712177 -2.18655188 -1.         -2.94969743]\n",
      "  [-3.89016718 -2.98814833 -2.15003339 -3.54547153]\n",
      "  [-3.52490765 -3.59847842 -3.22110498 -3.30131387]\n",
      "  [-3.03246798 -1.         -2.06689868 -2.97712095]\n",
      "  [-3.42460188 -2.22328806 -2.19886604 -3.77880484]\n",
      "  [-3.26589524 -3.35423603 -3.21744022 -3.18789268]\n",
      "  [-3.14489609 -3.49792338 -3.79199739 -2.13119496]\n",
      "  [-3.68860962 -2.07603746 -2.7189896  -3.66180317]\n",
      "  [-3.22854387 -3.13750452 -3.08217051 -3.27233995]\n",
      "  [-2.15425579 -3.63927887 -3.09564947 -2.57909193]\n",
      "  [-2.2223947  -3.15843477 -3.13514295 -1.        ]\n",
      "  [-3.37821321 -3.30115469 -3.89413614 -3.79273635]\n",
      "  [-2.12802065 -3.59070189 -3.3373027  -3.11301727]\n",
      "  [-1.         -3.09566875 -3.17660397 -1.94712012]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-3.05621845 -1.9887211  -1.         -2.90121498]\n",
      "  [-3.89974745 -3.05358478 -2.1084371  -3.34105836]\n",
      "  [-3.83923305 -3.91420502 -3.25914797 -3.39582311]\n",
      "  [-3.13352119 -1.         -1.95938806 -3.05417178]\n",
      "  [-3.37287401 -2.30478865 -2.07819524 -3.76445414]\n",
      "  [-3.21863656 -3.21595627 -3.23554499 -3.35951317]\n",
      "  [-2.90582988 -3.85681037 -3.62024417 -2.27688788]\n",
      "  [-3.53860276 -2.00859921 -2.92078493 -3.70138722]\n",
      "  [-3.34035669 -3.32701269 -3.30607515 -3.30141288]\n",
      "  [-2.1552935  -3.38043482 -3.7984216  -2.21181373]\n",
      "  [-1.94533212 -3.28610693 -3.20076045 -1.        ]\n",
      "  [-3.29719621 -3.26827128 -3.73198848 -3.77857482]\n",
      "  [-2.03478374 -3.63207077 -3.69362362 -2.98643481]\n",
      "  [-1.         -3.10307442 -3.30609477 -1.95478474]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "\n",
      "Final policy\n",
      "[[0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(q)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run on policy temporal difference n-tuple sarsa\n",
    "q, policy = on_policy_temporal_difference_n_tuple_sarsa(\n",
    "    num_non_terminal_states,\n",
    "    max_num_actions,\n",
    "    num_state_action_successor_states,\n",
    "    sp_idx,\n",
    "    p,\n",
    "    r,\n",
    "    num_qs,\n",
    "    q,\n",
    "    policy,\n",
    "    alpha,\n",
    "    epsilon,\n",
    "    gamma,\n",
    "    maximum_episode_length)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(q)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
