{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning and Learning: Prioritized Sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_known_environment_states():\n",
    "    \"\"\"Creates known environment states.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_term_states: int, number of terminal states.\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    num_term_states = 2\n",
    "    num_non_term_states = num_states - num_term_states\n",
    "\n",
    "    return num_states, num_term_states, num_non_term_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_known_environment_actions(num_non_term_states):\n",
    "    \"\"\"Creates environment actions.\n",
    "\n",
    "    Args:\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "\n",
    "    Returns:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_term_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "    \"\"\"\n",
    "    max_num_actions = 4\n",
    "\n",
    "    num_actions_per_non_term_state = np.repeat(\n",
    "        a=max_num_actions, repeats=num_non_term_states)\n",
    "\n",
    "    return max_num_actions, num_actions_per_non_term_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_known_environment():\n",
    "    \"\"\"Creates known environment.\n",
    "\n",
    "    Returns:\n",
    "        num_states: int, number of states.\n",
    "        num_term_states: int, number of terminal states.\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        num_actions_per_non_term_state: array[int], number of actions per\n",
    "            non terminal state.\n",
    "    \"\"\"\n",
    "    (num_states,\n",
    "     num_term_states,\n",
    "     num_non_term_states) = create_known_environment_states()\n",
    "\n",
    "    (max_num_actions,\n",
    "     num_actions_per_non_term_state) = create_known_environment_actions(\n",
    "        num_non_term_states)\n",
    "\n",
    "    return (num_states,\n",
    "            num_term_states,\n",
    "            num_non_term_states,\n",
    "            max_num_actions,\n",
    "            num_actions_per_non_term_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"Class to hold all environment properties.\n",
    "\n",
    "    Fields:\n",
    "        num_sp: array[int], number of successor states s' that can be reached\n",
    "            from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, num_non_term_states, max_num_actions):\n",
    "        # Create environment state-action successor state arrrays\n",
    "        self.num_sp = np.ones(\n",
    "            shape=[num_states, max_num_actions], dtype=np.int64)\n",
    "\n",
    "        self.sp_idx = np.reshape(\n",
    "            a=np.array([1, 0, 14, 4,\n",
    "                        2, 1, 0, 5,\n",
    "                        2, 2, 1, 6,\n",
    "                        4, 14, 3, 7,\n",
    "                        5, 0, 3, 8,\n",
    "                        6, 1, 4, 9,\n",
    "                        6, 2, 5, 10,\n",
    "                        8, 3, 7, 11,\n",
    "                        9, 4, 7, 12,\n",
    "                        10, 5, 8, 13,\n",
    "                        10, 6, 9, 15,\n",
    "                        12, 7, 11, 11,\n",
    "                        13, 8, 11, 12,\n",
    "                        15, 9, 12, 13],\n",
    "                       dtype=np.int64),\n",
    "            newshape=(num_non_term_states, max_num_actions, 1))\n",
    "        self.p = np.reshape(\n",
    "            a=np.repeat(\n",
    "                a=1.0, repeats=num_non_term_states * max_num_actions * 1),\n",
    "            newshape=(num_non_term_states, max_num_actions, 1))\n",
    "        self.r = np.reshape(\n",
    "            a=np.repeat(\n",
    "                a=-1.0, repeats=num_non_term_states * max_num_actions * 1),\n",
    "            newshape=(num_non_term_states, max_num_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Class to hold all model properties.\n",
    "\n",
    "    Fields:\n",
    "        num_seen_non_term_states: int, number of seen non-terminal states.\n",
    "        seen_non_term_s_stack: array[int], stack to hold all seen non-terminal\n",
    "            states.\n",
    "        seen_non_term_s_stack_rev_lu: array[int], reverse lookup of stack\n",
    "            that holds all seen non-terminal states.\n",
    "        num_seen_non_term_s_a: array[int], number of seen non-terminal\n",
    "            state-action pairs.\n",
    "        seen_non_term_s_a_stack: array[int], stack to hold all seen\n",
    "            non-terminal state-action pairs.\n",
    "        seen_non_term_s_a_stack_rev_lu: array[int], reverse lookup of stack\n",
    "            that holds all seen non-terminal states-action pairs.\n",
    "        num_sp: array[int], number of successor states s' that can be reached\n",
    "            from state s by taking action a.\n",
    "        sp_idx: array[int], state indices of new state s' of taking action a\n",
    "            from state s.\n",
    "        p: array[float], transition probability to go from state s to s' by\n",
    "            taking action a.\n",
    "        r: array[float], reward from new state s' from state s by taking\n",
    "            action a.\n",
    "        s_a_ss_num_visits: array[int], number of visits to a particular\n",
    "            (s, a, s') tuple.\n",
    "        num_s_pred_s_a_pairs: array[int], number of state predecessor state\n",
    "            action pairs.\n",
    "        s_pred_s_a_pairs: dict, maps state indices to a\n",
    "            list of actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, num_non_term_states, max_num_actions):\n",
    "        # Create model state visit counters\n",
    "        self.num_seen_non_term_states = 0\n",
    "        self.seen_non_term_s_stack = np.zeros(\n",
    "            shape=[num_non_term_states], dtype=np.int64)\n",
    "        self.seen_non_term_s_stack_rev_lu = np.zeros(\n",
    "            shape=[num_non_term_states], dtype=np.int64)\n",
    "\n",
    "        # Create model state-action visit counters\n",
    "        self.num_seen_non_term_s_a = np.zeros(\n",
    "            shape=[num_non_term_states], dtype=np.int64)\n",
    "        self.seen_non_term_s_a_stack = np.zeros(\n",
    "            shape=[num_non_term_states, max_num_actions], dtype=np.int64)\n",
    "        self.seen_non_term_s_a_stack_rev_lu = np.zeros(\n",
    "            shape=[num_non_term_states, max_num_actions], dtype=np.int64)\n",
    "\n",
    "        # Create model state-action successor state arrrays\n",
    "        self.num_sp = np.zeros(\n",
    "            shape=[num_states, max_num_actions], dtype=np.int64)\n",
    "\n",
    "        self.sp_idx = np.array(\n",
    "            object=[[[0] if s_idx == 0 and a_idx == 0 else []\n",
    "                     for a_idx in range(0, max_num_actions)]\n",
    "                    for s_idx in range(0, num_states)],\n",
    "            dtype=np.object)\n",
    "        self.p = np.array(\n",
    "            object=[[[0.0] if s_idx == 0 and a_idx == 0 else []\n",
    "                     for a_idx in range(0, max_num_actions)]\n",
    "                    for s_idx in range(0, num_states)],\n",
    "            dtype=np.object)\n",
    "        self.r = np.array(\n",
    "            object=[[[0.0] if s_idx == 0 and a_idx == 0 else []\n",
    "                     for a_idx in range(0, max_num_actions)]\n",
    "                    for s_idx in range(0, num_states)],\n",
    "            dtype=np.object)\n",
    "        self.s_a_ss_num_visits = np.array(\n",
    "            object=[[[0] if s_idx == 0 and a_idx == 0 else []\n",
    "                     for a_idx in range(0, max_num_actions)]\n",
    "                    for s_idx in range(0, num_states)],\n",
    "            dtype=np.object)\n",
    "\n",
    "        del self.sp_idx[0, 0][0]\n",
    "        del self.p[0, 0][0]\n",
    "        del self.r[0, 0][0]\n",
    "        del self.s_a_ss_num_visits[0, 0][0]\n",
    "\n",
    "        self.num_s_pred_s_a_pairs = np.zeros(\n",
    "            shape=[num_states], dtype=np.int64)\n",
    "        self.s_pred_s_a_pairs = {\n",
    "            state_index: []\n",
    "            for state_index in range(0, num_states)\n",
    "        }\n",
    "\n",
    "    def update_model_seen_state_actions(self, s_idx, a_idx):\n",
    "        \"\"\"Updates what state and actions the model has seen.\n",
    "\n",
    "        Args:\n",
    "            s_idx: int, current state index.\n",
    "            a_idx: int, current action index.\n",
    "        \"\"\"\n",
    "        # Check to see if state has already been visited\n",
    "        if (self.num_seen_non_term_states == 0 or\n",
    "            (self.seen_non_term_s_stack_rev_lu[s_idx] == 0 and\n",
    "             self.seen_non_term_s_stack[0] != s_idx)):  # if new state\n",
    "            # Add to state stack\n",
    "            # 1, 3, 2, 0, 4\n",
    "            self.seen_non_term_s_stack[self.num_seen_non_term_states] = s_idx\n",
    "            # 3, 0, 2, 1, 4\n",
    "            num_seen = self.num_seen_non_term_states\n",
    "            self.seen_non_term_s_stack_rev_lu[s_idx] = num_seen\n",
    "\n",
    "            # Add to action stack\n",
    "            # 2, 0, 3, 1\n",
    "            action_idx = self.num_seen_non_term_s_a[s_idx]\n",
    "            self.seen_non_term_s_a_stack[s_idx][action_idx] = a_idx\n",
    "            # 1, 3, 0, 2\n",
    "            lookup = self.num_seen_non_term_s_a[s_idx]\n",
    "            self.seen_non_term_s_a_stack_rev_lu[s_idx][a_idx] = lookup\n",
    "\n",
    "            # Increment counters\n",
    "            self.num_seen_non_term_s_a[s_idx] += 1\n",
    "            self.num_seen_non_term_states += 1\n",
    "        else:  # if already visited state\n",
    "            # Check to see if action has already been visited\n",
    "            if (self.seen_non_term_s_a_stack_rev_lu[s_idx][a_idx] == 0 and\n",
    "                    self.seen_non_term_s_a_stack[s_idx][0] != a_idx):\n",
    "                # Add to action stack\n",
    "                # 2, 0, 3, 1\n",
    "                action_idx = self.num_seen_non_term_s_a[s_idx]\n",
    "                self.seen_non_term_s_a_stack[s_idx][action_idx] = a_idx\n",
    "                # 1, 3, 0, 2\n",
    "                num_seen = self.num_seen_non_term_s_a[s_idx]\n",
    "                self.seen_non_term_s_a_stack_rev_lu[s_idx][a_idx] = num_seen\n",
    "\n",
    "                # Increment counters\n",
    "                self.num_seen_non_term_s_a[s_idx] += 1\n",
    "\n",
    "    def update_model_of_environment_from_experience(\n",
    "            self, s_idx, a_idx, reward, next_s_idx):\n",
    "        \"\"\"Updates the model from environment experience.\n",
    "\n",
    "        Args:\n",
    "            s_idx: int, current state index.\n",
    "            a_idx: int, current action index.\n",
    "            reward: float, reward of taking action a_idx in state s_idx.\n",
    "            next_s_idx: int, next state index.\n",
    "        \"\"\"\n",
    "        # Update model successor arrays\n",
    "        if next_s_idx in self.sp_idx[s_idx, a_idx]:\n",
    "            self.suc_idx = self.sp_idx[s_idx, a_idx].index(next_s_idx)\n",
    "            self.s_a_ss_num_visits[s_idx, a_idx][self.suc_idx] += 1\n",
    "        else:\n",
    "            self.num_sp[s_idx, a_idx] += 1\n",
    "            self.sp_idx[s_idx, a_idx].append(next_s_idx)\n",
    "            self.r[s_idx, a_idx].append(reward)\n",
    "            self.s_a_ss_num_visits[s_idx, a_idx].append(1)\n",
    "\n",
    "        self.s_a_ss_num_visits_sum = np.sum(\n",
    "            a=np.asarray(a=self.s_a_ss_num_visits[s_idx, a_idx]))\n",
    "        self.p[s_idx, a_idx] = [\n",
    "            float(self.s_a_ss_num_visits[s_idx, a_idx][suc_idx]) /\n",
    "            self.s_a_ss_num_visits_sum\n",
    "            for suc_idx in range(0, self.num_sp[s_idx, a_idx])\n",
    "        ]\n",
    "\n",
    "        # Update model state predecessors\n",
    "        if (s_idx, a_idx) not in self.s_pred_s_a_pairs[next_s_idx]:\n",
    "            self.s_pred_s_a_pairs[next_s_idx].append((s_idx, a_idx))\n",
    "\n",
    "            self.num_s_pred_s_a_pairs[next_s_idx] += 1\n",
    "\n",
    "    def model_simulate_planning(\n",
    "            self,\n",
    "            num_planning_steps,\n",
    "            num_non_term_states,\n",
    "            max_num_actions,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            theta,\n",
    "            q,\n",
    "            priority_queue):\n",
    "        \"\"\"Uses model to simulate experience and plan best actions.\n",
    "\n",
    "        Args:\n",
    "            num_planning_steps: int, number of steps for the planning stage.\n",
    "            num_non_term_states: int, number of non terminal states.\n",
    "            max_num_actions: int, max number of actions possible.\n",
    "            alpha: float, alpha > 0, learning rate.\n",
    "            gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "            theta: float, small threshold for adding state-action pairs to\n",
    "                priority queue.\n",
    "            q: array[float], keeps track of the estimated value of each\n",
    "                state-action pair Q(s, a).\n",
    "            priority_queue: instance of `PriorityQueue` class, an array of\n",
    "                `PriorityQueueNode`s that keep track of the state index,\n",
    "                action index, and priority of state-action pairs.\n",
    "        Returns:\n",
    "            q: array[float], keeps track of the estimated value of each\n",
    "                state-action pair Q(s, a).\n",
    "            priority_queue: instance of `PriorityQueue` class, an array of\n",
    "                `PriorityQueueNode`s that keep track of the state index,\n",
    "                action index, and priority of state-action pairs.\n",
    "        \"\"\"\n",
    "        for i in range(0, num_planning_steps):\n",
    "            # Check if priority queue is empty\n",
    "            if priority_queue.cur_p_q_size == 0:\n",
    "                break  # break i loop since priority queue is empty\n",
    "\n",
    "            # Get max priority state-action pair from queue\n",
    "            s_idx, a_idx = priority_queue.pop_max_node_from_p_q()\n",
    "\n",
    "            # Get reward\n",
    "            reward, sst_idx = observe_reward(s_idx, a_idx, self)\n",
    "\n",
    "            # Get next state\n",
    "            next_s_idx = self.sp_idx[s_idx, a_idx][sst_idx]\n",
    "\n",
    "            # Check to see if we actioned into a terminal state\n",
    "            if next_s_idx >= num_non_term_states:\n",
    "                q[s_idx, a_idx] += alpha * (reward - q[s_idx, a_idx])\n",
    "            else:\n",
    "                # Get next action, max action of next state\n",
    "                next_a_idx = select_max_q_action(\n",
    "                    next_s_idx, max_num_actions, q)\n",
    "\n",
    "                # Calculate state-action-function using quintuple\n",
    "                # SARSargmax(a,Q)\n",
    "                delta = gamma * q[next_s_idx, next_a_idx] - q[s_idx, a_idx]\n",
    "                q[s_idx, a_idx] += alpha * (reward + delta)\n",
    "\n",
    "            # Loop for all predicted Sbar and Abar to lead to S\n",
    "            for j in range(0, self.num_s_pred_s_a_pairs[s_idx]):\n",
    "                pred_s_idx = self.s_pred_s_a_pairs[s_idx][j][0]\n",
    "                pred_a_idx = self.s_pred_s_a_pairs[s_idx][j][1]\n",
    "\n",
    "                # Get reward\n",
    "                if s_idx in self.sp_idx[pred_s_idx, pred_a_idx]:\n",
    "                    sst_idx = self.sp_idx[pred_s_idx, pred_a_idx].index(s_idx)\n",
    "\n",
    "                # Get reward from predecessor state and action\n",
    "                reward = self.r[s_idx, a_idx][sst_idx]\n",
    "\n",
    "                # Get next action, max action of next state\n",
    "                next_a_idx = select_max_q_action(s_idx, max_num_actions, q)\n",
    "\n",
    "                # Calculate priority\n",
    "                expected = gamma * q[s_idx, next_a_idx]\n",
    "                delta = expected - q[pred_s_idx, pred_a_idx]\n",
    "                priority = np.abs(reward + delta)\n",
    "\n",
    "                # Check if priority is over threshold to add to priority queue\n",
    "                if priority > theta:\n",
    "                    priority_queue.search_and_update_p_q(\n",
    "                        pred_s_idx, pred_a_idx, priority)\n",
    "\n",
    "        return q, priority_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create priority queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityQueueNode:\n",
    "    \"\"\"Class to create nodes of a priority queue.\n",
    "\n",
    "    Fields:\n",
    "        s_idx: int, state index.\n",
    "        a_idx: int, action index.\n",
    "        priority: float, priority of state-action pair node.\n",
    "    \"\"\"\n",
    "    def __init__(self, i):\n",
    "        # Create environment state-action successor state arrrays\n",
    "        self.s_idx = -i\n",
    "        self.a_idx = i\n",
    "        self.priority = np.finfo(float).min\n",
    "\n",
    "class PriorityQueue:\n",
    "    \"\"\"Class to create a priority queue.\n",
    "\n",
    "    Fields:\n",
    "        p_q: array, priority queue that contains num_non_term_states *\n",
    "            max_num_actions `PriorityQueueNode`s\n",
    "        cur_p_q_size: int, current number of active nodes in priority queue.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_non_term_states, max_num_actions):\n",
    "        self.p_q = np.empty(\n",
    "            shape=[num_non_term_states * max_num_actions], dtype=object)\n",
    "        for i in range(0, num_non_term_states * max_num_actions):\n",
    "            self.p_q[i] = PriorityQueueNode(i)\n",
    "        self.p_q[0].priority = np.finfo(float).max\n",
    "\n",
    "        self.cur_p_q_size = 0\n",
    "\n",
    "    def search_and_update_p_q(self, s_idx, a_idx, priority):\n",
    "        \"\"\"Searches for and updates a node in the priority queue.\n",
    "\n",
    "        Args:\n",
    "            s_idx: int, state index.\n",
    "            a_idx: int, action index.\n",
    "            priority: float, priority of state-action pair node.\n",
    "        Returns:\n",
    "            q: array[float], keeps track of the estimated value of each\n",
    "                state-action pair Q(s, a).\n",
    "        \"\"\"\n",
    "        p_q_idx = -1\n",
    "        p_q_idx = self.search_p_q(s_idx, a_idx)\n",
    "\n",
    "        # Check if node was found\n",
    "        if p_q_idx >= 0:\n",
    "            # Check if found node has a lower priority saved than new priority\n",
    "            if self.p_q[p_q_idx].priority < priority:\n",
    "                self.p_q_node_increase_priority(p_q_idx, priority)\n",
    "        else:\n",
    "            # Node wasn't found so insert into priority queue\n",
    "            self.insert_into_p_q(s_idx, a_idx, priority)\n",
    "\n",
    "    def search_p_q(self, s_idx, a_idx):\n",
    "        \"\"\"Searches for a node in the priority queue.\n",
    "\n",
    "        Args:\n",
    "            s_idx: int, state index.\n",
    "            a_idx: int, action index.\n",
    "        Returns:\n",
    "            p_q_idx: int, index of priority queue node.\n",
    "        \"\"\"\n",
    "        p_q_idx = -1\n",
    "\n",
    "        # Search up to all nodes in worst case\n",
    "        for i in range(0, self.cur_p_q_size):\n",
    "            if (self.p_q[i].s_idx == s_idx and self.p_q[i].a_idx == a_idx):\n",
    "                p_q_idx = i\n",
    "                break  # break i loop since we found node\n",
    "\n",
    "        return p_q_idx\n",
    "\n",
    "    def p_q_node_increase_priority(self, p_q_idx, new_priority):\n",
    "        \"\"\"Increases priority of a node in the priority queue.\n",
    "\n",
    "        Increases priority at p_q_idx to new_priority, where it is assumed\n",
    "        that new_priority is greater than priority_queue[p_q_idx].\n",
    "\n",
    "        Args:\n",
    "            p_q_idx: int, index of priority queue node.\n",
    "            new_priority: float, new priority of state-action pair node.\n",
    "        \"\"\"\n",
    "        self.p_q[p_q_idx].priority = new_priority\n",
    "\n",
    "        while (p_q_idx != 0 and\n",
    "               self.p_q[self.get_par_idx(p_q_idx)].priority <\n",
    "               self.p_q[p_q_idx].priority):\n",
    "            (self.p_q[p_q_idx],\n",
    "             self.p_q[self.get_par_idx(p_q_idx)]) = self.swap_p_q_nodes(\n",
    "                self.p_q[p_q_idx],\n",
    "                self.p_q[self.get_par_idx(p_q_idx)])\n",
    "            p_q_idx = self.get_par_idx(p_q_idx)\n",
    "\n",
    "    def insert_into_p_q(self, s_idx, a_idx, priority):\n",
    "        \"\"\"Inserts a node into the priority queue.\n",
    "\n",
    "        Args:\n",
    "            s_idx: int, state index.\n",
    "            a_idx: int, action index.\n",
    "            priority: float, priority of state-action pair node.\n",
    "        \"\"\"\n",
    "        # First insert the new node at the end\n",
    "        self.cur_p_q_size += 1\n",
    "        p_q_idx = self.cur_p_q_size - 1\n",
    "\n",
    "        self.p_q[p_q_idx].s_idx = s_idx\n",
    "        self.p_q[p_q_idx].a_idx = a_idx\n",
    "        self.p_q[p_q_idx].priority = priority\n",
    "\n",
    "        # Fix the max heap property if it is violated\n",
    "        while (p_q_idx != 0 and\n",
    "               self.p_q[self.get_par_idx(p_q_idx)].priority <\n",
    "               self.p_q[p_q_idx].priority):\n",
    "            self.p_q[p_q_idx], self.p_q[self.get_par_idx(p_q_idx)] = \\\n",
    "                self.swap_p_q_nodes(\n",
    "                    self.p_q[p_q_idx], self.p_q[self.get_par_idx(p_q_idx)])\n",
    "            self.p_q_idx = self.get_par_idx(p_q_idx)\n",
    "\n",
    "    def pop_max_node_from_p_q(self):\n",
    "        \"\"\"Pops max node off from priority queue.\n",
    "\n",
    "        Returns:\n",
    "            s_idx: int, state index.\n",
    "            a_idx: int, action index.\n",
    "        \"\"\"\n",
    "        if self.cur_p_q_size == 1:\n",
    "            self.cur_p_q_size -= 1\n",
    "            return self.p_q[0].s_idx, self.p_q[0].a_idx\n",
    "\n",
    "        # Store the maximum value, and remove it from heap\n",
    "        s_idx = self.p_q[0].s_idx\n",
    "        a_idx = self.p_q[0].a_idx\n",
    "\n",
    "        self.p_q[0].s_idx = self.p_q[self.cur_p_q_size - 1].s_idx\n",
    "        self.p_q[0].a_idx = self.p_q[self.cur_p_q_size - 1].a_idx\n",
    "        self.p_q[0].priority = self.p_q[self.cur_p_q_size - 1].priority\n",
    "        self.cur_p_q_size -= 1\n",
    "\n",
    "        # Fix the max heap property if it is violated\n",
    "        self.max_heapify_p_q(0)\n",
    "\n",
    "        return s_idx, a_idx\n",
    "\n",
    "    def max_heapify_p_q(self, p_q_idx):\n",
    "        \"\"\"Max heapifies a subtree of priority queue.\n",
    "\n",
    "        Recursively heapifies a subtree with the root at given index, however\n",
    "            assumes that the subtrees are already heapified.\n",
    "\n",
    "        Args:\n",
    "            p_q_idx: int, index of priority queue node.\n",
    "        \"\"\"\n",
    "        l = self.get_left_idx(p_q_idx)\n",
    "        r = self.get_right_idx(p_q_idx)\n",
    "        biggest = p_q_idx\n",
    "\n",
    "        if (l < self.cur_p_q_size and\n",
    "                self.p_q[l].priority > self.p_q[p_q_idx].priority):\n",
    "            biggest = l\n",
    "\n",
    "        if (r < self.cur_p_q_size and\n",
    "                self.p_q[r].priority > self.p_q[biggest].priority):\n",
    "            biggest = r\n",
    "\n",
    "        if biggest != p_q_idx:\n",
    "            temp_s_idx = self.p_q[p_q_idx].s_idx\n",
    "            temp_a_idx = self.p_q[p_q_idx].a_idx\n",
    "            temp_priority = self.p_q[p_q_idx].priority\n",
    "\n",
    "            self.p_q[p_q_idx].s_idx = self.p_q[biggest].s_idx\n",
    "            self.p_q[p_q_idx].a_idx = self.p_q[biggest].a_idx\n",
    "            self.p_q[p_q_idx].priority = self.p_q[biggest].priority\n",
    "\n",
    "            self.p_q[biggest].s_idx = temp_s_idx\n",
    "            self.p_q[biggest].a_idx = temp_a_idx\n",
    "            self.p_q[biggest].priority = temp_priority\n",
    "\n",
    "            self.max_heapify_p_q(biggest)\n",
    "\n",
    "    def swap_p_q_nodes(self, x, y):\n",
    "        \"\"\"Swaps attributes between two `PriorityQueueNode`s.\n",
    "\n",
    "        Args:\n",
    "            x: instance of `PriorityQueueNode`.\n",
    "            y: instance of `PriorityQueueNode`.\n",
    "        Returns:\n",
    "            x: instance of `PriorityQueueNode`.\n",
    "            y: instance of `PriorityQueueNode`.\n",
    "        \"\"\"\n",
    "        temp_s_idx = x.s_idx\n",
    "        temp_a_idx = x.a_idx\n",
    "        temp_priority = x.priority\n",
    "\n",
    "        x.s_idx = y.s_idx\n",
    "        x.a_idx = y.a_idx\n",
    "        x.priority = y.priority\n",
    "\n",
    "        y.s_idx = temp_s_idx\n",
    "        y.a_idx = temp_a_idx\n",
    "        y.priority = temp_priority\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def get_par_idx(self, p_q_idx):\n",
    "        \"\"\"Gets the parent index of given priority queue node's index.\n",
    "\n",
    "        Args:\n",
    "            p_q_idx: int, index of priority queue node.\n",
    "        \"\"\"\n",
    "        return (p_q_idx - 1) // 2\n",
    "\n",
    "    def get_left_idx(self, p_q_idx):\n",
    "        \"\"\"Gets the left child index of given priority queue node's index.\n",
    "\n",
    "        Args:\n",
    "            p_q_idx: int, index of priority queue node.\n",
    "        \"\"\"\n",
    "        return (2 * p_q_idx + 1)\n",
    "\n",
    "    def get_right_idx(self, p_q_idx):\n",
    "        \"\"\"Gets the right child index of given priority queue node's index.\n",
    "\n",
    "        Args:\n",
    "            p_q_idx: int, index of priority queue node.\n",
    "        \"\"\"\n",
    "        return (2 * p_q_idx + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameters():\n",
    "    \"\"\"Sets hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        num_episodes: int, number of episodes to train over.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        num_planning_steps: int, number of steps for the planning stage.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        theta: float, small threshold for adding state-action pairs to priority\n",
    "            queue.\n",
    "    \"\"\"\n",
    "    num_episodes = 10000\n",
    "    maximum_episode_length = 200\n",
    "    num_planning_steps = 1\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.1\n",
    "    gamma = 1.0\n",
    "    theta = 0.0\n",
    "\n",
    "    return (num_episodes,\n",
    "            maximum_episode_length,\n",
    "            num_planning_steps,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            gamma,\n",
    "            theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_function_arrays(num_states, max_num_actions):\n",
    "    \"\"\"Creates value function arrays.\n",
    "\n",
    "    Args:\n",
    "        num_states: int, number of states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "    \"\"\"\n",
    "    return np.zeros(shape=[num_states, max_num_actions], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_arrays(num_non_term_states, max_num_actions):\n",
    "    \"\"\"Creates policy arrays.\n",
    "\n",
    "    Args:\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    policy = np.repeat(\n",
    "        a=1.0 / max_num_actions,\n",
    "        repeats=num_non_term_states * max_num_actions)\n",
    "\n",
    "    policy = np.reshape(\n",
    "        a=policy,\n",
    "        newshape=(num_non_term_states, max_num_actions))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_epsiode(num_non_term_states):\n",
    "    \"\"\"Initializes epsiode with initial state and initial action.\n",
    "\n",
    "    Args:\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "    Returns:\n",
    "        init_s_idx: int, initial state index from set of non terminal states.\n",
    "    \"\"\"\n",
    "    # Randomly choose an initial state from all non-terminal states\n",
    "    init_s_idx = np.random.randint(\n",
    "        low=0, high=num_non_term_states, dtype=np.int64)\n",
    "\n",
    "    return init_s_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_from_state_action_function(\n",
    "        max_num_actions, q, epsilon, s_idx, policy):\n",
    "    \"\"\"Create epsilon-greedy policy from state-action value function.\n",
    "\n",
    "    Args:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        s_idx: int, current state index.\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "    Returns:\n",
    "        policy: array[float], learned stochastic policy of which action a to\n",
    "            take in state s.\n",
    "    \"\"\"\n",
    "    # Save max state-action value and find the number of actions that have the\n",
    "    # same max state-action value\n",
    "    max_action_value = np.max(a=q[s_idx, :])\n",
    "    max_action_count = np.count_nonzero(a=q[s_idx, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs\n",
    "    # that have the same value and zero otherwise\n",
    "    if max_action_count == max_num_actions:\n",
    "        max_policy_prob_per_action = 1.0 / max_action_count\n",
    "        remain_prob_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_prob_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remain_prob_per_action = epsilon / (max_num_actions - max_action_count)\n",
    "\n",
    "    policy[s_idx, :] = np.where(\n",
    "        q[s_idx, :] == max_action_value,\n",
    "        max_policy_prob_per_action,\n",
    "        remain_prob_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_episode(\n",
    "        num_non_term_states,\n",
    "        max_num_actions,\n",
    "        environment,\n",
    "        model,\n",
    "        priority_queue,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        theta,\n",
    "        maximum_episode_length,\n",
    "        num_planning_steps,\n",
    "        s_idx):\n",
    "    \"\"\"Loops through episode to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        environment: instance of `Environment` class that holds environment\n",
    "            properties that are hidden from us, but that we can sample.\n",
    "        model: instance of `Model` class that holds model properties\n",
    "            that we learn through experience.\n",
    "        priority_queue: instance of `PriorityQueue` class, an array of\n",
    "            `PriorityQueueNode`s that keep track of the state index,\n",
    "            action index, and priority of state-action pairs.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        theta: float, small threshold for adding state-action pairs to priority\n",
    "            queue.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        num_planning_steps: int, number of steps for the planning stage.\n",
    "        s_idx: int, current state index.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        model: instance of `Model` class that holds model properties\n",
    "            that we learn through experience.\n",
    "        priority_queue: instance of `PriorityQueue` class, an array of\n",
    "            `PriorityQueueNode`s that keep track of the state index,\n",
    "            action index, and priority of state-action pairs.\n",
    "    \"\"\"\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Get epsilon-greedy action\n",
    "        a_idx, policy = select_action_from_epsilon_greedy_policy(\n",
    "            max_num_actions, q, epsilon, s_idx, policy)\n",
    "\n",
    "        # Update what state and actions the model has seen\n",
    "        model.update_model_seen_state_actions(s_idx, a_idx)\n",
    "\n",
    "        # Get reward\n",
    "        reward, sst_idx = observe_reward(s_idx, a_idx, environment)\n",
    "\n",
    "        # Get next state\n",
    "        next_s_idx = environment.sp_idx[s_idx, a_idx, sst_idx]\n",
    "\n",
    "        # Update model from environment experience\n",
    "        model.update_model_of_environment_from_experience(\n",
    "            s_idx, a_idx, reward, next_s_idx)\n",
    "\n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_s_idx >= num_non_term_states:\n",
    "            # Calculate priority\n",
    "            priority = np.abs(reward - q[s_idx, a_idx])\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            next_a_idx = select_max_q_action(next_s_idx, max_num_actions, q)\n",
    "\n",
    "            # Calculate priority\n",
    "            delta = gamma * q[next_s_idx][next_a_idx] - q[s_idx][a_idx]\n",
    "            priority = np.abs(reward + delta)\n",
    "\n",
    "        # Check if priority is over threshold to add to priority queue\n",
    "        if priority > theta:\n",
    "            priority_queue.search_and_update_p_q(\n",
    "                s_idx, a_idx, priority)\n",
    "\n",
    "        # Use updated model to simulate experience in planning phase\n",
    "        q, priority_queue = model.model_simulate_planning(\n",
    "            num_planning_steps,\n",
    "            num_non_term_states,\n",
    "            max_num_actions,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            theta,\n",
    "            q,\n",
    "            priority_queue)\n",
    "\n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_s_idx >= num_non_term_states:\n",
    "            break  # break i loop\n",
    "\n",
    "        # Update state to next state\n",
    "        s_idx = next_s_idx\n",
    "\n",
    "    return q, policy, model, priority_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_from_epsilon_greedy_policy(\n",
    "        max_num_actions, q, epsilon, s_idx, policy):\n",
    "    \"\"\"Selects an action in state s_idx from epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        s_idx: int, current state index.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    Returns:\n",
    "        a_idx: int, current action index.\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "    \"\"\"\n",
    "    # Choose policy for chosen state by epsilon-greedy choosing from the\n",
    "    # state-action-value function\n",
    "    policy = epsilon_greedy_policy_from_state_action_function(\n",
    "        max_num_actions, q, epsilon, s_idx, policy)\n",
    "\n",
    "    # Get epsilon-greedy action\n",
    "    a_idx = np.random.choice(a=max_num_actions, p=policy[s_idx, :])\n",
    "\n",
    "    return a_idx, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_reward(s_idx, a_idx, system):\n",
    "    \"\"\"Observes the reward from the given system (environment or model).\n",
    "\n",
    "    Args:\n",
    "        s_idx: int, current state index.\n",
    "        a_idx: int, current action index.\n",
    "        system: either an instance of `Environment` or `Model` class that\n",
    "            holds environment or model properties, respectively.\n",
    "    Returns:\n",
    "        reward: float, reward of taking action a_idx in state s_idx.\n",
    "        sst_idx: int, successor state transition index.\n",
    "    \"\"\"\n",
    "    sst_idx = np.random.choice(\n",
    "        a=system.num_sp[s_idx, a_idx],\n",
    "        p=system.p[s_idx, a_idx][:])\n",
    "\n",
    "    reward = system.r[s_idx, a_idx][sst_idx]\n",
    "\n",
    "    return reward, sst_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_max_q_action(s_idx, max_num_actions, q):\n",
    "    \"\"\"Selects action with max state-action-value function for given state.\n",
    "\n",
    "    Args:\n",
    "        s_idx: int, current state index.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "    Returns:\n",
    "        next_a_idx: int, next action index.\n",
    "    \"\"\"\n",
    "    max_action_value = np.max(a=q[s_idx, :])\n",
    "    max_action_stack = np.extract(\n",
    "        condition=q[s_idx, :] == max_action_value,\n",
    "        arr=np.arange(max_num_actions))\n",
    "\n",
    "    next_a_idx = np.random.choice(a=max_action_stack)\n",
    "\n",
    "    return next_a_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_planning_and_learning_prioritized_sweeping(\n",
    "        num_non_term_states,\n",
    "        max_num_actions,\n",
    "        environment,\n",
    "        model,\n",
    "        priority_queue,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        theta,\n",
    "        num_episodes,\n",
    "        maximum_episode_length,\n",
    "        num_planning_steps):\n",
    "    \"\"\"Loops through episodes to iteratively update policy.\n",
    "\n",
    "    Args:\n",
    "        num_non_term_states: int, number of non terminal states.\n",
    "        max_num_actions: int, max number of actions possible.\n",
    "        environment: instance of `Environment` class that holds environment\n",
    "            properties that are hidden from us, but that we can sample.\n",
    "        model: instance of `Model` class that holds model properties\n",
    "            that we learn through experience.\n",
    "        priority_queue: instance of `PriorityQueue` class, an array of\n",
    "            `PriorityQueueNode`s that keep track of the state index,\n",
    "            action index, and priority of state-action pairs.\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        alpha: float, alpha > 0, learning rate.\n",
    "        epsilon: float, 0 <= epsilon <= 1, exploitation-exploration trade-off,\n",
    "            higher means more exploration.\n",
    "        gamma: float, 0 <= gamma <= 1, amount to discount future reward.\n",
    "        theta: float, small threshold for adding state-action pairs to priority\n",
    "            queue.\n",
    "        num_episodes: int, number of episodes to train over.\n",
    "        maximum_episode_length: int, max number of timesteps for an episode.\n",
    "        num_planning_steps: int, number of steps for the planning stage.\n",
    "        s_idx: int, current state index.\n",
    "    Returns:\n",
    "        q: array[float], keeps track of the estimated value of each\n",
    "            state-action pair Q(s, a).\n",
    "        policy: array[float], learned stochastic policy of which\n",
    "            action a to take in state s.\n",
    "        model: instance of `Model` class that holds model properties\n",
    "            that we learn through experience.\n",
    "        priority_queue: instance of `PriorityQueue` class, an array of\n",
    "            `PriorityQueueNode`s that keep track of the state index,\n",
    "            action index, and priority of state-action pairs.\n",
    "    \"\"\"\n",
    "    for episode in range(0, num_episodes):\n",
    "        # Initialize episode to get initial state\n",
    "        init_s_idx = initialize_epsiode(num_non_term_states)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        q, policy, model, priority_queue = loop_through_episode(\n",
    "            num_non_term_states,\n",
    "            max_num_actions,\n",
    "            environment,\n",
    "            model,\n",
    "            priority_queue,\n",
    "            q,\n",
    "            policy,\n",
    "            alpha,\n",
    "            epsilon,\n",
    "            gamma,\n",
    "            theta,\n",
    "            maximum_episode_length,\n",
    "            num_planning_steps,\n",
    "            init_s_idx)\n",
    "\n",
    "    return q, policy, model, priority_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm():\n",
    "    \"\"\"Runs the algorithm.\n",
    "\n",
    "    Returns:\n",
    "        model: instance of `Model` class that holds model properties\n",
    "            that we learn through experience.\n",
    "    \"\"\"\n",
    "    (num_states,\n",
    "     _,\n",
    "     num_non_term_states,\n",
    "     max_num_actions,\n",
    "     _) = create_known_environment()\n",
    "\n",
    "    environment = Environment(\n",
    "        num_states, num_non_term_states, max_num_actions)\n",
    "    model = Model(num_states, num_non_term_states, max_num_actions)\n",
    "    priority_queue = PriorityQueue(num_non_term_states, max_num_actions)\n",
    "\n",
    "    (num_episodes,\n",
    "     maximum_episode_length,\n",
    "     num_planning_steps,\n",
    "     alpha,\n",
    "     epsilon,\n",
    "     gamma,\n",
    "     theta) = set_hyperparameters()\n",
    "\n",
    "    q = create_value_function_arrays(num_states, max_num_actions)\n",
    "\n",
    "    policy = create_policy_arrays(num_non_term_states, max_num_actions)\n",
    "\n",
    "    # Print initial arrays\n",
    "    print(\"\\nInitial state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nInitial policy\")\n",
    "    print(policy)\n",
    "\n",
    "    # Run off policy planning and learning prioritized sweeping\n",
    "    (q,\n",
    "     policy,\n",
    "     model,\n",
    "     priority_queue) = off_policy_planning_and_learning_prioritized_sweeping(\n",
    "        num_non_term_states,\n",
    "        max_num_actions,\n",
    "        environment,\n",
    "        model,\n",
    "        priority_queue,\n",
    "        q,\n",
    "        policy,\n",
    "        alpha,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        theta,\n",
    "        num_episodes,\n",
    "        maximum_episode_length,\n",
    "        num_planning_steps)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nFinal state-action value function\")\n",
    "    print(q)\n",
    "\n",
    "    print(\"\\nFinal policy\")\n",
    "    print(policy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-2.99999917 -1.99999943 -0.99999971 -2.99999917]\n",
      " [-3.99999891 -2.99999917 -1.99999943 -3.99999891]\n",
      " [-3.99999891 -3.99999891 -2.99999917 -2.9999992 ]\n",
      " [-2.99999917 -0.99999971 -1.99999943 -2.99999917]\n",
      " [-3.99999891 -1.99999946 -1.99999943 -3.99999891]\n",
      " [-2.9999992  -2.99999917 -2.99999917 -2.99999917]\n",
      " [-2.9999992  -3.99999891 -3.99999891 -1.99999946]\n",
      " [-3.99999891 -1.99999946 -2.99999917 -3.99999891]\n",
      " [-2.99999917 -2.99999917 -2.9999992  -2.9999992 ]\n",
      " [-1.99999943 -3.99999891 -3.99999891 -1.99999946]\n",
      " [-1.99999946 -2.9999992  -2.99999917 -0.99999971]\n",
      " [-2.9999992  -2.9999992  -3.99999891 -3.99999891]\n",
      " [-1.99999946 -3.99999891 -3.99999891 -2.9999992 ]\n",
      " [-0.99999971 -2.99999917 -2.9999992  -1.99999946]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Final policy\n",
      "[[0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.05       0.05       0.45       0.45      ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]]\n"
     ]
    }
   ],
   "source": [
    "model = run_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.num_seen_non_term_states\n",
      "14\n",
      "model.seen_non_term_s_stack\n",
      "[12 11  7  3  8  4  5  6  9 13  0  1  2 10]\n",
      "model.seen_non_term_s_stack_rev_lu\n",
      "[10 11 12  3  5  6  7  2  4  8 13  1  0  9]\n",
      "model.num_seen_non_term_s_a\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "model.seen_non_term_s_a_stack\n",
      "[[0 1 2 3]\n",
      " [0 3 1 2]\n",
      " [3 1 0 2]\n",
      " [3 2 0 1]\n",
      " [2 0 1 3]\n",
      " [0 3 1 2]\n",
      " [0 2 3 1]\n",
      " [1 3 0 2]\n",
      " [1 0 2 3]\n",
      " [3 2 0 1]\n",
      " [3 2 0 1]\n",
      " [2 1 3 0]\n",
      " [2 1 3 0]\n",
      " [2 3 1 0]]\n",
      "model.seen_non_term_s_a_stack_rev_lu\n",
      "[[0 1 2 3]\n",
      " [0 2 3 1]\n",
      " [2 1 3 0]\n",
      " [2 3 1 0]\n",
      " [1 2 0 3]\n",
      " [0 2 3 1]\n",
      " [0 3 1 2]\n",
      " [2 0 3 1]\n",
      " [1 0 2 3]\n",
      " [2 3 1 0]\n",
      " [2 3 1 0]\n",
      " [3 1 0 2]\n",
      " [3 1 0 2]\n",
      " [3 2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print model seen arrays\n",
    "print(\"model.num_seen_non_term_states\")\n",
    "print(model.num_seen_non_term_states)\n",
    "print(\"model.seen_non_term_s_stack\")\n",
    "print(model.seen_non_term_s_stack)\n",
    "print(\"model.seen_non_term_s_stack_rev_lu\")\n",
    "print(model.seen_non_term_s_stack_rev_lu)\n",
    "print(\"model.num_seen_non_term_s_a\")\n",
    "print(model.num_seen_non_term_s_a)\n",
    "print(\"model.seen_non_term_s_a_stack\")\n",
    "print(model.seen_non_term_s_a_stack)\n",
    "print(\"model.seen_non_term_s_a_stack_rev_lu\")\n",
    "print(model.seen_non_term_s_a_stack_rev_lu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.num_sp\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "model.sp_idx\n",
      "[[list([1]) list([0]) list([14]) list([4])]\n",
      " [list([2]) list([1]) list([0]) list([5])]\n",
      " [list([2]) list([2]) list([1]) list([6])]\n",
      " [list([4]) list([14]) list([3]) list([7])]\n",
      " [list([5]) list([0]) list([3]) list([8])]\n",
      " [list([6]) list([1]) list([4]) list([9])]\n",
      " [list([6]) list([2]) list([5]) list([10])]\n",
      " [list([8]) list([3]) list([7]) list([11])]\n",
      " [list([9]) list([4]) list([7]) list([12])]\n",
      " [list([10]) list([5]) list([8]) list([13])]\n",
      " [list([10]) list([6]) list([9]) list([15])]\n",
      " [list([12]) list([7]) list([11]) list([11])]\n",
      " [list([13]) list([8]) list([11]) list([12])]\n",
      " [list([15]) list([9]) list([12]) list([13])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.p\n",
      "[[list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.r\n",
      "[[list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.s_a_ss_num_visits\n",
      "[[list([116]) list([118]) list([2770]) list([102])]\n",
      " [list([70]) list([39]) list([1258]) list([46])]\n",
      " [list([39]) list([35]) list([444]) list([349])]\n",
      " [list([99]) list([2228]) list([77]) list([63])]\n",
      " [list([52]) list([1004]) list([416]) list([58])]\n",
      " [list([286]) list([122]) list([178]) list([320])]\n",
      " [list([47]) list([52]) list([48]) list([1305])]\n",
      " [list([50]) list([1227]) list([56]) list([36])]\n",
      " [list([162]) list([391]) list([178]) list([212])]\n",
      " [list([345]) list([51]) list([46]) list([929])]\n",
      " [list([67]) list([89]) list([81]) list([2166])]\n",
      " [list([500]) list([328]) list([20]) list([62])]\n",
      " [list([1407]) list([46]) list([59]) list([48])]\n",
      " [list([2836]) list([111]) list([101]) list([106])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n"
     ]
    }
   ],
   "source": [
    "# Print model successor arrays\n",
    "print(\"model.num_sp\")\n",
    "print(model.num_sp)\n",
    "print(\"model.sp_idx\")\n",
    "print(model.sp_idx)\n",
    "print(\"model.p\")\n",
    "print(model.p)\n",
    "print(\"model.r\")\n",
    "print(model.r)\n",
    "print(\"model.s_a_ss_num_visits\")\n",
    "print(model.s_a_ss_num_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
