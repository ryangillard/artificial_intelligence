{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/resnet.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResNet(object):\n",
    "    \"\"\"Class that contains methods that preprocess images through ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        params: dict, user passed parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        \"\"\"Initializes `ResNet` class instance.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "\n",
    "        self.resnet_model, self.pooling_layer = self.get_resnet_layers(\n",
    "            input_shape=(\n",
    "                self.params[\"image_height\"],\n",
    "                self.params[\"image_width\"],\n",
    "                self.params[\"image_depth\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_resnet_layers(self, input_shape):\n",
    "        \"\"\"Gets ResNet layers from ResNet50 model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, input shape of images.\n",
    "        \"\"\"\n",
    "        # Load the ResNet50 model.\n",
    "        resnet50_model = tf.keras.applications.resnet50.ResNet50(\n",
    "            include_top=False,\n",
    "            weights=self.params[\"resnet_weights\"],\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "        resnet50_model.trainable = False\n",
    "\n",
    "        # Create a new Model based on original resnet50 model ended after the\n",
    "        # chosen residual block.\n",
    "        layer_name = self.params[\"resnet_layer_name\"]\n",
    "        resnet50 = tf.keras.Model(\n",
    "            inputs=resnet50_model.input,\n",
    "            outputs=resnet50_model.get_layer(layer_name).output\n",
    "        )\n",
    "\n",
    "        # Add adaptive mean-spatial pooling after the new model.\n",
    "        adaptive_mean_spatial_layer = tf.keras.layers.GlobalAvgPool2D()\n",
    "\n",
    "        return resnet50, adaptive_mean_spatial_layer\n",
    "\n",
    "    def preprocess_image_batch(self, images):\n",
    "        \"\"\"Preprocesses batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: tensor, rank 4 image tensor of shape\n",
    "                (batch_size, image_height, image_width, image_depth).\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed images tensor.\n",
    "        \"\"\"\n",
    "        images = tf.cast(x=images, dtype=tf.float32)\n",
    "\n",
    "        if self.params[\"preprocess_input\"]:\n",
    "            images = tf.keras.applications.resnet50.preprocess_input(x=images)\n",
    "\n",
    "        return images\n",
    "\n",
    "    def get_image_resnet_feature_vectors(self, images):\n",
    "        \"\"\"Gets image ResNet feature vectors.\n",
    "\n",
    "        Args:\n",
    "            images: tensor, rank 4 image tensor of shape\n",
    "                (batch_size, image_height, image_width, image_depth).\n",
    "\n",
    "        Returns:\n",
    "            Processed ResNet feature rank 1 tensor for each image.\n",
    "        \"\"\"\n",
    "        preprocessed_images = self.preprocess_image_batch(images=images)\n",
    "        resnet_feature_image = self.resnet_model(inputs=preprocessed_images)\n",
    "        resnet_feature_vector = self.pooling_layer(inputs=resnet_feature_image)\n",
    "\n",
    "        return resnet_feature_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFW-IoDaHyZ3"
   },
   "source": [
    "## training_inputs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbfA7Mg3HyZ4",
    "outputId": "087e9bd6-1fe3-41a1-cb19-26b1f8c823fc"
   },
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/training_inputs.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    dtype_map = {\n",
    "        \"str\": tf.string,\n",
    "        \"int\": tf.int64,\n",
    "        \"float\": tf.float32\n",
    "    }\n",
    "\n",
    "    # Create feature schema map for protos.\n",
    "    tf_example_features = {\n",
    "        feat[\"name\"]: (\n",
    "            tf.io.FixedLenFeature(\n",
    "                shape=feat[\"shape\"], dtype=dtype_map[feat[\"dtype\"]]\n",
    "            )\n",
    "            if feat[\"type\"] == \"FixedLen\"\n",
    "            else tf.io.FixedLenSequenceFeature(\n",
    "                shape=feat[\"shape\"], dtype=dtype_map[feat[\"dtype\"]]\n",
    "            )\n",
    "        )\n",
    "        for feat in params[\"tf_record_example_schema\"]\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=protos, features=tf_example_features\n",
    "    )\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    if params[\"image_encoding\"] == \"raw\":\n",
    "        image = tf.io.decode_raw(\n",
    "            input_bytes=parsed_features[params[\"image_feature_name\"]],\n",
    "            out_type=tf.uint8\n",
    "        )\n",
    "    elif params[\"image_encoding\"] == \"png\":\n",
    "        image = tf.io.decode_png(\n",
    "            contents=parsed_features[params[\"image_feature_name\"]],\n",
    "            channels=params[\"image_depth\"]\n",
    "        )\n",
    "    elif params[\"image_encoding\"] == \"jpeg\":\n",
    "        image = tf.io.decode_jpeg(\n",
    "            contents=parsed_features[params[\"image_feature_name\"]],\n",
    "            channels=params[\"image_depth\"]\n",
    "        )\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[\n",
    "            params[\"image_height\"],\n",
    "            params[\"image_width\"],\n",
    "            params[\"image_depth\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_dataset(file_pattern, batch_size, params):\n",
    "    \"\"\"Reads TF Record data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read TF Record\n",
    "    dataset using Dataset API, apply necessary preprocessing, and return an\n",
    "    input function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: str, file pattern that to read into our tf.data dataset.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def fetch_dataset(filename):\n",
    "        \"\"\"Fetches TFRecord Dataset from given filename.\n",
    "\n",
    "        Args:\n",
    "            filename: str, name of TFRecord file.\n",
    "\n",
    "        Returns:\n",
    "            Dataset containing TFRecord Examples.\n",
    "        \"\"\"\n",
    "        buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=filename, buffer_size=buffer_size\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create dataset to contain list of files matching pattern.\n",
    "        dataset = tf.data.Dataset.list_files(\n",
    "            file_pattern=file_pattern, shuffle=False\n",
    "        )\n",
    "\n",
    "        # Parallel interleaves multiple files at once with map function.\n",
    "        dataset = dataset.apply(\n",
    "            tf.data.experimental.parallel_interleave(\n",
    "                map_func=fetch_dataset, cycle_length=64, sloppy=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Decode TF Record Example into a features dictionary of tensors.\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x: decode_example(\n",
    "                protos=x, params=params\n",
    "            ),\n",
    "            num_parallel_calls=(\n",
    "                tf.contrib.data.AUTOTUNE\n",
    "                if params[\"input_fn_autotune\"]\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Batch dataset and drop remainder so there are no partial batches.\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(\n",
    "            buffer_size=(\n",
    "                tf.data.experimental.AUTOTUNE\n",
    "                if params[\"input_fn_autotune\"]\n",
    "                else 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## covariance.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/covariance.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CovarianceMatrix(object):\n",
    "    \"\"\"Class that batch updates covariance matrix.\n",
    "\n",
    "    Attributes:\n",
    "        params: dict, user passed parameters.\n",
    "        seen_example_count: tf.Variable, rank 0 of shape () containing\n",
    "            the count of the number of examples seen so far.\n",
    "        col_means_vector: tf.Variable, rank 1 of shape (num_cols,) containing\n",
    "            column means.\n",
    "        covariance_matrix: tf.Variable, rank 2 of shape (num_cols, num_cols)\n",
    "            containing covariance matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        \"\"\"Initializes `CovarianceMatrix` class instance.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "\n",
    "        self.seen_example_count = tf.Variable(\n",
    "            initial_value=tf.zeros(shape=(), dtype=tf.int64), trainable=False\n",
    "        )\n",
    "        self.col_means_vector = tf.Variable(\n",
    "            initial_value=tf.zeros(\n",
    "                shape=(self.params[\"num_cols\"],), dtype=tf.float32\n",
    "            ),\n",
    "            trainable=False\n",
    "        )\n",
    "        self.covariance_matrix = tf.Variable(\n",
    "            initial_value=tf.zeros(\n",
    "                shape=(self.params[\"num_cols\"], self.params[\"num_cols\"]),\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def assign_seen_example_count(self, seen_example_count):\n",
    "        \"\"\"Assigns seen example count tf.Variable.\n",
    "\n",
    "        Args:\n",
    "            seen_example_count: tensor, rank 0 of shape () containing\n",
    "            the count of the number of examples seen so far.\n",
    "        \"\"\"\n",
    "        self.seen_example_count.assign(value=seen_example_count)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_col_means_vector(self, col_means_vector):\n",
    "        \"\"\"Assigns column means vector tf.Variable.\n",
    "\n",
    "        Args:\n",
    "            col_means_vector: tensor, rank 1 of shape (num_cols,) containing\n",
    "            column means.\n",
    "        \"\"\"\n",
    "        self.col_means_vector.assign(value=col_means_vector)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_covariance_matrix(self, covariance_matrix):\n",
    "        \"\"\"Assigns covariance matrix tf.Variable.\n",
    "\n",
    "        Args:\n",
    "            covariance_matrix: tensor, rank 2 of shape (num_cols, num_cols)\n",
    "            containing covariance matrix.\n",
    "        \"\"\"\n",
    "        self.covariance_matrix.assign(value=covariance_matrix)\n",
    "\n",
    "    def update_example_count(self, count_a, count_b):\n",
    "        \"\"\"Updates the running number of examples processed.\n",
    "\n",
    "        Given previous running total and current batch size, return new\n",
    "        running total.\n",
    "\n",
    "        Args:\n",
    "            count_a: tensor, tf.int64 rank 0 tensor of previous running total\n",
    "                of examples.\n",
    "            count_b: tensor, tf.int64 rank 0 tensor of current batch size.\n",
    "\n",
    "        Returns:\n",
    "            A tf.int64 rank 0 tensor of new running total of examples.\n",
    "        \"\"\"\n",
    "        return count_a + count_b\n",
    "\n",
    "    def update_mean_incremental(self, count_a, mean_a, value_b):\n",
    "        \"\"\"Updates the running mean vector incrementally.\n",
    "\n",
    "        Given previous running total, running column means, and single\n",
    "            example's column values, return new running column means.\n",
    "\n",
    "        Args:\n",
    "            count_a: tensor, tf.int64 rank 0 tensor of previous running total\n",
    "                of examples.\n",
    "            mean_a: tensor, tf.float32 rank 1 tensor of previous running column\n",
    "                means.\n",
    "            value_b: tensor, tf.float32 rank 1 tensor of single example's\n",
    "                column values.\n",
    "\n",
    "        Returns:\n",
    "            A tf.float32 rank 1 tensor of new running column means.\n",
    "        \"\"\"\n",
    "        umean_a = mean_a * tf.cast(x=count_a, dtype=tf.float32)\n",
    "        mean_ab_num = umean_a + tf.squeeze(input=value_b, axis=0)\n",
    "        mean_ab = mean_ab_num / tf.cast(x=count_a + 1, dtype=tf.float32)\n",
    "\n",
    "        return mean_ab\n",
    "\n",
    "    def update_covariance_incremental(\n",
    "        self, count_a, mean_a, cov_a, value_b, mean_ab, use_sample_covariance\n",
    "    ):\n",
    "        \"\"\"Updates the running covariance matrix incrementally.\n",
    "\n",
    "        Given previous running total, running column means, running covariance\n",
    "        matrix, single example's column values, new running column means, and\n",
    "        whether to use sample covariance or not, return new running covariance\n",
    "        matrix.\n",
    "\n",
    "        Args:\n",
    "            count_a: tensor, tf.int64 rank 0 tensor of previous running total\n",
    "                of examples.\n",
    "            mean_a: tensor, tf.float32 rank 1 tensor of previous running column\n",
    "                means.\n",
    "            cov_a: tensor, tf.float32 rank 2 tensor of previous running\n",
    "                covariance matrix.\n",
    "            value_b: tensor, tf.float32 rank 1 tensor of single example's\n",
    "                column values.\n",
    "            mean_ab: tensor, tf.float32 rank 1 tensor of new running column\n",
    "                means.\n",
    "            use_sample_covariance: bool, flag on whether sample or population\n",
    "                covariance is used.\n",
    "\n",
    "        Returns:\n",
    "            A tf.float32 rank 2 tensor of new covariance matrix.\n",
    "        \"\"\"\n",
    "        mean_diff = tf.matmul(\n",
    "                a=value_b - mean_a, b=value_b - mean_ab, transpose_a=True\n",
    "        )\n",
    "\n",
    "        if use_sample_covariance:\n",
    "            ucov_a = cov_a * tf.cast(x=count_a - 1, dtype=tf.float32)\n",
    "            cov_ab_denominator = tf.cast(x=count_a, dtype=tf.float32)\n",
    "        else:\n",
    "            ucov_a = cov_a * tf.cast(x=count_a, dtype=tf.float32)\n",
    "            cov_ab_denominator = tf.cast(x=count_a + 1, dtype=tf.float32)\n",
    "        cov_ab_numerator = ucov_a + mean_diff\n",
    "        cov_ab = cov_ab_numerator / cov_ab_denominator\n",
    "\n",
    "        return cov_ab\n",
    "\n",
    "    def singleton_batch_update(\n",
    "        self,\n",
    "        X,\n",
    "        running_count,\n",
    "        running_mean,\n",
    "        running_covariance,\n",
    "        use_sample_covariance\n",
    "    ):\n",
    "        \"\"\"Updates running tensors incrementally when batch_size equals 1.\n",
    "\n",
    "        Given the the data vector X, the tensor tracking running example\n",
    "        counts, the tensor tracking running column means, and the tensor\n",
    "        tracking running covariance matrix, returns updated running example\n",
    "        count tensor, column means tensor, and covariance matrix tensor.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, tf.float32 rank 2 tensor of input data.\n",
    "            running_count: tensor, tf.int64 rank 0 tensor tracking running\n",
    "                example counts.\n",
    "            running_mean: tensor, tf.float32 rank 1 tensor tracking running\n",
    "                column means.\n",
    "            running_covariance: tensor, tf.float32 rank 2 tensor tracking\n",
    "                running covariance matrix.\n",
    "            use_sample_covariance: bool, flag on whether sample or population\n",
    "                covariance is used.\n",
    "\n",
    "        Returns:\n",
    "            Updated updated running example count tensor, column means tensor,\n",
    "                and covariance matrix tensor.\n",
    "        \"\"\"\n",
    "        # shape = (num_cols, num_cols)\n",
    "        if running_count == 0:\n",
    "            # Would produce NaNs, so rollover example for next iteration.\n",
    "            self.rollover_singleton_example = X\n",
    "\n",
    "            # Update count though so that we don't end up in this block again.\n",
    "            count = self.update_example_count(\n",
    "                count_a=running_count, count_b=1\n",
    "            )\n",
    "\n",
    "            # No need to update mean or covariance this iteration\n",
    "            mean = running_mean\n",
    "            covariance = running_covariance\n",
    "        elif running_count == 1:\n",
    "            # Batch update since we're combining previous & current batches.\n",
    "            count, mean, covariance = self.non_singleton_batch_update(\n",
    "                batch_size=2,\n",
    "                X=tf.concat(\n",
    "                    values=[self.rollover_singleton_example, X], axis=0\n",
    "                ),\n",
    "                running_count=0,\n",
    "                running_mean=running_mean,\n",
    "                running_covariance=running_covariance,\n",
    "                use_sample_covariance=use_sample_covariance\n",
    "            )\n",
    "        else:\n",
    "            # Calculate new combined mean for incremental covariance matrix.\n",
    "            # shape = (num_cols,)\n",
    "            mean = self.update_mean_incremental(\n",
    "                count_a=running_count, mean_a=running_mean, value_b=X\n",
    "            )\n",
    "\n",
    "            # Update running tensors from single example\n",
    "            # shape = ()\n",
    "            count = self.update_example_count(\n",
    "                count_a=running_count, count_b=1\n",
    "            )\n",
    "\n",
    "            # shape = (num_cols, num_cols)\n",
    "            covariance = self.update_covariance_incremental(\n",
    "                count_a=running_count,\n",
    "                mean_a=running_mean,\n",
    "                cov_a=running_covariance,\n",
    "                value_b=X,\n",
    "                mean_ab=mean,\n",
    "                use_sample_covariance=use_sample_covariance\n",
    "            )\n",
    "\n",
    "        return count, mean, covariance\n",
    "\n",
    "    def update_mean_batch(self, count_a, mean_a, count_b, mean_b):\n",
    "        \"\"\"Updates the running mean vector with a batch of data.\n",
    "\n",
    "        Given previous running example count, running column means, current\n",
    "        batch size, and batch's column means, return new running column means.\n",
    "\n",
    "        Args:\n",
    "            count_a: tensor, tf.int64 rank 0 tensor of previous running total\n",
    "                of examples.\n",
    "            mean_a: tensor, tf.float32 rank 1 tensor of previous running column\n",
    "                means.\n",
    "            count_b: tensor, tf.int64 rank 0 tensor of current batch size.\n",
    "            mean_b: tensor, tf.float32 rank 1 tensor of batch's column means.\n",
    "\n",
    "        Returns:\n",
    "            A tf.float32 rank 1 tensor of new running column means.\n",
    "        \"\"\"\n",
    "        sum_a = mean_a * tf.cast(x=count_a, dtype=tf.float32)\n",
    "        sum_b = mean_b * tf.cast(x=count_b, dtype=tf.float32)\n",
    "        mean_ab_denominator = tf.cast(x=count_a + count_b, dtype=tf.float32)\n",
    "        mean_ab = (sum_a + sum_b) / mean_ab_denominator\n",
    "\n",
    "        return mean_ab\n",
    "\n",
    "    def update_covariance_batch(\n",
    "        self,\n",
    "        count_a,\n",
    "        mean_a,\n",
    "        cov_a,\n",
    "        count_b,\n",
    "        mean_b,\n",
    "        cov_b,\n",
    "        use_sample_covariance\n",
    "    ):\n",
    "        \"\"\"Updates the running covariance matrix with batch of data.\n",
    "\n",
    "        Given previous running example count, column means, and\n",
    "        covariance matrix, current batch size, column means, and covariance\n",
    "        matrix, and whether to use sample covariance or not, return new running\n",
    "        covariance matrix.\n",
    "\n",
    "        Args:\n",
    "            count_a: tensor, tf.int64 rank 0 tensor of previous running total\n",
    "                of examples.\n",
    "            mean_a: tensor, tf.float32 rank 1 tensor of previous running column\n",
    "                means.\n",
    "            cov_a: tensor, tf.float32 rank 2 tensor of previous running\n",
    "                covariance matrix.\n",
    "            count_b: tensor, tf.int64 rank 0 tensor of current batch size.\n",
    "            mean_b: tensor, tf.float32 rank 1 tensor of batch's column means.\n",
    "            cov_b: tensor, tf.float32 rank 2 tensor of batch's covariance\n",
    "                matrix.\n",
    "            use_sample_covariance: bool, flag on whether sample or population\n",
    "                covariance is used.\n",
    "\n",
    "        Returns:\n",
    "            A tf.float32 rank 2 tensor of new running covariance matrix.\n",
    "        \"\"\"\n",
    "        mean_diff = tf.expand_dims(input=mean_a - mean_b, axis=0)\n",
    "\n",
    "        if use_sample_covariance:\n",
    "            ucov_a = cov_a * tf.cast(x=count_a - 1, dtype=tf.float32)\n",
    "            ucov_b = cov_b * tf.cast(x=count_b - 1, dtype=tf.float32)\n",
    "            den = tf.cast(x=count_a + count_b - 1, dtype=tf.float32)\n",
    "        else:\n",
    "            ucov_a = cov_a * tf.cast(x=count_a, dtype=tf.float32)\n",
    "            ucov_b = cov_b * tf.cast(x=count_b, dtype=tf.float32)\n",
    "            den = tf.cast(x=count_a + count_b, dtype=tf.float32)\n",
    "\n",
    "        mean_diff = tf.matmul(a=mean_diff, b=mean_diff, transpose_a=True)\n",
    "        mean_scaling_num = tf.cast(x=count_a * count_b, dtype=tf.float32)\n",
    "        mean_scaling_den = tf.cast(x=count_a + count_b, dtype=tf.float32)\n",
    "        mean_scaling = mean_scaling_num / mean_scaling_den\n",
    "        cov_ab = (ucov_a + ucov_b + mean_diff * mean_scaling) / den\n",
    "\n",
    "        return cov_ab\n",
    "\n",
    "    def non_singleton_batch_update(\n",
    "        self,\n",
    "        batch_size,\n",
    "        X,\n",
    "        running_count,\n",
    "        running_mean,\n",
    "        running_covariance,\n",
    "        use_sample_covariance\n",
    "    ):\n",
    "        \"\"\"Updates running tensors when batch_size does NOT equal 1.\n",
    "\n",
    "        Given the current batch size, the data matrix X, the tensor tracking\n",
    "        running example counts, the tensor tracking running column means, and\n",
    "        the tensor tracking running covariance matrix, returns updated running\n",
    "        example count tensor, column means tensor, and covariance matrix\n",
    "        tensor.\n",
    "\n",
    "        Args:\n",
    "            batch_size: int, number of examples in current batch (could be\n",
    "                partial).\n",
    "            X: tensor, tf.float32 rank 2 tensor of input data.\n",
    "            running_count: tensor, tf.int64 rank 0 tensor tracking running\n",
    "                example counts.\n",
    "            running_mean: tensor, tf.float32 rank 1 tensor tracking running\n",
    "                column means.\n",
    "            running_covariance: tensor, tf.float32 rank 2 tensor tracking\n",
    "                running covariance matrix.\n",
    "            use_sample_covariance: bool, flag on whether sample or population\n",
    "                covariance is used.\n",
    "\n",
    "        Returns:\n",
    "            Updated updated running example count tensor, column means tensor,\n",
    "                and covariance matrix tensor.\n",
    "        \"\"\"\n",
    "        # shape = (num_cols,)\n",
    "        X_mean = tf.reduce_mean(input_tensor=X, axis=0)\n",
    "\n",
    "        # shape = (batch_size, num_cols)\n",
    "        X_centered = X - X_mean\n",
    "\n",
    "        # shape = (num_cols, num_cols)\n",
    "        X_cov = tf.matmul(\n",
    "                a=X_centered,\n",
    "                b=X_centered,\n",
    "                transpose_a=True\n",
    "        )\n",
    "        X_cov /= tf.cast(x=batch_size - 1, dtype=tf.float32)\n",
    "\n",
    "        # Update running tensors from batch statistics.\n",
    "        # shape = ()\n",
    "        count = self.update_example_count(\n",
    "            count_a=running_count, count_b=batch_size\n",
    "        )\n",
    "\n",
    "        # shape = (num_cols,)\n",
    "        mean = self.update_mean_batch(\n",
    "            count_a=running_count,\n",
    "            mean_a=running_mean,\n",
    "            count_b=batch_size,\n",
    "            mean_b=X_mean\n",
    "        )\n",
    "\n",
    "        # shape = (num_cols, num_cols)\n",
    "        covariance = self.update_covariance_batch(\n",
    "            count_a=running_count,\n",
    "            mean_a=running_mean,\n",
    "            cov_a=running_covariance,\n",
    "            count_b=batch_size,\n",
    "            mean_b=X_mean,\n",
    "            cov_b=X_cov,\n",
    "            use_sample_covariance=use_sample_covariance\n",
    "        )\n",
    "\n",
    "        return count, mean, covariance\n",
    "\n",
    "    def calculate_data_stats(self, data):\n",
    "        \"\"\"Calculates statistics of data.\n",
    "\n",
    "        Args:\n",
    "            data: tensor, rank 2 tensor of shape\n",
    "                (current_batch_size, num_cols) containing batch of input data.\n",
    "        \"\"\"\n",
    "        current_batch_size = data.shape[0]\n",
    "\n",
    "        if current_batch_size == 1:\n",
    "            (seen_example_count,\n",
    "             col_means_vector,\n",
    "             covariance_matrix) = self.singleton_batch_update(\n",
    "                X=data,\n",
    "                running_count=self.seen_example_count,\n",
    "                running_mean=self.col_means_vector,\n",
    "                running_covariance=self.covariance_matrix,\n",
    "                use_sample_covariance=self.params[\"use_sample_covariance\"]\n",
    "            )\n",
    "        else:\n",
    "            (seen_example_count,\n",
    "             col_means_vector,\n",
    "             covariance_matrix) = self.non_singleton_batch_update(\n",
    "                batch_size=current_batch_size,\n",
    "                X=data,\n",
    "                running_count=self.seen_example_count,\n",
    "                running_mean=self.col_means_vector,\n",
    "                running_covariance=self.covariance_matrix,\n",
    "                use_sample_covariance=self.params[\"use_sample_covariance\"]\n",
    "            )\n",
    "\n",
    "        self.assign_seen_example_count(seen_example_count=seen_example_count)\n",
    "        self.assign_col_means_vector(col_means_vector=col_means_vector)\n",
    "        self.assign_covariance_matrix(covariance_matrix=covariance_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pca.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/pca.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import covariance\n",
    "\n",
    "\n",
    "class PCA(covariance.CovarianceMatrix):\n",
    "    \"\"\"Class that performs PCA projection and reconstruction.\n",
    "\n",
    "    Attributes:\n",
    "        rollover_singleton_example: tensor, rank 2 tensor of shape\n",
    "            (1, num_cols) containing a rollover singleton example in case the\n",
    "            data batch size begins at 1. This avoids NaN covariances.\n",
    "        eigenvalues: tf.Variable, rank 1 of shape (num_cols,) containing the\n",
    "            eigenvalues of the covariance matrix.\n",
    "        eigenvectors: tf.Variable, rank 2 of shape (num_cols, num_cols)\n",
    "            containing the eigenvectors of the covariance matrix.\n",
    "        top_k_eigenvectors: tensor, rank 2 tensor of shape\n",
    "            (num_cols, top_k_pc) containing the eigenvectors associated with\n",
    "            the top_k_pc eigenvalues.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        \"\"\"Initializes `PCA` class instance.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        super().__init__(params=params)\n",
    "        self.params = params\n",
    "\n",
    "        self.rollover_singleton_example = None\n",
    "\n",
    "        self.eigenvalues = tf.Variable(\n",
    "            initial_value=tf.zeros(\n",
    "                shape=(self.params[\"num_cols\"],), dtype=tf.float32\n",
    "            ),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "        self.eigenvectors = tf.Variable(\n",
    "            initial_value=tf.zeros(\n",
    "                shape=(self.params[\"num_cols\"], self.params[\"num_cols\"]),\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "        self.top_k_eigenvectors = tf.zeros(\n",
    "            shape=(self.params[\"num_cols\"], self.params[\"top_k_pc\"])\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def assign_eigenvalues(self, eigenvalues):\n",
    "        \"\"\"Assigns covariance matrix eigenvalues tf.Variable.\n",
    "\n",
    "        Args:\n",
    "            eigenvalues: tensor, rank 1 of shape (num_cols,) containing the\n",
    "            eigenvalues of the covariance matrix.\n",
    "        \"\"\"\n",
    "        self.eigenvalues.assign(value=eigenvalues)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_eigenvectors(self, eigenvectors):\n",
    "        \"\"\"Assigns covariance matrix eigenvectors tf.Variable.\n",
    "\n",
    "        Args:\n",
    "            eigenvectors: tensor, rank 2 of shape (num_cols, num_cols)\n",
    "            containing the eigenvectors of the covariance matrix.\n",
    "        \"\"\"\n",
    "        self.eigenvectors.assign(value=eigenvectors)\n",
    "\n",
    "    def calculate_eigenvalues_and_eigenvectors(self):\n",
    "        \"\"\"Calculates eigenvalues and eigenvectors of data.\n",
    "        \"\"\"\n",
    "        # shape = (num_cols,) & (num_cols, num_cols)\n",
    "        eigenvalues, eigenvectors = tf.linalg.eigh(\n",
    "            tensor=self.covariance_matrix\n",
    "        )\n",
    "\n",
    "        self.assign_eigenvalues(eigenvalues=eigenvalues)\n",
    "        self.assign_eigenvectors(eigenvectors=eigenvectors)\n",
    "\n",
    "    def pca_projection_to_top_k_pc(self, data):\n",
    "        \"\"\"Projects data down to top_k principal components.\n",
    "\n",
    "        Args:\n",
    "            data: tensor, rank 2 tensor of shape (num_examples, num_cols)\n",
    "                containing batch of input data.\n",
    "\n",
    "        Returns:\n",
    "            Rank 2 tensor of shape (num_examples, top_k_pc) containing\n",
    "                projected centered data.\n",
    "        \"\"\"\n",
    "        # shape = (num_cols, top_k_pc)\n",
    "        self.top_k_eigenvectors = (\n",
    "            self.eigenvectors[:, -self.params[\"top_k_pc\"]:]\n",
    "        )\n",
    "\n",
    "        # shape = (num_examples, num_cols)\n",
    "        centered_data = data - self.col_means_vector\n",
    "\n",
    "        # shape = (num_examples, top_k_pc)\n",
    "        projected_centered_data = tf.matmul(\n",
    "            a=centered_data,\n",
    "            b=self.top_k_eigenvectors\n",
    "        )\n",
    "\n",
    "        return projected_centered_data\n",
    "\n",
    "    def pca_reconstruction_from_top_k_pc(self, data):\n",
    "        \"\"\"Reconstructs data up from top_k principal components.\n",
    "\n",
    "        Args:\n",
    "            data: tensor, rank 2 tensor of shape (num_examples, num_cols)\n",
    "                containing batch of input data.\n",
    "\n",
    "        Returns:\n",
    "            Rank 2 tensor of shape (num_examples, num_cols) containing\n",
    "                lossy, reconstructed input data.\n",
    "        \"\"\"\n",
    "        # shape = (num_examples, top_k_pc)\n",
    "        projected_centered_data = self.pca_projection_to_top_k_pc(data=data)\n",
    "\n",
    "        # shape = (num_examples, num_cols)\n",
    "        unprojected_centered_data = tf.matmul(\n",
    "            a=projected_centered_data,\n",
    "            b=self.top_k_eigenvectors,\n",
    "            transpose_b=True\n",
    "        )\n",
    "\n",
    "        # shape = (num_examples, num_cols)\n",
    "        data_reconstructed = unprojected_centered_data + self.col_means_vector\n",
    "\n",
    "        return data_reconstructed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoints.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/checkpoints.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Checkpoints(object):\n",
    "    \"\"\"Class that contains methods used for training checkpoints.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `Checkpoints`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def create_checkpoint_manager(self):\n",
    "        \"\"\"Creates checkpoint manager for reading and writing checkpoints.\n",
    "        \"\"\"\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.checkpoint,\n",
    "            directory=os.path.join(\n",
    "                self.params[\"output_dir\"], \"checkpoints\"\n",
    "            ),\n",
    "            max_to_keep=self.params[\"keep_checkpoint_max\"],\n",
    "            checkpoint_name=\"ckpt\",\n",
    "            step_counter=self.global_step,\n",
    "            checkpoint_interval=self.params[\"save_checkpoints_steps\"]\n",
    "        )\n",
    "\n",
    "    def create_checkpoint_machinery(self):\n",
    "        \"\"\"Creates checkpoint machinery needed to save & restore checkpoints.\n",
    "        \"\"\"\n",
    "        # Create checkpoint instance.\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            global_step=self.global_step,\n",
    "            seen_example_count=self.pca_model.seen_example_count,\n",
    "            col_means_vector=self.pca_model.col_means_vector,\n",
    "            covariance_matrix=self.pca_model.covariance_matrix,\n",
    "            eigenvalues=self.pca_model.eigenvalues,\n",
    "            eigenvectors=self.pca_model.eigenvectors\n",
    "        )\n",
    "\n",
    "        # Create initial checkpoint manager.\n",
    "        self.create_checkpoint_manager()\n",
    "\n",
    "        # Restore any prior checkpoints.\n",
    "        print(\n",
    "            \"Loading latest checkpoint: {}\".format(\n",
    "                self.checkpoint_manager.latest_checkpoint\n",
    "            )\n",
    "        )\n",
    "        status = self.checkpoint.restore(\n",
    "            save_path=self.checkpoint_manager.latest_checkpoint\n",
    "        )\n",
    "\n",
    "        if self.checkpoint_manager.latest_checkpoint:\n",
    "            status.assert_consumed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FW2mzg32HyaV"
   },
   "source": [
    "## train_step.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoqxLO-uHyaW",
    "outputId": "391c7a75-2a24-453a-8b54-95ee13b69045"
   },
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/train_step.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TrainStep(object):\n",
    "    \"\"\"Class that contains methods concerning train steps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `TrainStep`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_batch(self, features):\n",
    "        \"\"\"Trains model with a batch of feature data.\n",
    "\n",
    "        Args:\n",
    "            features: tensor, rank 2 tensor of feature data.\n",
    "\n",
    "        Returns:\n",
    "            Scalar loss tensor.\n",
    "        \"\"\"\n",
    "        # Pass images through ResNet to get feature vectors.\n",
    "        resnet_feature_vectors = (\n",
    "            self.resnet_instance.get_image_resnet_feature_vectors(\n",
    "                images=features\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Train PCA model.\n",
    "        self.pca_model.calculate_data_stats(data=resnet_feature_vectors)\n",
    "\n",
    "        return tf.zeros(shape=(), dtype=tf.float32)\n",
    "\n",
    "    def distributed_eager_train_step(self, features):\n",
    "        \"\"\"Perform one distributed, eager train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Scalar loss of model.\n",
    "        \"\"\"\n",
    "        if self.params[\"tf_version\"] > 2.1:\n",
    "            run_function = self.strategy.run\n",
    "        else:\n",
    "            run_function = self.strategy.experimental_run_v2\n",
    "\n",
    "        per_replica_losses = run_function(\n",
    "            fn=self.train_batch,\n",
    "            kwargs={\"features\": features}\n",
    "        )\n",
    "\n",
    "        return self.strategy.reduce(\n",
    "            reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "            value=per_replica_losses,\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "    def non_distributed_eager_train_step(self, features):\n",
    "        \"\"\"Perform one non-distributed, eager train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Scalar loss of model.\n",
    "        \"\"\"\n",
    "        return self.train_batch(features=features)\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_graph_train_step(self, features):\n",
    "        \"\"\"Perform one distributed, graph train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Scalar loss of model.\n",
    "        \"\"\"\n",
    "        if self.params[\"tf_version\"] > 2.1:\n",
    "            run_function = self.strategy.run\n",
    "        else:\n",
    "            run_function = self.strategy.experimental_run_v2\n",
    "\n",
    "        per_replica_losses = run_function(\n",
    "            fn=self.train_batch,\n",
    "            kwargs={\"features\": features}\n",
    "        )\n",
    "\n",
    "        return self.strategy.reduce(\n",
    "            reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "            value=per_replica_losses,\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def non_distributed_graph_train_step(self, features):\n",
    "        \"\"\"Perform one non-distributed, graph train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Scalar loss of model.\n",
    "        \"\"\"\n",
    "        return self.train_batch(features=features)\n",
    "\n",
    "    def get_train_step_functions(self):\n",
    "        \"\"\"Gets model train step functions for strategy and mode.\n",
    "        \"\"\"\n",
    "        if self.strategy:\n",
    "            if self.params[\"use_graph_mode\"]:\n",
    "                self.train_step_fn = (\n",
    "                    self.distributed_graph_train_step\n",
    "                )\n",
    "            else:\n",
    "                self.train_step_fn = (\n",
    "                    self.distributed_eager_train_step\n",
    "                )\n",
    "        else:\n",
    "            if self.params[\"use_graph_mode\"]:\n",
    "                self.train_step_fn = (\n",
    "                    self.non_distributed_graph_train_step\n",
    "                )\n",
    "            else:\n",
    "                self.train_step_fn = (\n",
    "                    self.non_distributed_eager_train_step\n",
    "                )\n",
    "\n",
    "    @tf.function\n",
    "    def increment_global_step_var(self):\n",
    "        \"\"\"Increments global step variable.\n",
    "        \"\"\"\n",
    "        self.global_step.assign_add(\n",
    "            delta=tf.ones(shape=(), dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "    def perform_training_step(self, train_dataset_iterator, train_step_fn):\n",
    "        \"\"\"Performs one training step of model.\n",
    "\n",
    "        Args:\n",
    "            train_dataset_iterator: iterator, iterator of instance of\n",
    "                `Dataset` for training data.\n",
    "            train_step_fn: unbound function, trains the given model\n",
    "                with a given set of features.\n",
    "        \"\"\"\n",
    "        # Train model on batch of features and get loss.\n",
    "        features = next(train_dataset_iterator)\n",
    "\n",
    "        # Train for a step and get loss.\n",
    "        self.loss = train_step_fn(features=features)\n",
    "\n",
    "        # Checkpoint model every save_checkpoints_steps steps.\n",
    "        checkpoint_saved = self.checkpoint_manager.save(\n",
    "            checkpoint_number=self.global_step, check_interval=True\n",
    "        )\n",
    "\n",
    "        if checkpoint_saved:\n",
    "            print(\"Checkpoint saved at {}\".format(checkpoint_saved))\n",
    "\n",
    "        # Increment steps.\n",
    "        self.increment_global_step_var()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-hKdwwIHyaZ"
   },
   "source": [
    "## training_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvQAtxuwHyaa",
    "outputId": "01c19a61-f5a1-47b0-a7ac-e2990db79b51"
   },
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/training_loop.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TrainingLoop(object):\n",
    "    \"\"\"Class that contains methods for training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `TrainStep`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def training_loop(self):\n",
    "        \"\"\"Loops through training dataset to train model.\n",
    "        \"\"\"\n",
    "        # Get correct train function based on parameters.\n",
    "        self.get_train_step_functions()\n",
    "\n",
    "        num_steps = (\n",
    "            self.params[\"train_dataset_length\"] // self.global_batch_size\n",
    "        )\n",
    "\n",
    "        while self.global_step.numpy() < num_steps:\n",
    "            # Train model.\n",
    "            self.perform_training_step(\n",
    "                train_dataset_iterator=self.train_dataset_iterator,\n",
    "                train_step_fn=self.train_step_fn\n",
    "            )\n",
    "\n",
    "        self.training_loop_end_save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/export.py\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Export(object):\n",
    "    \"\"\"Class that contains methods used for exporting model objects.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `Export`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def create_serving_model(self):\n",
    "        \"\"\"Creates Keras `Model` for serving.\n",
    "\n",
    "        Returns:\n",
    "            `tf.Keras.Model` for serving predictions.\n",
    "        \"\"\"\n",
    "        # Create input layer for raw images.\n",
    "        input_layer = tf.keras.Input(\n",
    "            shape=(\n",
    "                self.params[\"image_height\"],\n",
    "                self.params[\"image_width\"],\n",
    "                self.params[\"image_depth\"]\n",
    "            ),\n",
    "            name=\"serving_inputs\",\n",
    "            dtype=tf.uint8\n",
    "        )\n",
    "\n",
    "        # Pass images through ResNet to get feature vectors.\n",
    "        resnet_feature_vectors = (\n",
    "            self.resnet_instance.get_image_resnet_feature_vectors(\n",
    "                images=input_layer\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Project ResNet feature vectors using PCA eigenvectors.\n",
    "        pca_projections = tf.identity(\n",
    "            input=self.pca_model.pca_projection_to_top_k_pc(\n",
    "                data=resnet_feature_vectors\n",
    "            ),\n",
    "            name=\"pca_projections\"\n",
    "        )\n",
    "\n",
    "        return tf.keras.Model(\n",
    "            inputs=input_layer,\n",
    "            outputs=pca_projections,\n",
    "            name=\"serving_model\"\n",
    "        )\n",
    "\n",
    "    def export_saved_model(self):\n",
    "        \"\"\"Exports SavedModel to output directory for serving.\n",
    "        \"\"\"\n",
    "        # Build export path.\n",
    "        export_path = os.path.join(\n",
    "            self.params[\"output_dir\"],\n",
    "            \"export\",\n",
    "            datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        )\n",
    "\n",
    "        # Create serving models.\n",
    "        serving_model = self.create_serving_model()\n",
    "\n",
    "        # Signature will be serving_default.\n",
    "        tf.saved_model.save(\n",
    "            obj=serving_model,\n",
    "            export_dir=export_path\n",
    "        )\n",
    "\n",
    "    def training_loop_end_save_model(self):\n",
    "        \"\"\"Saving model when training loop ends.\n",
    "        \"\"\"\n",
    "        # Write final checkpoint.\n",
    "        checkpoint_saved = self.checkpoint_manager.save(\n",
    "            checkpoint_number=self.global_step, check_interval=False\n",
    "        )\n",
    "\n",
    "        if checkpoint_saved:\n",
    "            print(\"Checkpoint saved at {}\".format(checkpoint_saved))\n",
    "\n",
    "        # Export SavedModel for serving.\n",
    "        self.export_saved_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/model.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import checkpoints\n",
    "from . import export\n",
    "from . import pca\n",
    "from . import resnet\n",
    "from . import train_step\n",
    "from . import training_inputs\n",
    "from . import training_loop\n",
    "\n",
    "\n",
    "class TrainModel(\n",
    "    checkpoints.Checkpoints,\n",
    "    train_step.TrainStep,\n",
    "    training_loop.TrainingLoop,\n",
    "    export.Export\n",
    "):\n",
    "    \"\"\"Class that trains a model.\n",
    "\n",
    "    Attributes:\n",
    "        params: dict, user passed parameters.\n",
    "        resnet_instance: instance or `ResNet` class.\n",
    "        pca_model: instance of `PCA` class.\n",
    "        strategy: instance of tf.distribute.strategy.\n",
    "        global_batch_size: int, global batch size after summing batch sizes\n",
    "            across replicas.\n",
    "        train_dataset_iterator: iterator, iterator of instance of `Dataset`\n",
    "            for training data.\n",
    "        train_step_fn: unbound function, function for a train step using\n",
    "            correct strategy and mode.\n",
    "        global_step: tf.Variable, the global step counter.\n",
    "        checkpoint: instance of tf.train.Checkpoint, for saving and restoring\n",
    "            checkpoints.\n",
    "        checkpoint_manager: instance of tf.train.CheckpointManager, for\n",
    "            managing checkpoint path, how often to write, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        \"\"\"Instantiate trainer.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "\n",
    "        self.resnet_instance = resnet.ResNet(\n",
    "            params={\n",
    "                \"image_height\": self.params[\"image_height\"],\n",
    "                \"image_width\": self.params[\"image_width\"],\n",
    "                \"image_depth\": self.params[\"image_depth\"],\n",
    "                \"resnet_weights\": self.params[\"resnet_weights\"],\n",
    "                \"resnet_layer_name\": self.params[\"resnet_layer_name\"],\n",
    "                \"preprocess_input\": self.params[\"preprocess_input\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.pca_model = pca.PCA(\n",
    "            params={\n",
    "                \"num_cols\": self.params[\"num_cols\"],\n",
    "                \"use_sample_covariance\": self.params[\"use_sample_covariance\"],\n",
    "                \"top_k_pc\": self.params[\"top_k_pc\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.strategy = None\n",
    "        self.global_batch_size = []\n",
    "\n",
    "        self.train_dataset_iterator = None\n",
    "\n",
    "        self.train_step_fn = None\n",
    "\n",
    "        self.global_step = tf.Variable(\n",
    "            initial_value=tf.zeros(shape=[], dtype=tf.int64),\n",
    "            trainable=False,\n",
    "            name=\"global_step\"\n",
    "        )\n",
    "\n",
    "        self.checkpoint = None\n",
    "        self.checkpoint_manager = None\n",
    "\n",
    "    def get_train_dataset(self, num_replicas):\n",
    "        \"\"\"Gets train dataset.\n",
    "\n",
    "        Args:\n",
    "            num_replicas: int, number of device replicas.\n",
    "\n",
    "        Returns:\n",
    "            `tf.data.Dataset` for training data.\n",
    "        \"\"\"\n",
    "        return training_inputs.read_dataset(\n",
    "            file_pattern=self.params[\"train_file_pattern\"],\n",
    "            batch_size=self.params[\"train_batch_size\"] * num_replicas,\n",
    "            params=self.params\n",
    "        )()\n",
    "\n",
    "    def train_block(self, train_dataset):\n",
    "        \"\"\"Training block setups training, then loops through datasets.\n",
    "\n",
    "        Args:\n",
    "            train_dataset: instance of `Dataset` for training data.\n",
    "        \"\"\"\n",
    "        # Create iterators of datasets.\n",
    "        self.train_dataset_iterator = iter(train_dataset)\n",
    "\n",
    "        # Create checkpoint machinery to save/restore checkpoints.\n",
    "        self.create_checkpoint_machinery()\n",
    "\n",
    "        # Run training loop.\n",
    "        self.training_loop()\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Trains Keras model.\n",
    "\n",
    "        Args:\n",
    "            args: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        if self.params[\"distribution_strategy\"]:\n",
    "            # If the list of devices is not specified in the\n",
    "            # Strategy constructor, it will be auto-detected.\n",
    "            if self.params[\"distribution_strategy\"] == \"Mirrored\":\n",
    "                self.strategy = tf.distribute.MirroredStrategy()\n",
    "            print(\n",
    "                \"Number of devices = {}\".format(\n",
    "                    self.strategy.num_replicas_in_sync\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Set global batch size for training.\n",
    "            self.global_batch_size = (\n",
    "                self.params[\"train_batch_size\"] * self.strategy.num_replicas_in_sync\n",
    "            )\n",
    "\n",
    "            # Get input dataset. Batch size is split evenly between replicas.\n",
    "            train_dataset = self.get_train_dataset(\n",
    "                num_replicas=self.strategy.num_replicas_in_sync\n",
    "            )\n",
    "\n",
    "            with self.strategy.scope():\n",
    "                # Create distributed datasets.\n",
    "                train_dataset = (\n",
    "                    self.strategy.experimental_distribute_dataset(\n",
    "                        dataset=train_dataset\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Training block setups training, then loops through datasets.\n",
    "                self.train_block(train_dataset=train_dataset)\n",
    "        else:\n",
    "            # Set global batch size for training.\n",
    "            self.global_batch_size = self.params[\"train_batch_size\"]\n",
    "\n",
    "            # Get input datasets.\n",
    "            train_dataset = self.get_train_dataset(num_replicas=1)\n",
    "\n",
    "            # Training block setups training, then loops through datasets.\n",
    "            self.train_block(train_dataset=train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli_parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/cli_parser.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "def parse_file_arguments(parser):\n",
    "    \"\"\"Parses command line file arguments.\n",
    "\n",
    "    Args:\n",
    "        parser: instance of `argparse.ArgumentParser`.\n",
    "    \"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--train_file_pattern\",\n",
    "        help=\"GCS location to read training data.\",\n",
    "        type=str,\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models.\",\n",
    "        type=str,\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud.\",\n",
    "        type=str,\n",
    "        default=\"junk\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_data_arguments(parser):\n",
    "    \"\"\"Parses command line data arguments.\n",
    "\n",
    "    Args:\n",
    "        parser: instance of `argparse.ArgumentParser`.\n",
    "    \"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--tf_record_example_schema\",\n",
    "        help=\"Serialized TF Record Example schema.\",\n",
    "        type=str,\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_feature_name\",\n",
    "        help=\"Name of image feature.\",\n",
    "        type=str,\n",
    "        default=\"image\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoding\",\n",
    "        help=\"Encoding of image: raw, png, or jpeg.\",\n",
    "        type=str,\n",
    "        default=\"raw\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_height\",\n",
    "        help=\"Height of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_width\",\n",
    "        help=\"Width of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_depth\",\n",
    "        help=\"Depth of image.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--label_feature_name\",\n",
    "        help=\"Name of label feature.\",\n",
    "        type=str,\n",
    "        default=\"label\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_training_arguments(parser):\n",
    "    \"\"\"Parses command line training arguments.\n",
    "\n",
    "    Args:\n",
    "        parser: instance of `argparse.ArgumentParser`.\n",
    "    \"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--tf_version\",\n",
    "        help=\"Version of TensorFlow\",\n",
    "        type=float,\n",
    "        default=2.3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_graph_mode\",\n",
    "        help=\"Whether to use graph mode or not (eager).\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--distribution_strategy\",\n",
    "        help=\"Which distribution strategy to use, if any.\",\n",
    "        type=str,\n",
    "        default=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_dataset_length\",\n",
    "        help=\"Number of examples in one epoch of training set.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Number of examples in training batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_fn_autotune\",\n",
    "        help=\"Whether to autotune input function performance.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_checkpoints_steps\",\n",
    "        help=\"How many steps to train before saving a checkpoint.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_checkpoint_max\",\n",
    "        help=\"Max number of checkpoints to keep.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_resnet_arguments(parser):\n",
    "    \"\"\"Parses command line ResNet arguments.\n",
    "\n",
    "    Args:\n",
    "        parser: instance of `argparse.ArgumentParser`.\n",
    "    \"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--resnet_weights\",\n",
    "        help=\"The type of weights to use in Resnet, i.e. imagenet.\",\n",
    "        type=str,\n",
    "        default=\"imagenet\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resnet_layer_name\",\n",
    "        help=\"Number of top principal components to keep.\",\n",
    "        type=str,\n",
    "        default=\"conv4_block1_0_conv\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocess_input\",\n",
    "        help=\"Whether to preprocess input for ResNet.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_pca_arguments(parser):\n",
    "    \"\"\"Parses command line PCA arguments.\n",
    "\n",
    "    Args:\n",
    "        parser: instance of `argparse.ArgumentParser`.\n",
    "    \"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--num_cols\",\n",
    "        help=\"Number of dimensions for each data instance.\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sample_covariance\",\n",
    "        help=\"Whether using sample or population covariance.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_k_pc\",\n",
    "        help=\"Number of top principal components to keep.\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_command_line_arguments():\n",
    "    \"\"\"Parses command line arguments and returns dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Add various arguments to parser.\n",
    "    parse_file_arguments(parser)\n",
    "    parse_data_arguments(parser)\n",
    "    parse_training_arguments(parser)\n",
    "    parse_resnet_arguments(parser)\n",
    "    parse_pca_arguments(parser)\n",
    "\n",
    "    # Parse all arguments.\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    return arguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli_argument_reformat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/cli_argument_reformat.py\n",
    "import json\n",
    "\n",
    "\n",
    "def convert_string_to_bool(string):\n",
    "    \"\"\"Converts string to bool.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        Boolean conversion of string.\n",
    "    \"\"\"\n",
    "    return False if string.lower() == \"false\" else True\n",
    "\n",
    "\n",
    "def fix_arguments(arguments):\n",
    "    \"\"\"Fixes command line arguments dictionary in place.\n",
    "    \"\"\"\n",
    "    # Fix tf_record_example_schema.\n",
    "    arguments[\"tf_record_example_schema\"] = json.loads(\n",
    "        arguments[\"tf_record_example_schema\"].replace(\";\", \" \")\n",
    "    )\n",
    "\n",
    "    # Fix use_graph_mode.\n",
    "    arguments[\"use_graph_mode\"] = convert_string_to_bool(\n",
    "        string=arguments[\"use_graph_mode\"]\n",
    "    )\n",
    "\n",
    "    # Fix input_fn_autotune.\n",
    "    arguments[\"input_fn_autotune\"] = convert_string_to_bool(\n",
    "        string=arguments[\"input_fn_autotune\"]\n",
    "    )\n",
    "\n",
    "    # Fix preprocess_input.\n",
    "    arguments[\"preprocess_input\"] = convert_string_to_bool(\n",
    "        string=arguments[\"preprocess_input\"]\n",
    "    )\n",
    "\n",
    "    # Fix use_sample_covariance.\n",
    "    arguments[\"use_sample_covariance\"] = convert_string_to_bool(\n",
    "        string=arguments[\"use_sample_covariance\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPXbCgwXHyar"
   },
   "source": [
    "## task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzPxTlmuHyar",
    "outputId": "edb82b08-28de-4913-f306-fefacfa699b3"
   },
   "outputs": [],
   "source": [
    "%%writefile pca_out_of_core_distributed_module/trainer/task.py\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import cli_argument_reformat\n",
    "from trainer import cli_parser\n",
    "from trainer import model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse command line arguments.\n",
    "    arguments = cli_parser.parse_command_line_arguments()\n",
    "\n",
    "    # Unused args provided by service.\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Fix formatting of command line arguments.\n",
    "    cli_argument_reformat.fix_arguments(arguments)\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning.\n",
    "    # This code can be removed if you are not using hyperparameter tuning.\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\n",
    "                \"TF_CONFIG\", \"{}\"\n",
    "            )\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\"))\n",
    "\n",
    "    print(arguments)\n",
    "\n",
    "    # Instantiate instance of model trainer.\n",
    "    trainer = model.TrainModel(params=arguments)\n",
    "\n",
    "    # Run the training job.\n",
    "    trainer.train_model()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
