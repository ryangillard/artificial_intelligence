{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model module locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments.\n",
    "arguments[\"train_file_pattern\"] = \"gs://machine-learning-1234-bucket/gan/data/cifar10_car/train*.tfrecord\"\n",
    "arguments[\"eval_file_pattern\"] = \"gs://machine-learning-1234-bucket/gan/data/cifar10_car/test*.tfrecord\"\n",
    "arguments[\"output_dir\"] = \"gs://machine-learning-1234-bucket/gan/pgan/tf2/trained_model2\"\n",
    "\n",
    "# Training parameters.\n",
    "arguments[\"tf_version\"] = 2.3\n",
    "arguments[\"use_graph_mode\"] = True\n",
    "arguments[\"distribution_strategy\"] = \"\"\n",
    "arguments[\"write_summaries\"] = False\n",
    "arguments[\"num_epochs\"] = 27\n",
    "arguments[\"train_dataset_length\"] = 10000\n",
    "arguments[\"train_batch_size_schedule\"] = [32] * 5 + [16] + [4] + [2] + [1]\n",
    "arguments[\"input_fn_autotune\"] = False\n",
    "arguments[\"log_step_count_steps\"] = 100\n",
    "arguments[\"save_summary_steps\"] = 100\n",
    "arguments[\"save_checkpoints_steps\"] = 10000\n",
    "arguments[\"keep_checkpoint_max\"] = 10\n",
    "arguments[\"export_every_growth_phase\"] = True\n",
    "\n",
    "# Eval parameters.\n",
    "arguments[\"eval_batch_size_schedule\"] = [32] * 5 + [16] + [4] + [2] + [1]\n",
    "arguments[\"eval_steps\"] = 100\n",
    "\n",
    "# Image parameters.\n",
    "arguments[\"height\"] = 32\n",
    "arguments[\"width\"] = 32\n",
    "arguments[\"depth\"] = 3\n",
    "\n",
    "# Shared network parameters.\n",
    "arguments[\"use_equalized_learning_rate\"] = True\n",
    "\n",
    "# Generator parameters.\n",
    "arguments[\"generator_latent_size\"] = 512\n",
    "arguments[\"generator_normalize_latents\"] = True\n",
    "arguments[\"generator_use_pixel_norm\"] = True\n",
    "arguments[\"generator_pixel_norm_epsilon\"] = 1e-8\n",
    "arguments[\"generator_projection_dims\"] = [4, 4, 512]\n",
    "arguments[\"generator_leaky_relu_alpha\"] = 0.2\n",
    "arguments[\"generator_final_activation\"] = \"tanh\"\n",
    "arguments[\"generator_l1_regularization_scale\"] = 0.\n",
    "arguments[\"generator_l2_regularization_scale\"] = 0.\n",
    "arguments[\"generator_optimizer\"] = \"Adam\"\n",
    "arguments[\"generator_learning_rate\"] = 0.001\n",
    "arguments[\"generator_adam_beta1\"] = 0.0\n",
    "arguments[\"generator_adam_beta2\"] = 0.99\n",
    "arguments[\"generator_adam_epsilon\"] = 1e-8\n",
    "arguments[\"generator_clip_gradients\"] = None\n",
    "arguments[\"generator_train_steps\"] = 1\n",
    "\n",
    "# Discriminator hyperparameters.\n",
    "arguments[\"discriminator_use_minibatch_stddev\"] = True\n",
    "arguments[\"discriminator_minibatch_stddev_group_size\"] = 4\n",
    "arguments[\"discriminator_minibatch_stddev_averaging\"] = True\n",
    "arguments[\"discriminator_leaky_relu_alpha\"] = 0.2\n",
    "arguments[\"discriminator_l1_regularization_scale\"] = 0.\n",
    "arguments[\"discriminator_l2_regularization_scale\"] = 0.\n",
    "arguments[\"discriminator_optimizer\"] = \"Adam\"\n",
    "arguments[\"discriminator_learning_rate\"] = 0.001\n",
    "arguments[\"discriminator_adam_beta1\"] = 0.0\n",
    "arguments[\"discriminator_adam_beta2\"] = 0.99\n",
    "arguments[\"discriminator_adam_epsilon\"] = 1e-8\n",
    "arguments[\"discriminator_clip_gradients\"] = None\n",
    "arguments[\"discriminator_gradient_penalty_coefficient\"] = 10.0\n",
    "arguments[\"discriminator_epsilon_drift\"] = 0.001\n",
    "arguments[\"discriminator_train_steps\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_num_filters = 512,512;512,512;512,512;512,512\n",
      "conv_kernel_sizes = 4,3;3,3;3,3;3,3\n",
      "conv_strides = 1,1;1,1;1,1;1,1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "def convert_conv_layer_property_lists_to_string(property_list, prop_list_len):\n",
    "    \"\"\"Convert conv layer property list to string.\n",
    "\n",
    "    Args:\n",
    "        property_list: list, nested list of blocks of a conv layer property.\n",
    "        prop_list_len: int, length of list to process.\n",
    "\n",
    "    Returns:\n",
    "        Doubly delimited string of conv layer property values.\n",
    "    \"\"\"\n",
    "    return (\";\").join(\n",
    "        [\n",
    "            (\",\").join([str(val) for val in block])\n",
    "            for block in property_list[0:prop_list_len]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Import os environment variables for file hyperparameters.\n",
    "os.environ[\"TRAIN_FILE_PATTERN\"] = arguments[\"train_file_pattern\"]\n",
    "os.environ[\"EVAL_FILE_PATTERN\"] = arguments[\"eval_file_pattern\"]\n",
    "os.environ[\"OUTPUT_DIR\"] = arguments[\"output_dir\"]\n",
    "\n",
    "# Import os environment variables for train hyperparameters.\n",
    "os.environ[\"TF_VERSION\"] = str(arguments[\"tf_version\"])\n",
    "os.environ[\"USE_GRAPH_MODE\"] = str(arguments[\"use_graph_mode\"])\n",
    "os.environ[\"DISTRIBUTION_STRATEGY\"] = arguments[\"distribution_strategy\"]\n",
    "os.environ[\"WRITE_SUMMARIES\"] = str(arguments[\"write_summaries\"])\n",
    "os.environ[\"NUM_EPOCHS\"] = str(arguments[\"num_epochs\"])\n",
    "os.environ[\"TRAIN_DATASET_LENGTH\"] = str(arguments[\"train_dataset_length\"])\n",
    "os.environ[\"TRAIN_BATCH_SIZE_SCHEDULE\"] = \",\".join(\n",
    "    [\n",
    "        str(x) for x in arguments[\"train_batch_size_schedule\"]\n",
    "    ]\n",
    ")\n",
    "os.environ[\"INPUT_FN_AUTOTUNE\"] = str(arguments[\"input_fn_autotune\"])\n",
    "os.environ[\"LOG_STEP_COUNT_STEPS\"] = str(arguments[\"log_step_count_steps\"])\n",
    "os.environ[\"SAVE_SUMMARY_STEPS\"] = str(arguments[\"save_summary_steps\"])\n",
    "os.environ[\"SAVE_CHECKPOINTS_STEPS\"] = (\n",
    "    str(arguments[\"save_checkpoints_steps\"])\n",
    ")\n",
    "os.environ[\"KEEP_CHECKPOINT_MAX\"] = str(arguments[\"keep_checkpoint_max\"])\n",
    "os.environ[\"EXPORT_EVERY_GROWTH_PHASE\"] = (\n",
    "    str(arguments[\"export_every_growth_phase\"])\n",
    ")\n",
    "\n",
    "# Import os environment variables for eval hyperparameters.\n",
    "os.environ[\"EVAL_BATCH_SIZE_SCHEDULE\"] = \",\".join(\n",
    "    [\n",
    "        str(x) for x in arguments[\"eval_batch_size_schedule\"]\n",
    "    ]\n",
    ")\n",
    "os.environ[\"EVAL_STEPS\"] = str(arguments[\"eval_steps\"])\n",
    "\n",
    "# Import os environment variables for image hyperparameters.\n",
    "os.environ[\"HEIGHT\"] = str(arguments[\"height\"])\n",
    "os.environ[\"WIDTH\"] = str(arguments[\"width\"])\n",
    "os.environ[\"DEPTH\"] = str(arguments[\"depth\"])\n",
    "\n",
    "# Import os environment variables for shared network hyperparameters.\n",
    "os.environ[\"USE_EQUALIZED_LEARNING_RATE\"] = (\n",
    "    str(arguments[\"use_equalized_learning_rate\"])\n",
    ")\n",
    "\n",
    "# Full lists for full 1024x1024 network growth.\n",
    "full_conv_num_filters = [[512, 512], [512, 512], [512, 512], [512, 512], [256, 256], [128, 128], [64, 64], [32, 32], [16, 16]]\n",
    "full_conv_kernel_sizes = [[4, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]\n",
    "full_conv_strides = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n",
    "\n",
    "# Set final image size as a multiple of 2, starting at 4.\n",
    "image_size = 32\n",
    "os.environ[\"IMAGE_SIZE\"] = str(image_size)\n",
    "prop_list_len = max(\n",
    "    min(int(math.log(image_size, 2) - 1), len(full_conv_num_filters)), 1\n",
    ")\n",
    "\n",
    "# Get slices of lists.\n",
    "conv_num_filters = convert_conv_layer_property_lists_to_string(\n",
    "    full_conv_num_filters, prop_list_len\n",
    ")\n",
    "print(\"conv_num_filters = {}\".format(conv_num_filters))\n",
    "conv_kernel_sizes = convert_conv_layer_property_lists_to_string(\n",
    "    full_conv_kernel_sizes, prop_list_len\n",
    ")\n",
    "print(\"conv_kernel_sizes = {}\".format(conv_kernel_sizes))\n",
    "conv_strides = convert_conv_layer_property_lists_to_string(\n",
    "    full_conv_strides, prop_list_len\n",
    ")\n",
    "print(\"conv_strides = {}\".format(conv_strides))\n",
    "\n",
    "os.environ[\"CONV_NUM_FILTERS\"] = conv_num_filters\n",
    "os.environ[\"CONV_KERNEL_SIZES\"] = conv_kernel_sizes\n",
    "os.environ[\"CONV_STRIDES\"] = conv_strides\n",
    "\n",
    "# Import os environment variables for generator hyperparameters.\n",
    "os.environ[\"GENERATOR_LATENT_SIZE\"] = str(arguments[\"generator_latent_size\"])\n",
    "os.environ[\"GENERATOR_NORMALIZE_LATENTS\"] = (\n",
    "    str(arguments[\"generator_normalize_latents\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_USE_PIXEL_NORM\"] = (\n",
    "    str(arguments[\"generator_use_pixel_norm\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_PIXEL_NORM_EPSILON\"] = (\n",
    "    str(arguments[\"generator_pixel_norm_epsilon\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_PROJECTION_DIMS\"] = \",\".join(\n",
    "    [str(x) for x in arguments[\"generator_projection_dims\"]]\n",
    ")\n",
    "os.environ[\"GENERATOR_LEAKY_RELU_ALPHA\"] = (\n",
    "    str(arguments[\"generator_leaky_relu_alpha\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_FINAL_ACTIVATION\"] = (\n",
    "    arguments[\"generator_final_activation\"]\n",
    ")\n",
    "os.environ[\"GENERATOR_L1_REGULARIZATION_SCALE\"] = (\n",
    "    str(arguments[\"generator_l1_regularization_scale\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_L2_REGULARIZATION_SCALE\"] = (\n",
    "    str(arguments[\"generator_l2_regularization_scale\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_OPTIMIZER\"] = arguments[\"generator_optimizer\"]\n",
    "os.environ[\"GENERATOR_LEARNING_RATE\"] = (\n",
    "    str(arguments[\"generator_learning_rate\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_ADAM_BETA1\"] = str(arguments[\"generator_adam_beta1\"])\n",
    "os.environ[\"GENERATOR_ADAM_BETA2\"] = str(arguments[\"generator_adam_beta2\"])\n",
    "os.environ[\"GENERATOR_ADAM_EPSILON\"] = (\n",
    "    str(arguments[\"generator_adam_epsilon\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_CLIP_GRADIENTS\"] = (\n",
    "    str(arguments[\"generator_clip_gradients\"])\n",
    ")\n",
    "os.environ[\"GENERATOR_TRAIN_STEPS\"] = str(arguments[\"generator_train_steps\"])\n",
    "\n",
    "# Import os environment variables for discriminator hyperparameters.\n",
    "os.environ[\"DISCRIMINATOR_USE_MINIBATCH_STDDEV\"] = (\n",
    "    str(arguments[\"discriminator_use_minibatch_stddev\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_MINIBATCH_STDDEV_GROUP_SIZE\"] = (\n",
    "    str(arguments[\"discriminator_minibatch_stddev_group_size\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_MINIBATCH_STDDEV_AVERAGING\"] = (\n",
    "    str(arguments[\"discriminator_minibatch_stddev_averaging\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_LEAKY_RELU_ALPHA\"] = (\n",
    "    str(arguments[\"discriminator_leaky_relu_alpha\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_L1_REGULARIZATION_SCALE\"] = (\n",
    "    str(arguments[\"discriminator_l1_regularization_scale\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_L2_REGULARIZATION_SCALE\"] = (\n",
    "    str(arguments[\"discriminator_l2_regularization_scale\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_OPTIMIZER\"] = arguments[\"discriminator_optimizer\"]\n",
    "os.environ[\"DISCRIMINATOR_LEARNING_RATE\"] = (\n",
    "    str(arguments[\"discriminator_learning_rate\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_ADAM_BETA1\"] = (\n",
    "    str(arguments[\"discriminator_adam_beta1\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_ADAM_BETA2\"] = (\n",
    "    str(arguments[\"discriminator_adam_beta2\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_ADAM_EPSILON\"] = (\n",
    "    str(arguments[\"discriminator_adam_epsilon\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_GRADIENT_PENALTY_COEFFICIENT\"] = (\n",
    "    str(arguments[\"discriminator_gradient_penalty_coefficient\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_EPSILON_DRIFT\"] = (\n",
    "    str(arguments[\"discriminator_epsilon_drift\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_CLIP_GRADIENTS\"] = (\n",
    "    str(arguments[\"discriminator_clip_gradients\"])\n",
    ")\n",
    "os.environ[\"DISCRIMINATOR_TRAIN_STEPS\"] = (\n",
    "    str(arguments[\"discriminator_train_steps\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m rm -rf ${OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Progressive GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m rm -rf ${OUTPUT_DIR}\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/pgan_class_ctl_module\n",
    "python3 -m trainer.task \\\n",
    "    --train_file_pattern=${TRAIN_FILE_PATTERN} \\\n",
    "    --eval_file_pattern=${EVAL_FILE_PATTERN} \\\n",
    "    --output_dir=${OUTPUT_DIR} \\\n",
    "    --job-dir=./tmp \\\n",
    "    \\\n",
    "    --tf_version=${TF_VERSION} \\\n",
    "    --use_graph_mode=${USE_GRAPH_MODE} \\\n",
    "    --distribution_strategy=${DISTRIBUTION_STRATEGY} \\\n",
    "    --write_summaries=${WRITE_SUMMARIES} \\\n",
    "    --num_epochs=${NUM_EPOCHS} \\\n",
    "    --train_dataset_length=${TRAIN_DATASET_LENGTH} \\\n",
    "    --train_batch_size_schedule=${TRAIN_BATCH_SIZE_SCHEDULE} \\\n",
    "    --input_fn_autotune=${INPUT_FN_AUTOTUNE} \\\n",
    "    --log_step_count_steps=${LOG_STEP_COUNT_STEPS} \\\n",
    "    --save_summary_steps=${SAVE_SUMMARY_STEPS} \\\n",
    "    --save_checkpoints_steps=${SAVE_CHECKPOINTS_STEPS} \\\n",
    "    --keep_checkpoint_max=${KEEP_CHECKPOINT_MAX} \\\n",
    "    --export_every_growth_phase=${EXPORT_EVERY_GROWTH_PHASE} \\\n",
    "    \\\n",
    "    --eval_batch_size_schedule=${EVAL_BATCH_SIZE_SCHEDULE} \\\n",
    "    --eval_steps=${EVAL_STEPS} \\\n",
    "    \\\n",
    "    --height=${HEIGHT} \\\n",
    "    --width=${WIDTH} \\\n",
    "    --depth=${DEPTH} \\\n",
    "    \\\n",
    "    --use_equalized_learning_rate=${USE_EQUALIZED_LEARNING_RATE} \\\n",
    "    --conv_num_filters=${CONV_NUM_FILTERS} \\\n",
    "    --conv_kernel_sizes=${CONV_KERNEL_SIZES} \\\n",
    "    --conv_strides=${CONV_STRIDES} \\\n",
    "    \\\n",
    "    --generator_latent_size=${GENERATOR_LATENT_SIZE} \\\n",
    "    --generator_use_pixel_norm=${GENERATOR_USE_PIXEL_NORM} \\\n",
    "    --generator_pixel_norm_epsilon=${GENERATOR_PIXEL_NORM_EPSILON} \\\n",
    "    --generator_projection_dims=${GENERATOR_PROJECTION_DIMS} \\\n",
    "    --generator_leaky_relu_alpha=${GENERATOR_LEAKY_RELU_ALPHA} \\\n",
    "    --generator_final_activation=${GENERATOR_FINAL_ACTIVATION} \\\n",
    "    --generator_l1_regularization_scale=${GENERATOR_L1_REGULARIZATION_SCALE} \\\n",
    "    --generator_l2_regularization_scale=${GENERATOR_L2_REGULARIZATION_SCALE} \\\n",
    "    --generator_optimizer=${GENERATOR_OPTIMIZER} \\\n",
    "    --generator_learning_rate=${GENERATOR_LEARNING_RATE} \\\n",
    "    --generator_adam_beta1=${GENERATOR_ADAM_BETA1} \\\n",
    "    --generator_adam_beta2=${GENERATOR_ADAM_BETA2} \\\n",
    "    --generator_adam_epsilon=${GENERATOR_ADAM_EPSILON} \\\n",
    "    --generator_clip_gradients=${GENERATOR_CLIP_GRADIENTS} \\\n",
    "    --generator_train_steps=${GENERATOR_TRAIN_STEPS} \\\n",
    "    \\\n",
    "    --discriminator_use_minibatch_stddev=${DISCRIMINATOR_USE_MINIBATCH_STDDEV} \\\n",
    "    --discriminator_minibatch_stddev_group_size=${DISCRIMINATOR_MINIBATCH_STDDEV_GROUP_SIZE} \\\n",
    "    --discriminator_minibatch_stddev_averaging=${DISCRIMINATOR_MINIBATCH_STDDEV_AVERAGING} \\\n",
    "    --discriminator_leaky_relu_alpha=${DISCRIMINATOR_LEAKY_RELU_ALPHA} \\\n",
    "    --discriminator_l1_regularization_scale=${DISCRIMINATOR_L1_REGULARIZATION_SCALE} \\\n",
    "    --discriminator_l2_regularization_scale=${DISCRIMINATOR_L2_REGULARIZATION_SCALE} \\\n",
    "    --discriminator_optimizer=${DISCRIMINATOR_OPTIMIZER} \\\n",
    "    --discriminator_learning_rate=${DISCRIMINATOR_LEARNING_RATE} \\\n",
    "    --discriminator_adam_beta1=${DISCRIMINATOR_ADAM_BETA1} \\\n",
    "    --discriminator_adam_beta2=${DISCRIMINATOR_ADAM_BETA2} \\\n",
    "    --discriminator_adam_epsilon=${DISCRIMINATOR_ADAM_EPSILON} \\\n",
    "    --discriminator_clip_gradients=${DISCRIMINATOR_CLIP_GRADIENTS} \\\n",
    "    --discriminator_train_steps=${DISCRIMINATOR_TRAIN_STEPS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls ${OUTPUT_DIR}/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(\n",
    "    export_dir=os.path.join(\n",
    "        arguments[\"output_dir\"], \"export\", \"20200807100237\"\n",
    "    )\n",
    ")\n",
    "print(list(loaded.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = loaded.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = tf.random.normal(shape=(10, 512))\n",
    "predictions = infer(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = np.clip(\n",
    "    a=tf.cast(\n",
    "        x=((predictions[list(predictions.keys())[0]] + 1.0) * (255. / 2)),\n",
    "        dtype=tf.int32\n",
    "    ),\n",
    "    a_min=0,\n",
    "    a_max=255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, args):\n",
    "    \"\"\"Plots images.\n",
    "\n",
    "    Args:\n",
    "        images: np.array, array of images of\n",
    "            [num_images, image_size, image_size, num_channels].\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        if args[\"depth\"] == 1:\n",
    "            plt.imshow(\n",
    "                tf.reshape(image, image.shape[:-1]), cmap=\"gray_r\"\n",
    "            )\n",
    "        elif args[\"depth\"] == 3:\n",
    "            plt.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images=generated_images, args=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
