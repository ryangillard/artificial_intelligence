{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile README.md\n",
    "Implementation of [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/inputs.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess image tensor.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, input image with shape\n",
    "            [batch_size, height, width, depth].\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image tensor with shape\n",
    "            [batch_size, height, width, depth].\n",
    "    \"\"\"\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.io.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "\n",
    "    # Preprocess image.\n",
    "    image = preprocess_image(image=image)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def set_static_shape(features, labels, batch_size):\n",
    "    \"\"\"Sets static shape of batched input tensors in dataset.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        batch_size: int, number of examples per batch.\n",
    "\n",
    "    Returns:\n",
    "        Features tensor dictionary and labels tensor.\n",
    "    \"\"\"\n",
    "    features[\"image\"].set_shape(\n",
    "        features[\"image\"].get_shape().merge_with(\n",
    "            tf.TensorShape([batch_size, None, None, None])\n",
    "        )\n",
    "    )\n",
    "    labels.set_shape(\n",
    "        labels.get_shape().merge_with(tf.TensorShape([batch_size]))\n",
    "    )\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def read_dataset(filename, batch_size, params, training):\n",
    "    \"\"\"Reads TF Record data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read TF Record\n",
    "    dataset using Dataset API, apply necessary preprocessing, and return an\n",
    "    input function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "        training: bool, if training or not.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def fetch_dataset(filename):\n",
    "        \"\"\"Fetches TFRecord Dataset from given filename.\n",
    "        Args:\n",
    "            filename: str, name of TFRecord file.\n",
    "        Returns:\n",
    "            Dataset containing TFRecord Examples.\n",
    "        \"\"\"\n",
    "        buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=filename, buffer_size=buffer_size\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create dataset to contain list of files matching pattern.\n",
    "        dataset = tf.data.Dataset.list_files(\n",
    "            file_pattern=filename, shuffle=training\n",
    "        )\n",
    "\n",
    "        # Repeat dataset files indefinitely if in training.\n",
    "        if training:\n",
    "            dataset = dataset.repeat()\n",
    "\n",
    "        # Parallel interleaves multiple files at once with map function.\n",
    "        dataset = dataset.apply(\n",
    "            tf.data.experimental.parallel_interleave(\n",
    "                map_func=fetch_dataset, cycle_length=64, sloppy=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Shuffle the Dataset TFRecord Examples if in training.\n",
    "        if training:\n",
    "            dataset = dataset.shuffle(buffer_size=1024)\n",
    "\n",
    "        # Decode TF Record Example into a features dictionary of tensors.\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x: decode_example(\n",
    "                protos=x,\n",
    "                params=params\n",
    "            ),\n",
    "            num_parallel_calls=(\n",
    "                tf.contrib.data.AUTOTUNE\n",
    "                if params[\"input_fn_autotune\"]\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        batch_size = (\n",
    "            params[\"train_batch_size\"]\n",
    "            if training\n",
    "            else params[\"eval_batch_size\"]\n",
    "        )\n",
    "\n",
    "        # Batch dataset and drop remainder so there are no partial batches.\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "        # Assign static shape, namely make the batch size axis static.\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x, y: set_static_shape(\n",
    "                features=x, labels=y, batch_size=batch_size\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(\n",
    "            buffer_size=(\n",
    "                tf.data.experimental.AUTOTUNE\n",
    "                if params[\"input_fn_autotune\"]\n",
    "                else 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom_layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/custom_layers.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class WeightScaledDense(tf.keras.layers.Dense):\n",
    "    \"\"\"Subclassing `Dense` layer to allow equalized learning rate scaling.\n",
    "\n",
    "    Attributes:\n",
    "        use_equalized_learning_rate: bool, if want to scale layer weights to\n",
    "            equalize learning rate each forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            units,\n",
    "            activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=None,\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            trainable=True,\n",
    "            use_equalized_learning_rate=False,\n",
    "            name=None,\n",
    "            **kwargs):\n",
    "        \"\"\"Initializes `Dense` layer.\n",
    "        Args:\n",
    "            units: Integer or Long, dimensionality of the output space.\n",
    "            activation: Activation function (callable). Set it to None to maintain a\n",
    "              linear activation.\n",
    "            use_bias: Boolean, whether the layer uses a bias.\n",
    "            kernel_initializer: Initializer function for the weight matrix.\n",
    "              If `None` (default), weights are initialized using the default\n",
    "              initializer used by `tf.compat.v1.get_variable`.\n",
    "            bias_initializer: Initializer function for the bias.\n",
    "            kernel_regularizer: Regularizer function for the weight matrix.\n",
    "            bias_regularizer: Regularizer function for the bias.\n",
    "            activity_regularizer: Regularizer function for the output.\n",
    "            kernel_constraint: An optional projection function to be applied to the\n",
    "                kernel after being updated by an `Optimizer` (e.g. used to implement\n",
    "                norm constraints or value constraints for layer weights). The function\n",
    "                must take as input the unprojected variable and must return the\n",
    "                projected variable (which must have the same shape). Constraints are\n",
    "                not safe to use when doing asynchronous distributed training.\n",
    "            bias_constraint: An optional projection function to be applied to the\n",
    "                bias after being updated by an `Optimizer`.\n",
    "            trainable: Boolean, if `True` also add variables to the graph collection\n",
    "              `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
    "            use_equalized_learning_rate: bool, if want to scale layer weights to\n",
    "                equalize learning rate each forward pass.\n",
    "            name: String, the name of the layer. Layers with the same name will\n",
    "              share weights, but to avoid mistakes we require reuse=True in such cases.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            units=units,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Whether we will scale weights using He init every forward pass.\n",
    "        self.use_equalized_learning_rate = use_equalized_learning_rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Calls layer and returns outputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, input tensor of shape [batch_size, features].\n",
    "        \"\"\"\n",
    "        if self.use_equalized_learning_rate:\n",
    "            # Scale kernel weights by He init fade-in constant.\n",
    "            kernel_shape = [x for x in self.kernel.shape]\n",
    "            fan_in = kernel_shape[0]\n",
    "            he_constant = tf.sqrt(x=2. / float(fan_in))\n",
    "            kernel = self.kernel * he_constant\n",
    "        else:\n",
    "            kernel = self.kernel\n",
    "\n",
    "        rank = len(inputs.shape)\n",
    "        if rank > 2:\n",
    "            # Broadcasting is required for the inputs.\n",
    "            outputs = tf.tensordot(\n",
    "                a=inputs, b=kernel, axes=[[rank - 1], [0]]\n",
    "            )\n",
    "            # Reshape the output back to the original ndim of the input.\n",
    "            if not context.executing_eagerly():\n",
    "                shape = inputs.shape.as_list()\n",
    "                output_shape = shape[:-1] + [self.units]\n",
    "                outputs.set_shape(shape=output_shape)\n",
    "        else:\n",
    "            inputs = tf.cast(x=inputs, dtype=self._compute_dtype)\n",
    "            if isinstance(inputs, tf.SparseTensor):\n",
    "                outputs = tf.sparse_tensor_dense_matmul(sp_a=inputs, b=kernel)\n",
    "            else:\n",
    "                outputs = tf.matmul(a=inputs, b=kernel)\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(value=outputs, bias=self.bias)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)  # pylint: disable=not-callable\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class WeightScaledConv2D(tf.keras.layers.Conv2D):\n",
    "    \"\"\"Subclassing `Conv2D` layer to allow equalized learning rate scaling.\n",
    "\n",
    "    Attributes:\n",
    "        use_equalized_learning_rate: bool, if want to scale layer weights to\n",
    "            equalize learning rate each forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            data_format=\"channels_last\",\n",
    "            dilation_rate=(1, 1),\n",
    "            activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=None,\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            trainable=True,\n",
    "            use_equalized_learning_rate=False,\n",
    "            name=None,\n",
    "            **kwargs):\n",
    "        \"\"\"Initializes `Conv2D` layer.\n",
    "        Args:\n",
    "            filters: Integer, the dimensionality of the output space (i.e. the number\n",
    "              of filters in the convolution).\n",
    "            kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
    "              height and width of the 2D convolution window.\n",
    "              Can be a single integer to specify the same value for\n",
    "              all spatial dimensions.\n",
    "            strides: An integer or tuple/list of 2 integers,\n",
    "              specifying the strides of the convolution along the height and width.\n",
    "              Can be a single integer to specify the same value for\n",
    "              all spatial dimensions.\n",
    "              Specifying any stride value != 1 is incompatible with specifying\n",
    "              any `dilation_rate` value != 1.\n",
    "            padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
    "            data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
    "              The ordering of the dimensions in the inputs.\n",
    "              `channels_last` corresponds to inputs with shape\n",
    "              `(batch, height, width, channels)` while `channels_first` corresponds to\n",
    "              inputs with shape `(batch, channels, height, width)`.\n",
    "            dilation_rate: An integer or tuple/list of 2 integers, specifying\n",
    "              the dilation rate to use for dilated convolution.\n",
    "              Can be a single integer to specify the same value for\n",
    "              all spatial dimensions.\n",
    "              Currently, specifying any `dilation_rate` value != 1 is\n",
    "              incompatible with specifying any stride value != 1.\n",
    "            activation: Activation function. Set it to None to maintain a\n",
    "              linear activation.\n",
    "            use_bias: Boolean, whether the layer uses a bias.\n",
    "            kernel_initializer: An initializer for the convolution kernel.\n",
    "            bias_initializer: An initializer for the bias vector. If None, the default\n",
    "              initializer will be used.\n",
    "            kernel_regularizer: Optional regularizer for the convolution kernel.\n",
    "            bias_regularizer: Optional regularizer for the bias vector.\n",
    "            activity_regularizer: Optional regularizer function for the output.\n",
    "            kernel_constraint: Optional projection function to be applied to the\n",
    "                kernel after being updated by an `Optimizer` (e.g. used to implement\n",
    "                norm constraints or value constraints for layer weights). The function\n",
    "                must take as input the unprojected variable and must return the\n",
    "                projected variable (which must have the same shape). Constraints are\n",
    "                not safe to use when doing asynchronous distributed training.\n",
    "            bias_constraint: Optional projection function to be applied to the\n",
    "                bias after being updated by an `Optimizer`.\n",
    "            trainable: Boolean, if `True` also add variables to the graph collection\n",
    "              `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
    "            use_equalized_learning_rate: bool, if want to scale layer weights to\n",
    "                equalize learning rate each forward pass.\n",
    "            name: A string, the name of the layer.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Whether we will scale weights using He init every forward pass.\n",
    "        self.use_equalized_learning_rate = use_equalized_learning_rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Calls layer and returns outputs.\n",
    "        Args:\n",
    "            inputs: tensor, input tensor of shape\n",
    "                [batch_size, height, width, channels].\n",
    "        \"\"\"\n",
    "        if self.use_equalized_learning_rate:\n",
    "            # Scale kernel weights by He init constant.\n",
    "            kernel_shape = [x for x in self.kernel.shape]\n",
    "            fan_in = kernel_shape[0] * kernel_shape[1] * kernel_shape[2]\n",
    "            he_constant = tf.sqrt(x=2. / float(fan_in))\n",
    "            kernel = self.kernel * he_constant\n",
    "        else:\n",
    "            kernel = self.kernel\n",
    "\n",
    "        outputs = self._convolution_op(inputs, kernel)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.data_format == \"channels_first\":\n",
    "                if self.rank == 1:\n",
    "                    # nn.bias_add does not accept a 1D input tensor.\n",
    "                    bias = tf.reshape(\n",
    "                        tensor=self.bias, shape=(1, self.filters, 1)\n",
    "                    )\n",
    "                    outputs += bias\n",
    "                else:\n",
    "                    outputs = tf.nn.bias_add(\n",
    "                        value=outputs, bias=self.bias, data_format=\"NCHW\"\n",
    "                    )\n",
    "            else:\n",
    "                outputs = tf.nn.bias_add(\n",
    "                    value=outputs, bias=self.bias, data_format=\"NHWC\"\n",
    "                )\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Normalizes the feature vector in each pixel to unit length.\n",
    "\n",
    "    Attributes:\n",
    "        epsilon: float, small value to add to denominator for numerical\n",
    "            stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Calls PixelNormalization layer with given inputs tensor.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, image feature vectors.\n",
    "\n",
    "        Returns:\n",
    "            Pixel normalized feature vector tensor.\n",
    "        \"\"\"\n",
    "        return inputs * tf.math.rsqrt(\n",
    "            x=tf.add(\n",
    "                x=tf.reduce_mean(\n",
    "                    input_tensor=tf.square(x=inputs), axis=1, keepdims=True\n",
    "                ),\n",
    "                y=self.epsilon\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generators.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/generators.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import custom_layers\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    \"\"\"Generator that takes latent vector input and outputs image.\n",
    "\n",
    "    Attributes:\n",
    "        name: str, name of `Generator`.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "            kernel variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        conv_layers: list, `Conv2D` layers.\n",
    "        leaky_relu_layers: list, leaky relu layers that follow `Conv2D`\n",
    "            layers.\n",
    "        to_rgb_conv_layers: list, `Conv2D` toRGB layers.\n",
    "        model: instance of generator `Model`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_regularizer,\n",
    "        bias_regularizer,\n",
    "        name,\n",
    "        params,\n",
    "        alpha_var\n",
    "    ):\n",
    "        \"\"\"Instantiates and builds generator network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of generator.\n",
    "            params: dict, user passed parameters.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        \"\"\"\n",
    "        # Set name of generator.\n",
    "        self.name = name\n",
    "\n",
    "        # Store regularizers.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        # Store parameters.\n",
    "        self.params = params\n",
    "\n",
    "        # Store reference to alpha variable.\n",
    "        self.alpha_var = alpha_var\n",
    "\n",
    "        # Store lists of layers.\n",
    "        self.conv_layers = []\n",
    "        self.leaky_relu_layers = []\n",
    "        self.to_rgb_conv_layers = []\n",
    "\n",
    "        # Instantiate generator layers.\n",
    "        self._create_generator_layers()\n",
    "\n",
    "        # Store current generator model.\n",
    "        self.model = None\n",
    "\n",
    "    def use_pixel_norm(self, epsilon=1e-8):\n",
    "        \"\"\"Decides based on user parameter whether to use pixel norm or not.\n",
    "\n",
    "        Args:\n",
    "            epsilon: float, small value to add to denominator for numerical\n",
    "                stability.\n",
    "        Returns:\n",
    "            Pixel normalized feature vectors if using pixel norm, else\n",
    "                original feature vectors.\n",
    "        \"\"\"\n",
    "        if self.params[\"generator_use_pixel_norm\"]:\n",
    "            return custom_layers.PixelNormalization(epsilon=epsilon)\n",
    "        return None\n",
    "\n",
    "    def fused_conv2d_act_pixel_norm_block(\n",
    "        self, conv_layer, activation_layer, inputs\n",
    "    ):\n",
    "        \"\"\"Fused Conv2D, activation, and pixel norm operation block.\n",
    "\n",
    "        Args:\n",
    "            conv_layer: instance of `Conv2D` layer.\n",
    "            activation_layer: instance of `Layer`, such as LeakyRelu layer.\n",
    "            inputs: tensor, inputs to fused block.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of fused block.\n",
    "        \"\"\"\n",
    "        network = conv_layer(inputs=inputs)\n",
    "        network = activation_layer(inputs=network)\n",
    "\n",
    "        # Possibly add pixel normalization to image.\n",
    "        pixel_norm_layer = self.use_pixel_norm(\n",
    "            epsilon=self.params[\"generator_pixel_norm_epsilon\"]\n",
    "        )\n",
    "\n",
    "        if pixel_norm_layer is not None:\n",
    "            network = pixel_norm_layer(inputs=network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def _project_latent_vectors(self, latent_vectors):\n",
    "        \"\"\"Defines generator network.\n",
    "\n",
    "        Args:\n",
    "            latent_vectors: tensor, latent vector inputs of shape\n",
    "                [batch_size, latent_size].\n",
    "\n",
    "        Returns:\n",
    "            Projected image of latent vector inputs.\n",
    "        \"\"\"\n",
    "        projection_height = self.params[\"generator_projection_dims\"][0]\n",
    "        projection_width = self.params[\"generator_projection_dims\"][1]\n",
    "        projection_depth = self.params[\"generator_projection_dims\"][2]\n",
    "\n",
    "        # shape = (\n",
    "        #     batch_size,\n",
    "        #     projection_height * projection_width * projection_depth\n",
    "        # )\n",
    "        projection = custom_layers.WeightScaledDense(\n",
    "            units=projection_height * projection_width * projection_depth,\n",
    "            activation=None,\n",
    "            kernel_initializer=(\n",
    "                tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                if self.params[\"use_equalized_learning_rate\"]\n",
    "                else \"he_normal\"\n",
    "            ),\n",
    "            kernel_regularizer=self.kernel_regularizer,\n",
    "            bias_regularizer=self.bias_regularizer,\n",
    "            use_equalized_learning_rate=(\n",
    "                self.params[\"use_equalized_learning_rate\"]\n",
    "            ),\n",
    "            name=\"projection_dense_layer\"\n",
    "        )(inputs=latent_vectors)\n",
    "\n",
    "        projection_leaky_relu = tf.keras.layers.LeakyReLU(\n",
    "            alpha=self.params[\"generator_leaky_relu_alpha\"],\n",
    "            name=\"projection_leaky_relu\"\n",
    "        )(inputs=projection)\n",
    "\n",
    "        # Reshape projection into \"image\".\n",
    "        # shape = (\n",
    "        #     batch_size,\n",
    "        #     projection_height,\n",
    "        #     projection_width,\n",
    "        #     projection_depth\n",
    "        # )\n",
    "        projected_image = tf.reshape(\n",
    "            tensor=projection_leaky_relu,\n",
    "            shape=[\n",
    "                -1, projection_height, projection_width, projection_depth\n",
    "            ],\n",
    "            name=\"projected_image\"\n",
    "        )\n",
    "\n",
    "        # Possibly add pixel normalization to image.\n",
    "        if self.params[\"generator_normalize_latents\"]:\n",
    "            pixel_norm_layer = self.use_pixel_norm(\n",
    "                epsilon=self.params[\"generator_pixel_norm_epsilon\"]\n",
    "            )\n",
    "\n",
    "            if pixel_norm_layer is not None:\n",
    "                projected_image = pixel_norm_layer(inputs=projected_image)\n",
    "\n",
    "        return projected_image\n",
    "\n",
    "    def _create_base_conv_layer_block(self):\n",
    "        \"\"\"Creates generator base conv layer block.\n",
    "\n",
    "        Returns:\n",
    "            List of base block conv layers and list of leaky relu layers.\n",
    "        \"\"\"\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = self.params[\"generator_base_conv_blocks\"][0]\n",
    "\n",
    "        # Create list of base conv layers.\n",
    "        base_conv_layers = [\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=self.params[\"use_equalized_learning_rate\"],\n",
    "                name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        base_leaky_relu_layers = [\n",
    "            tf.keras.layers.LeakyReLU(\n",
    "                alpha=self.params[\"generator_leaky_relu_alpha\"],\n",
    "                name=\"{}_base_conv_leaky_relu_{}\".format(self.name, i)\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        return base_conv_layers, base_leaky_relu_layers\n",
    "\n",
    "    def _create_growth_conv_layer_block(self, block_idx):\n",
    "        \"\"\"Creates generator growth conv layer block.\n",
    "\n",
    "        Args:\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of growth block's conv layers and list of growth block's\n",
    "                leaky relu layers.\n",
    "        \"\"\"\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = self.params[\"generator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "        # Create new growth convolutional layers.\n",
    "        growth_conv_layers = [\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=self.params[\"use_equalized_learning_rate\"],\n",
    "                name=\"{}_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    block_idx,\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        growth_leaky_relu_layers = [\n",
    "            tf.keras.layers.LeakyReLU(\n",
    "                alpha=self.params[\"generator_leaky_relu_alpha\"],\n",
    "                name=\"{}_growth_conv_leaky_relu_{}_{}\".format(\n",
    "                    self.name, block_idx, i\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        return growth_conv_layers, growth_leaky_relu_layers\n",
    "\n",
    "    def _create_to_rgb_layers(self):\n",
    "        \"\"\"Creates generator toRGB layers of 1x1 convs.\n",
    "\n",
    "        Returns:\n",
    "            List of toRGB 1x1 conv layers.\n",
    "        \"\"\"\n",
    "        # Dictionary containing possible final activations.\n",
    "        final_activation_set = {\"sigmoid\", \"relu\", \"tanh\"}\n",
    "\n",
    "        # Get toRGB layer properties.\n",
    "        to_rgb = [\n",
    "            self.params[\"generator_to_rgb_layers\"][i][0][:]\n",
    "            for i in range(\n",
    "                len(self.params[\"generator_to_rgb_layers\"])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Create list to hold toRGB 1x1 convs.\n",
    "        to_rgb_conv_layers = [\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=to_rgb[i][3],\n",
    "                kernel_size=to_rgb[i][0:2],\n",
    "                strides=to_rgb[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=(\n",
    "                    self.params[\"generator_final_activation\"].lower()\n",
    "                    if self.params[\"generator_final_activation\"].lower()\n",
    "                    in final_activation_set\n",
    "                    else None\n",
    "                ),\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=self.params[\"use_equalized_learning_rate\"],\n",
    "                name=\"{}_to_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    i,\n",
    "                    to_rgb[i][0],\n",
    "                    to_rgb[i][1],\n",
    "                    to_rgb[i][2],\n",
    "                    to_rgb[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(to_rgb))\n",
    "        ]\n",
    "\n",
    "        return to_rgb_conv_layers\n",
    "\n",
    "    def _create_generator_layers(self):\n",
    "        \"\"\"Creates generator layers.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "        \"\"\"\n",
    "        (base_conv_layers,\n",
    "         base_leaky_relu_layers) = self._create_base_conv_layer_block()\n",
    "        self.conv_layers.append(base_conv_layers)\n",
    "        self.leaky_relu_layers.append(base_leaky_relu_layers)\n",
    "\n",
    "        for block_idx in range(\n",
    "            len(self.params[\"generator_growth_conv_blocks\"])\n",
    "        ):\n",
    "            (growth_conv_layers,\n",
    "             growth_leaky_relu_layers\n",
    "             ) = self._create_growth_conv_layer_block(block_idx)\n",
    "\n",
    "            self.conv_layers.append(growth_conv_layers)\n",
    "            self.leaky_relu_layers.append(growth_leaky_relu_layers)\n",
    "\n",
    "        self.to_rgb_conv_layers = self._create_to_rgb_layers()\n",
    "\n",
    "    def _upsample_generator_image(self, image, orig_img_size, block_idx):\n",
    "        \"\"\"Upsamples generator intermediate image.\n",
    "        Args:\n",
    "            image: tensor, image created by vec_to_img conv block.\n",
    "            orig_img_size: list, the height and width dimensions of the\n",
    "                original image before any growth.\n",
    "            block_idx: int, index of the current vec_to_img growth block.\n",
    "        Returns:\n",
    "            Upsampled image tensor.\n",
    "        \"\"\"\n",
    "        # Upsample from s X s to 2s X 2s image.\n",
    "        upsampled_image = tf.image.resize(\n",
    "            images=image,\n",
    "            size=tf.convert_to_tensor(\n",
    "                value=orig_img_size,\n",
    "                dtype=tf.int32\n",
    "            ) * 2 ** block_idx,\n",
    "            method=\"nearest\",\n",
    "            name=\"{}_growth_upsampled_image_{}_{}x{}_{}x{}\".format(\n",
    "                self.name,\n",
    "                block_idx,\n",
    "                orig_img_size[0] * 2 ** (block_idx - 1),\n",
    "                orig_img_size[1] * 2 ** (block_idx - 1),\n",
    "                orig_img_size[0] * 2 ** block_idx,\n",
    "                orig_img_size[1] * 2 ** block_idx\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return upsampled_image\n",
    "\n",
    "    def _build_base_model(self, input_shape, batch_size):\n",
    "        \"\"\"Builds generator base model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to generator.\n",
    "        # shape = (batch_size, latent_size)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Project latent vectors.\n",
    "        network = self._project_latent_vectors(latent_vectors=inputs)\n",
    "\n",
    "        # Get base block layers.\n",
    "        base_conv_layers = self.conv_layers[0]\n",
    "        base_leaky_relu_layers = self.leaky_relu_layers[0]\n",
    "        base_to_rgb_conv_layer = self.to_rgb_conv_layers[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        for i in range(len(base_conv_layers)):\n",
    "            network = self.fused_conv2d_act_pixel_norm_block(\n",
    "                conv_layer=base_conv_layers[i],\n",
    "                activation_layer=base_leaky_relu_layers[i],\n",
    "                inputs=network\n",
    "            )\n",
    "\n",
    "        fake_images = base_to_rgb_conv_layer(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=fake_images,\n",
    "            name=\"{}_base\".format(self.name)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_growth_transition_model(\n",
    "        self, input_shape, batch_size, block_idx\n",
    "    ):\n",
    "        \"\"\"Builds generator growth transition model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "            block_idx: int, current block index of model progression.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to generator.\n",
    "        # shape = (batch_size, latent_size)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Project latent vectors.\n",
    "        network = self._project_latent_vectors(latent_vectors=inputs)\n",
    "\n",
    "        # Permanent blocks.\n",
    "        permanent_conv_layers = self.conv_layers[0:block_idx]\n",
    "        permanent_leaky_relu_layers = self.leaky_relu_layers[0:block_idx]\n",
    "\n",
    "        # Base block doesn't need any upsampling so handle differently.\n",
    "        base_conv_layers = permanent_conv_layers[0]\n",
    "        base_leaky_relu_layers = permanent_leaky_relu_layers[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        for i in range(len(base_conv_layers)):\n",
    "            network = self.fused_conv2d_act_pixel_norm_block(\n",
    "                conv_layer=base_conv_layers[i],\n",
    "                activation_layer=base_leaky_relu_layers[i],\n",
    "                inputs=network\n",
    "            )\n",
    "\n",
    "        # Growth blocks require first prev conv layer's image upsampled.\n",
    "        for i in range(1, len(permanent_conv_layers)):\n",
    "            # Upsample previous block's image.\n",
    "            network = self._upsample_generator_image(\n",
    "                image=network,\n",
    "                orig_img_size=self.params[\"generator_projection_dims\"][0:2],\n",
    "                block_idx=i\n",
    "            )\n",
    "\n",
    "            block_conv_layers = permanent_conv_layers[i]\n",
    "            block_leaky_relu_layers = permanent_leaky_relu_layers[i]\n",
    "            for j in range(0, len(block_conv_layers)):\n",
    "                network = self.fused_conv2d_act_pixel_norm_block(\n",
    "                    conv_layer=block_conv_layers[j],\n",
    "                    activation_layer=block_leaky_relu_layers[j],\n",
    "                    inputs=network\n",
    "                )\n",
    "\n",
    "        # Upsample most recent block conv image for both side chains.\n",
    "        upsampled_block_conv = self._upsample_generator_image(\n",
    "            image=network,\n",
    "            orig_img_size=self.params[\"generator_projection_dims\"][0:2],\n",
    "            block_idx=len(permanent_conv_layers)\n",
    "        )\n",
    "\n",
    "        # Growing side chain.\n",
    "        growing_conv_layers = self.conv_layers[block_idx]\n",
    "        growing_leaky_relu_layers = self.leaky_relu_layers[block_idx]\n",
    "        growing_to_rgb_conv_layer = self.to_rgb_conv_layers[block_idx]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        network = upsampled_block_conv\n",
    "        for i in range(0, len(growing_conv_layers)):\n",
    "            network = self.fused_conv2d_act_pixel_norm_block(\n",
    "                conv_layer=growing_conv_layers[i],\n",
    "                activation_layer=growing_leaky_relu_layers[i],\n",
    "                inputs=network\n",
    "            )\n",
    "\n",
    "        growing_to_rgb_conv = growing_to_rgb_conv_layer(inputs=network)\n",
    "\n",
    "        # Shrinking side chain.\n",
    "        shrinking_to_rgb_conv_layer = self.to_rgb_conv_layers[block_idx - 1]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        shrinking_to_rgb_conv = shrinking_to_rgb_conv_layer(\n",
    "            inputs=upsampled_block_conv\n",
    "        )\n",
    "\n",
    "        # Weighted sum.\n",
    "        weighted_sum = tf.add(\n",
    "            x=growing_to_rgb_conv * self.alpha_var,\n",
    "            y=shrinking_to_rgb_conv * (1.0 - self.alpha_var),\n",
    "            name=\"{}_growth_transition_weighted_sum_{}\".format(\n",
    "                self.name, block_idx\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fake_images = weighted_sum\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=fake_images,\n",
    "            name=\"{}_growth_transition_{}\".format(self.name, block_idx)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_growth_stable_model(self, input_shape, batch_size, block_idx):\n",
    "        \"\"\"Builds generator growth stable model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "            block_idx: int, current block index of model progression.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to generator.\n",
    "        # shape = (batch_size, latent_size)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Project latent vectors.\n",
    "        network = self._project_latent_vectors(latent_vectors=inputs)\n",
    "\n",
    "        # Permanent blocks.\n",
    "        permanent_conv_layers = self.conv_layers[0:block_idx + 1]\n",
    "        permanent_leaky_relu_layers = self.leaky_relu_layers[0:block_idx + 1]\n",
    "\n",
    "        # Base block doesn't need any upsampling so handle differently.\n",
    "        base_conv_layers = permanent_conv_layers[0]\n",
    "        base_leaky_relu_layers = permanent_leaky_relu_layers[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        for i in range(len(base_conv_layers)):\n",
    "            network = self.fused_conv2d_act_pixel_norm_block(\n",
    "                conv_layer=base_conv_layers[i],\n",
    "                activation_layer=base_leaky_relu_layers[i],\n",
    "                inputs=network\n",
    "            )\n",
    "\n",
    "        # Growth blocks require first prev conv layer's image upsampled.\n",
    "        for i in range(1, len(permanent_conv_layers)):\n",
    "            # Upsample previous block's image.\n",
    "            network = self._upsample_generator_image(\n",
    "                image=network,\n",
    "                orig_img_size=self.params[\"generator_projection_dims\"][0:2],\n",
    "                block_idx=i\n",
    "            )\n",
    "\n",
    "            block_conv_layers = permanent_conv_layers[i]\n",
    "            block_leaky_relu_layers = permanent_leaky_relu_layers[i]\n",
    "            for j in range(0, len(block_conv_layers)):\n",
    "                network = self.fused_conv2d_act_pixel_norm_block(\n",
    "                    conv_layer=block_conv_layers[j],\n",
    "                    activation_layer=block_leaky_relu_layers[j],\n",
    "                    inputs=network\n",
    "                )\n",
    "\n",
    "        # Get toRGB layer.\n",
    "        to_rgb_conv_layer = self.to_rgb_conv_layers[block_idx]\n",
    "\n",
    "        fake_images = to_rgb_conv_layer(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=fake_images,\n",
    "            name=\"{}_growth_stable_{}\".format(self.name, block_idx)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_model(self, input_shape, batch_size, growth_idx):\n",
    "        \"\"\"Returns generator's `Model` object.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "            growth_idx: int, index of current growth stage.\n",
    "                0 = base,\n",
    "                odd = growth transition,\n",
    "                even = growth stability.\n",
    "\n",
    "        Returns:\n",
    "            Generator's `Model` object.\n",
    "        \"\"\"\n",
    "        block_idx = (growth_idx + 1) // 2\n",
    "        if growth_idx == 0:\n",
    "            self.model = self._build_base_model(input_shape, batch_size)\n",
    "        elif growth_idx % 2 == 1:\n",
    "            self.model = self._build_growth_transition_model(\n",
    "                input_shape, batch_size, block_idx\n",
    "            )\n",
    "        elif growth_idx % 2 == 0:\n",
    "            self.model = self._build_growth_stable_model(\n",
    "                input_shape, batch_size, block_idx\n",
    "            )\n",
    "        else:\n",
    "            print(\"ERROR: Bad growth index!\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def get_generator_loss(\n",
    "        self,\n",
    "        global_batch_size,\n",
    "        fake_logits,\n",
    "        global_step,\n",
    "        summary_file_writer\n",
    "    ):\n",
    "        \"\"\"Gets generator loss.\n",
    "\n",
    "        Args:\n",
    "            global_batch_size: int, global batch size for distribution.\n",
    "            fake_logits: tensor, shape of\n",
    "                [batch_size, 1].\n",
    "            global_step: int, current global step for training.\n",
    "            summary_file_writer: summary file writer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of generator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        if self.params[\"distribution_strategy\"]:\n",
    "            # Calculate base generator loss.\n",
    "            generator_loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss=-fake_logits,\n",
    "                global_batch_size=global_batch_size\n",
    "            )\n",
    "\n",
    "            # Get regularization losses.\n",
    "            generator_reg_loss = tf.nn.scale_regularization_loss(\n",
    "                regularization_loss=sum(self.model.losses)\n",
    "            )\n",
    "        else:\n",
    "            # Calculate base generator loss.\n",
    "            generator_loss = -tf.reduce_mean(\n",
    "                input_tensor=fake_logits,\n",
    "                name=\"{}_loss\".format(self.name)\n",
    "            )\n",
    "\n",
    "            # Get regularization losses.\n",
    "            generator_reg_loss = sum(self.model.losses)\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        generator_total_loss = tf.math.add(\n",
    "            x=generator_loss,\n",
    "            y=generator_reg_loss,\n",
    "            name=\"generator_total_loss\"\n",
    "        )\n",
    "\n",
    "        if self.params[\"write_summaries\"]:\n",
    "            # Add summaries for TensorBoard.\n",
    "            with summary_file_writer.as_default():\n",
    "                with tf.summary.record_if(\n",
    "                    condition=tf.equal(\n",
    "                        x=tf.math.floormod(\n",
    "                            x=global_step,\n",
    "                            y=self.params[\"save_summary_steps\"]\n",
    "                        ), y=0\n",
    "                    )\n",
    "                ):\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/generator_loss\",\n",
    "                        data=generator_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/generator_reg_loss\",\n",
    "                        data=generator_reg_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"optimized_losses/generator_total_loss\",\n",
    "                        data=generator_total_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    summary_file_writer.flush()\n",
    "\n",
    "        return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/discriminators.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import custom_layers\n",
    "\n",
    "\n",
    "class Discriminator(object):\n",
    "    \"\"\"Discriminator that takes image input and outputs logits.\n",
    "\n",
    "    Attributes:\n",
    "        name: str, name of `Discriminator`.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "            kernel variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        from_rgb_conv_layers: list, `Conv2D` fromRGB layers.\n",
    "        from_rgb_leaky_relu_layers: list, leaky relu layers that follow\n",
    "            `Conv2D` fromRGB layers.\n",
    "        conv_layers: list, `Conv2D` layers.\n",
    "        leaky_relu_layers: list, leaky relu layers that follow `Conv2D`\n",
    "            layers.\n",
    "        growing_downsample_layers: list, `AveragePooling2D` layers for growing\n",
    "            branch.\n",
    "        shrinking_downsample_layers: list, `AveragePooling2D` layers for\n",
    "            shrinking branch.\n",
    "        flatten_layer: `Flatten` layer, flattens image for logits layer.\n",
    "        logits_layer: `Dense` layer, used for calculating logits.\n",
    "        model: instance of discriminator `Model`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_regularizer,\n",
    "        bias_regularizer,\n",
    "        name,\n",
    "        params,\n",
    "        alpha_var\n",
    "    ):\n",
    "        \"\"\"Instantiates and builds discriminator network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of discriminator.\n",
    "            params: dict, user passed parameters.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        \"\"\"\n",
    "        # Set name of discriminator.\n",
    "        self.name = name\n",
    "\n",
    "        # Store regularizers.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        # Store parameters.\n",
    "        self.params = params\n",
    "\n",
    "        # Store reference to alpha variable.\n",
    "        self.alpha_var = alpha_var\n",
    "\n",
    "        # Store lists of layers.\n",
    "        self.from_rgb_conv_layers = []\n",
    "        self.from_rgb_leaky_relu_layers = []\n",
    "\n",
    "        self.conv_layers = []\n",
    "        self.leaky_relu_layers = []\n",
    "\n",
    "        self.growing_downsample_layers = []\n",
    "        self.shrinking_downsample_layers = []\n",
    "\n",
    "        self.flatten_layer = None\n",
    "        self.logits_layer = None\n",
    "\n",
    "        # Instantiate discriminator layers.\n",
    "        self._create_discriminator_layers()\n",
    "\n",
    "        # Store current discriminator model.\n",
    "        self.model = None\n",
    "\n",
    "    def _create_from_rgb_layers(self):\n",
    "        \"\"\"Creates discriminator fromRGB layers of 1x1 convs.\n",
    "\n",
    "        Returns:\n",
    "            List of fromRGB 1x1 conv layers and leaky relu layers.\n",
    "        \"\"\"\n",
    "        # Get fromRGB layer properties.\n",
    "        from_rgb = [\n",
    "            self.params[\"discriminator_from_rgb_layers\"][i][0][:]\n",
    "            for i in range(\n",
    "                len(self.params[\"discriminator_from_rgb_layers\"])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Create list to hold toRGB 1x1 convs.\n",
    "        from_rgb_conv_layers = [\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=from_rgb[i][3],\n",
    "                kernel_size=from_rgb[i][0:2],\n",
    "                strides=from_rgb[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=self.params[\"use_equalized_learning_rate\"],\n",
    "                name=\"{}_from_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    i,\n",
    "                    from_rgb[i][0],\n",
    "                    from_rgb[i][1],\n",
    "                    from_rgb[i][2],\n",
    "                    from_rgb[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(from_rgb))\n",
    "        ]\n",
    "\n",
    "        from_rgb_leaky_relu_layers = [\n",
    "            tf.keras.layers.LeakyReLU(\n",
    "                alpha=self.params[\"discriminator_leaky_relu_alpha\"],\n",
    "                name=\"{}_from_rgb_conv_leaky_relu_{}\".format(self.name, i)\n",
    "            )\n",
    "            for i in range(len(from_rgb))\n",
    "        ]\n",
    "\n",
    "        return from_rgb_conv_layers, from_rgb_leaky_relu_layers\n",
    "\n",
    "    def _create_base_conv_layer_block(self):\n",
    "        \"\"\"Creates discriminator base conv layer block.\n",
    "\n",
    "        Returns:\n",
    "            List of base block conv layers and list of leaky relu layers.\n",
    "        \"\"\"\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = self.params[\"discriminator_base_conv_blocks\"][0]\n",
    "\n",
    "        # Create list of base conv layers.\n",
    "        base_conv_layers = [\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=(\n",
    "                    self.params[\"use_equalized_learning_rate\"]\n",
    "                ),\n",
    "                name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block) - 1)\n",
    "        ]\n",
    "\n",
    "        # Have valid padding for layer just before flatten and logits.\n",
    "        base_conv_layers.append(\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=conv_block[-1][3],\n",
    "                kernel_size=conv_block[-1][0:2],\n",
    "                strides=conv_block[-1][4:6],\n",
    "                padding=\"valid\",\n",
    "                activation=None,\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=(\n",
    "                    self.params[\"use_equalized_learning_rate\"]\n",
    "                ),\n",
    "                name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    len(conv_block) - 1,\n",
    "                    conv_block[-1][0],\n",
    "                    conv_block[-1][1],\n",
    "                    conv_block[-1][2],\n",
    "                    conv_block[-1][3]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        base_leaky_relu_layers = [\n",
    "            tf.keras.layers.LeakyReLU(\n",
    "                alpha=self.params[\"discriminator_leaky_relu_alpha\"],\n",
    "                name=\"{}_base_conv_leaky_relu_{}\".format(self.name, i)\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        return base_conv_layers, base_leaky_relu_layers\n",
    "\n",
    "    def _create_growth_conv_layer_block(self, block_idx):\n",
    "        \"\"\"Creates discriminator growth conv layer block.\n",
    "\n",
    "        Args:\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of growth block's conv layers and list of growth block's\n",
    "                leaky relu layers.\n",
    "        \"\"\"\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = self.params[\"discriminator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "        # Create new growth convolutional layers.\n",
    "        growth_conv_layers = [\n",
    "            custom_layers.WeightScaledConv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_initializer=(\n",
    "                    tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                    if self.params[\"use_equalized_learning_rate\"]\n",
    "                    else \"he_normal\"\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                use_equalized_learning_rate=(\n",
    "                    self.params[\"use_equalized_learning_rate\"]\n",
    "                ),\n",
    "                name=\"{}_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                    self.name,\n",
    "                    block_idx,\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        growth_leaky_relu_layers = [\n",
    "            tf.keras.layers.LeakyReLU(\n",
    "                alpha=self.params[\"discriminator_leaky_relu_alpha\"],\n",
    "                name=\"{}_growth_conv_leaky_relu_{}_{}\".format(\n",
    "                    self.name, block_idx, i\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "\n",
    "        return growth_conv_layers, growth_leaky_relu_layers\n",
    "\n",
    "    def _create_downsample_layers(self):\n",
    "        \"\"\"Creates discriminator downsample layers.\n",
    "\n",
    "        Returns:\n",
    "            Lists of AveragePooling2D layers for growing and shrinking\n",
    "                branches.\n",
    "        \"\"\"\n",
    "        # Create list to hold growing branch's downsampling layers.\n",
    "        growing_downsample_layers = [\n",
    "            tf.keras.layers.AveragePooling2D(\n",
    "                pool_size=(2, 2),\n",
    "                strides=(2, 2),\n",
    "                name=\"{}_growing_average_pooling_2d_{}\".format(\n",
    "                    self.name, i - 1\n",
    "                )\n",
    "            )\n",
    "            for i in range(\n",
    "                1, len(self.params[\"discriminator_from_rgb_layers\"])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Create list to hold shrinking branch's downsampling layers.\n",
    "        shrinking_downsample_layers = [\n",
    "            tf.keras.layers.AveragePooling2D(\n",
    "                pool_size=(2, 2),\n",
    "                strides=(2, 2),\n",
    "                name=\"{}_shrinking_average_pooling_2d_{}\".format(\n",
    "                    self.name, i - 1\n",
    "                )\n",
    "            )\n",
    "            for i in range(\n",
    "                1, len(self.params[\"discriminator_from_rgb_layers\"])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return growing_downsample_layers, shrinking_downsample_layers\n",
    "\n",
    "    def _create_discriminator_layers(self):\n",
    "        \"\"\"Creates discriminator layers.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "        \"\"\"\n",
    "        (self.from_rgb_conv_layers,\n",
    "         self.from_rgb_leaky_relu_layers) = self._create_from_rgb_layers()\n",
    "\n",
    "        (base_conv_layers,\n",
    "         base_leaky_relu_layers) = self._create_base_conv_layer_block()\n",
    "        self.conv_layers.append(base_conv_layers)\n",
    "        self.leaky_relu_layers.append(base_leaky_relu_layers)\n",
    "\n",
    "        for block_idx in range(\n",
    "            len(self.params[\"discriminator_growth_conv_blocks\"])\n",
    "        ):\n",
    "            (growth_conv_layers,\n",
    "             growth_leaky_relu_layers\n",
    "             ) = self._create_growth_conv_layer_block(block_idx)\n",
    "\n",
    "            self.conv_layers.append(growth_conv_layers)\n",
    "            self.leaky_relu_layers.append(growth_leaky_relu_layers)\n",
    "\n",
    "        (self.growing_downsample_layers,\n",
    "         self.shrinking_downsample_layers) = self._create_downsample_layers()\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.logits_layer = custom_layers.WeightScaledDense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=(\n",
    "                tf.random_normal_initializer(mean=0., stddev=1.0)\n",
    "                if self.params[\"use_equalized_learning_rate\"]\n",
    "                else \"he_normal\"\n",
    "            ),\n",
    "            kernel_regularizer=self.kernel_regularizer,\n",
    "            bias_regularizer=self.bias_regularizer,\n",
    "            use_equalized_learning_rate=(\n",
    "                self.params[\"use_equalized_learning_rate\"]\n",
    "            ),\n",
    "            name=\"{}_layers_dense_logits\".format(self.name)\n",
    "        )\n",
    "\n",
    "    def minibatch_stddev_common(self, variance, tile_multiples):\n",
    "        \"\"\"Adds minibatch stddev feature map to image using grouping.\n",
    "\n",
    "        This is the code that is common between the grouped and ungroup\n",
    "        minibatch stddev functions.\n",
    "\n",
    "        Args:\n",
    "            variance: tensor, variance of minibatch or minibatch groups.\n",
    "            tile_multiples: list, length 4, used to tile input to final shape\n",
    "                input_dims[i] * mutliples[i].\n",
    "\n",
    "        Returns:\n",
    "            Minibatch standard deviation feature map image added to\n",
    "                channels of shape\n",
    "                [batch_size, image_height, image_width, 1].\n",
    "        \"\"\"\n",
    "        # Calculate standard deviation over the group plus small epsilon.\n",
    "        # shape = (\n",
    "        #     {\"grouped\": batch_size / group_size, \"ungrouped\": 1},\n",
    "        #     image_size,\n",
    "        #     image_size,\n",
    "        #     num_channels\n",
    "        # )\n",
    "        stddev = tf.sqrt(x=variance + 1e-8, name=\"minibatch_stddev\")\n",
    "\n",
    "        # Take average over feature maps and pixels.\n",
    "        if self.params[\"discriminator_minibatch_stddev_averaging\"]:\n",
    "            # grouped shape = (batch_size / group_size, 1, 1, 1)\n",
    "            # ungrouped shape = (1, 1, 1, 1)\n",
    "            stddev = tf.reduce_mean(\n",
    "                input_tensor=stddev,\n",
    "                axis=[1, 2, 3],\n",
    "                keepdims=True,\n",
    "                name=\"minibatch_stddev_average\"\n",
    "            )\n",
    "\n",
    "        # Replicate over group and pixels.\n",
    "        # shape = (batch_size, image_size, image_size, 1)\n",
    "        stddev_feature_map = tf.tile(\n",
    "            input=stddev,\n",
    "            multiples=tile_multiples,\n",
    "            name=\"minibatch_stddev_feature_map\"\n",
    "        )\n",
    "\n",
    "        return stddev_feature_map\n",
    "\n",
    "    def grouped_minibatch_stddev(self, inputs, batch_size, group_size):\n",
    "        \"\"\"Adds minibatch stddev feature map to image using grouping.\n",
    "\n",
    "        Args:\n",
    "            inputs: tf.float32 tensor, image of shape\n",
    "                [batch_size, image_height, image_width, num_channels].\n",
    "            batch_size: tf.int64 tensor, the dynamic batch size (in case\n",
    "                of partial batch).\n",
    "            group_size: int, size of image groups.\n",
    "\n",
    "        Returns:\n",
    "            Minibatch standard deviation feature map image added to\n",
    "                channels of shape\n",
    "                [batch_size, image_height, image_width, 1].\n",
    "        \"\"\"\n",
    "        # The group size should be less than or equal to the batch size.\n",
    "        group_size = tf.minimum(x=group_size, y=batch_size)\n",
    "\n",
    "        # Split minibatch into M groups of size group_size, rank 5 tensor.\n",
    "        # shape = (\n",
    "        #     group_size,\n",
    "        #     batch_size / group_size,\n",
    "        #     image_size,\n",
    "        #     image_size,\n",
    "        #     num_channels\n",
    "        # )\n",
    "        grouped_image = tf.reshape(\n",
    "            tensor=inputs,\n",
    "            shape=[group_size, -1] + list(inputs.shape[1:]),\n",
    "            name=\"grouped_image\"\n",
    "        )\n",
    "\n",
    "        # Find the mean of each group.\n",
    "        # shape = (\n",
    "        #     1,\n",
    "        #     batch_size / group_size,\n",
    "        #     image_size,\n",
    "        #     image_size,\n",
    "        #     num_channels\n",
    "        # )\n",
    "        grouped_mean = tf.reduce_mean(\n",
    "            input_tensor=grouped_image,\n",
    "            axis=0,\n",
    "            keepdims=True,\n",
    "            name=\"grouped_mean\"\n",
    "        )\n",
    "\n",
    "        # Center each group using the mean.\n",
    "        # shape = (\n",
    "        #     group_size,\n",
    "        #     batch_size / group_size,\n",
    "        #     image_size,\n",
    "        #     image_size,\n",
    "        #     num_channels\n",
    "        # )\n",
    "        centered_grouped_image = tf.subtract(\n",
    "            x=grouped_image, y=grouped_mean, name=\"centered_grouped_image\"\n",
    "        )\n",
    "\n",
    "        # Calculate variance over group.\n",
    "        # shape = (\n",
    "        #     batch_size / group_size, image_size, image_size, num_channels\n",
    "        # )\n",
    "        grouped_variance = tf.reduce_mean(\n",
    "            input_tensor=tf.square(x=centered_grouped_image),\n",
    "            axis=0,\n",
    "            name=\"grouped_variance\"\n",
    "        )\n",
    "\n",
    "        # Get stddev image using ops common to both grouped & ungrouped.\n",
    "        tile_multiples = [group_size] + list(inputs.shape[1:3]) + [1]\n",
    "        stddev_feature_map = self.minibatch_stddev_common(\n",
    "            variance=grouped_variance, tile_multiples=tile_multiples\n",
    "        )\n",
    "\n",
    "        return stddev_feature_map\n",
    "\n",
    "    def ungrouped_minibatch_stddev(self, inputs, batch_size):\n",
    "        \"\"\"Adds minibatch stddev feature map added to image channels.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, image of shape\n",
    "                [batch_size, image_height, image_width, num_channels].\n",
    "            batch_size: tf.int64 tensor, the dynamic batch size (in case\n",
    "                of partial batch).\n",
    "\n",
    "        Returns:\n",
    "            Minibatch standard deviation feature map image added to\n",
    "                channels of shape\n",
    "                [batch_size, image_height, image_width, 1].\n",
    "        \"\"\"\n",
    "        # Find the mean of each group.\n",
    "        # shape = (1, image_size, image_size, num_channels)\n",
    "        mean = tf.reduce_mean(\n",
    "            input_tensor=inputs, axis=0, keepdims=True, name=\"mean\"\n",
    "        )\n",
    "\n",
    "        # Center each group using the mean.\n",
    "        # shape = (batch_size, image_size, image_size, num_channels)\n",
    "        centered_image = tf.subtract(\n",
    "            x=inputs, y=mean, name=\"centered_image\"\n",
    "        )\n",
    "\n",
    "        # Calculate variance over group.\n",
    "        # shape = (1, image_size, image_size, num_channels)\n",
    "        variance = tf.reduce_mean(\n",
    "            input_tensor=tf.square(x=centered_image),\n",
    "            axis=0,\n",
    "            keepdims=True,\n",
    "            name=\"variance\"\n",
    "        )\n",
    "\n",
    "        # Get stddev image using ops common to both grouped & ungrouped.\n",
    "        tile_multiples = [batch_size] + list(inputs.shape[1:3]) + [1]\n",
    "        stddev_feature_map = self.minibatch_stddev_common(\n",
    "            variance=variance, tile_multiples=tile_multiples\n",
    "        )\n",
    "\n",
    "        return stddev_feature_map\n",
    "\n",
    "    def minibatch_stddev(self, inputs, group_size=4):\n",
    "        \"\"\"Adds minibatch stddev feature map added to image.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, image of shape\n",
    "                [batch_size, image_height, image_width, num_channels].\n",
    "            group_size: int, size of image groups.\n",
    "\n",
    "        Returns:\n",
    "            Image with minibatch standard deviation feature map added to\n",
    "                channels of shape\n",
    "                [batch_size, image_height, image_width, num_channels + 1].\n",
    "        \"\"\"\n",
    "        # Get batch size.\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        if (batch_size % group_size == 0 or batch_size < group_size):\n",
    "            stddev_feature_map = self.grouped_minibatch_stddev(\n",
    "                inputs=inputs, batch_size=batch_size, group_size=group_size\n",
    "            )\n",
    "        else:\n",
    "            stddev_feature_map = self.ungrouped_minibatch_stddev(\n",
    "                inputs=inputs, batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        # Append new feature map to image.\n",
    "        # shape = (batch_size, image_height, image_width, num_channels + 1)\n",
    "        appended_image = tf.concat(\n",
    "            values=[inputs, stddev_feature_map],\n",
    "            axis=-1,\n",
    "            name=\"appended_image\"\n",
    "        )\n",
    "\n",
    "        return appended_image\n",
    "\n",
    "    def _use_logits_layer(self, inputs):\n",
    "        \"\"\"Uses flatten and logits layers to get logits tensor.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, output of last conv layer of discriminator.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        # Set shape to remove ambiguity for dense layer.\n",
    "        inputs.set_shape(\n",
    "            [\n",
    "                inputs.get_shape()[0],\n",
    "                self.params[\"generator_projection_dims\"][0] // 4,\n",
    "                self.params[\"generator_projection_dims\"][1] // 4,\n",
    "                inputs.get_shape()[-1]]\n",
    "        )\n",
    "\n",
    "        # Flatten final block conv tensor.\n",
    "        flat_inputs = self.flatten_layer(inputs=inputs)\n",
    "\n",
    "        # Final linear layer for logits.\n",
    "        logits = self.logits_layer(inputs=flat_inputs)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _create_base_block_and_logits(self, inputs):\n",
    "        \"\"\"Creates base discriminator block and logits.\n",
    "\n",
    "        Args:\n",
    "            block_conv: tensor, output of previous `Conv2D` block's layer.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        # Only need the first conv layer block for base network.\n",
    "        base_conv_layers = self.conv_layers[0]\n",
    "        base_leaky_relu_layers = self.leaky_relu_layers[0]\n",
    "\n",
    "        network = inputs\n",
    "        if self.params[\"discriminator_use_minibatch_stddev\"]:\n",
    "            network = self.minibatch_stddev(\n",
    "                inputs=network,\n",
    "                group_size=(\n",
    "                    self.params[\"discriminator_minibatch_stddev_group_size\"]\n",
    "                )\n",
    "            )\n",
    "        for i in range(len(base_conv_layers)):\n",
    "            network = base_conv_layers[i](inputs=network)\n",
    "            network = base_leaky_relu_layers[i](inputs=network)\n",
    "\n",
    "        # Get logits now.\n",
    "        logits = self._use_logits_layer(inputs=network)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _create_growth_transition_weighted_sum(self, inputs, block_idx):\n",
    "        \"\"\"Creates growth transition img_to_vec weighted_sum.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, input image to discriminator.\n",
    "            block_idx: int, current block index of model progression.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of weighted sum between shrinking and growing block paths.\n",
    "        \"\"\"\n",
    "        # Growing side chain.\n",
    "        growing_from_rgb_conv_layer = self.from_rgb_conv_layers[block_idx]\n",
    "        growing_from_rgb_leaky_relu_layer = (\n",
    "            self.from_rgb_leaky_relu_layers[block_idx]\n",
    "        )\n",
    "        growing_downsample_layer = (\n",
    "            self.growing_downsample_layers[block_idx - 1]\n",
    "        )\n",
    "\n",
    "        growing_conv_layers = self.conv_layers[block_idx]\n",
    "        growing_leaky_relu_layers = self.leaky_relu_layers[block_idx]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        network = growing_from_rgb_conv_layer(inputs=inputs)\n",
    "        network = growing_from_rgb_leaky_relu_layer(inputs=network)\n",
    "\n",
    "        for i in range(len(growing_conv_layers)):\n",
    "            network = growing_conv_layers[i](inputs=network)\n",
    "            network = growing_leaky_relu_layers[i](inputs=network)\n",
    "\n",
    "        # Down sample from 2s X 2s to s X s image.\n",
    "        growing_network = growing_downsample_layer(inputs=network)\n",
    "\n",
    "        # Shrinking side chain.\n",
    "        shrinking_from_rgb_conv_layer = (\n",
    "            self.from_rgb_conv_layers[block_idx - 1]\n",
    "        )\n",
    "        shrinking_from_rgb_leaky_relu_layer = (\n",
    "            self.from_rgb_leaky_relu_layers[block_idx - 1]\n",
    "        )\n",
    "        shrinking_downsample_layer = (\n",
    "            self.shrinking_downsample_layers[block_idx - 1]\n",
    "        )\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        # Down sample from 2s X 2s to s X s image.\n",
    "        network = shrinking_downsample_layer(inputs=inputs)\n",
    "\n",
    "        network = shrinking_from_rgb_conv_layer(inputs=network)\n",
    "        shrinking_network = shrinking_from_rgb_leaky_relu_layer(\n",
    "            inputs=network\n",
    "        )\n",
    "\n",
    "        # Weighted sum.\n",
    "        weighted_sum = tf.add(\n",
    "            x=growing_network * self.alpha_var,\n",
    "            y=shrinking_network * (1.0 - self.alpha_var),\n",
    "            name=\"{}_growth_transition_weighted_sum_{}\".format(\n",
    "                self.name, block_idx\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "    def _create_perm_growth_block_network(self, inputs, block_idx):\n",
    "        \"\"\"Creates discriminator permanent block network.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, output of previous block's layer.\n",
    "            block_idx: int, current block index of model progression.\n",
    "\n",
    "        Returns:\n",
    "            Tensor from final permanent block `Conv2D` layer.\n",
    "        \"\"\"\n",
    "        # Get permanent growth blocks, so skip the base block.\n",
    "        permanent_conv_layers = self.conv_layers[1:block_idx]\n",
    "        permanent_leaky_relu_layers = self.leaky_relu_layers[1:block_idx]\n",
    "        permanent_downsample_layers = self.growing_downsample_layers[0:block_idx - 1]\n",
    "\n",
    "        # Reverse order of blocks.\n",
    "        permanent_conv_layers = permanent_conv_layers[::-1]\n",
    "        permanent_leaky_relu_layers = permanent_leaky_relu_layers[::-1]\n",
    "        permanent_downsample_layers = permanent_downsample_layers[::-1]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        network = inputs\n",
    "\n",
    "        # Loop through the permanent growth blocks.\n",
    "        for i in range(len(permanent_conv_layers)):\n",
    "            # Get layers from ith permanent block.\n",
    "            conv_layers = permanent_conv_layers[i]\n",
    "            leaky_relu_layers = permanent_leaky_relu_layers[i]\n",
    "            permanent_downsample_layer = permanent_downsample_layers[i]\n",
    "\n",
    "            # Loop through layers of ith permanent block.\n",
    "            for j in range(len(conv_layers)):\n",
    "                network = conv_layers[j](inputs=network)\n",
    "                network = leaky_relu_layers[j](inputs=network)\n",
    "\n",
    "            # Down sample from 2s X 2s to s X s image.\n",
    "            network = permanent_downsample_layer(inputs=network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def _build_base_model(self, input_shape, batch_size):\n",
    "        \"\"\"Builds discriminator base model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of image vector input of shape\n",
    "                [batch_size, height, width, depth].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to discriminator.\n",
    "        # shape = (batch_size, height, width, depth)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Only need the first fromRGB conv layer & block for base network.\n",
    "        base_from_rgb_conv_layer = self.from_rgb_conv_layers[0]\n",
    "        base_from_rgb_leaky_relu_layer = self.from_rgb_leaky_relu_layers[0]\n",
    "\n",
    "        base_conv_layers = self.conv_layers[0]\n",
    "        base_leaky_relu_layers = self.leaky_relu_layers[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        network = base_from_rgb_conv_layer(inputs=inputs)\n",
    "        network = base_from_rgb_leaky_relu_layer(inputs=network)\n",
    "\n",
    "        # Get logits after continuing through base conv block.\n",
    "        logits = self._create_base_block_and_logits(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=logits,\n",
    "            name=\"{}_base\".format(self.name)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_growth_transition_model(\n",
    "        self, input_shape, batch_size, block_idx\n",
    "    ):\n",
    "        \"\"\"Builds discriminator growth transition model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, height, width, depth].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "            block_idx: int, current block index of model progression.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to discriminator.\n",
    "        # shape = (batch_size, height, width, depth)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Get weighted sum between shrinking and growing block paths.\n",
    "        weighted_sum = self._create_growth_transition_weighted_sum(\n",
    "            inputs=inputs, block_idx=block_idx\n",
    "        )\n",
    "\n",
    "        # Get output of final permanent growth block's last `Conv2D` layer.\n",
    "        network = self._create_perm_growth_block_network(\n",
    "            inputs=weighted_sum, block_idx=block_idx\n",
    "        )\n",
    "\n",
    "        # Get logits after continuing through base conv block.\n",
    "        logits = self._create_base_block_and_logits(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=logits,\n",
    "            name=\"{}_growth_transition_{}\".format(self.name, block_idx)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_growth_stable_model(self, input_shape, batch_size, block_idx):\n",
    "        \"\"\"Builds generator growth stable model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "            block_idx: int, current block index of model progression.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to generator.\n",
    "        # shape = (batch_size, latent_size)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Get fromRGB layers.\n",
    "        from_rgb_conv_layer = self.from_rgb_conv_layers[block_idx]\n",
    "        from_rgb_leaky_relu_layer = self.from_rgb_leaky_relu_layers[block_idx]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        network = from_rgb_conv_layer(inputs=inputs)\n",
    "        network = from_rgb_leaky_relu_layer(inputs=network)\n",
    "\n",
    "        # Get output of final permanent growth block's last `Conv2D` layer.\n",
    "        network = self._create_perm_growth_block_network(\n",
    "            inputs=network, block_idx=block_idx + 1\n",
    "        )\n",
    "\n",
    "        # Get logits after continuing through base conv block.\n",
    "        logits = self._create_base_block_and_logits(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=logits,\n",
    "            name=\"{}_growth_stable_{}\".format(self.name, block_idx)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_model(self, input_shape, batch_size, growth_idx):\n",
    "        \"\"\"Returns discriminator's `Model` object.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of image input of shape\n",
    "                [batch_size, height, width, depth].\n",
    "            batch_size: int, fixed number of examples within batch.\n",
    "            growth_idx: int, index of current growth stage.\n",
    "                0 = base,\n",
    "                odd = growth transition,\n",
    "                even = growth stability.\n",
    "\n",
    "        Returns:\n",
    "            Discriminator's `Model` object.\n",
    "        \"\"\"\n",
    "        block_idx = (growth_idx + 1) // 2\n",
    "        if growth_idx == 0:\n",
    "            self.model = self._build_base_model(input_shape, batch_size)\n",
    "        elif growth_idx % 2 == 1:\n",
    "            self.model = self._build_growth_transition_model(\n",
    "                input_shape, batch_size, block_idx\n",
    "            )\n",
    "        elif growth_idx % 2 == 0:\n",
    "            self.model = self._build_growth_stable_model(\n",
    "                input_shape, batch_size, block_idx\n",
    "            )\n",
    "        else:\n",
    "            print(\"ERROR: Bad growth index!\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def get_gradient_penalty_loss(self, fake_images, real_images):\n",
    "        \"\"\"Gets discriminator gradient penalty loss.\n",
    "\n",
    "        Args:\n",
    "            fake_images: tensor, images generated by the generator from random\n",
    "                noise of shape [batch_size, image_size, image_size, 3].\n",
    "            real_images: tensor, real images from input of shape\n",
    "                [batch_size, image_height, image_width, 3].\n",
    "\n",
    "        Returns:\n",
    "            Discriminator's gradient penalty loss of shape [].\n",
    "        \"\"\"\n",
    "        batch_size = real_images.shape[0]\n",
    "\n",
    "        # Get a random uniform number rank 4 tensor.\n",
    "        random_uniform_num = tf.random.uniform(\n",
    "            shape=[batch_size, 1, 1, 1],\n",
    "            minval=0., maxval=1.,\n",
    "            dtype=tf.float32,\n",
    "            name=\"gp_random_uniform_num\"\n",
    "        )\n",
    "\n",
    "        # Find the element-wise difference between images.\n",
    "        image_difference = fake_images - real_images\n",
    "\n",
    "        # Get random samples from this mixed image distribution.\n",
    "        mixed_images = random_uniform_num * image_difference\n",
    "        mixed_images += real_images\n",
    "\n",
    "        # Get loss from interpolated mixed images and watch for gradients.\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            # Watch interpolated mixed images.\n",
    "            gp_tape.watch(tensor=mixed_images)\n",
    "\n",
    "            # Send to the discriminator to get logits.\n",
    "            mixed_logits = self.model(inputs=mixed_images, training=True)\n",
    "\n",
    "            # Get the mixed loss.\n",
    "            mixed_loss = tf.reduce_sum(\n",
    "                input_tensor=mixed_logits,\n",
    "                name=\"gp_mixed_loss\"\n",
    "            )\n",
    "\n",
    "        # Get gradient from returned list of length 1.\n",
    "        mixed_gradients = gp_tape.gradient(\n",
    "            target=mixed_loss, sources=[mixed_images]\n",
    "        )[0]\n",
    "\n",
    "        # Get gradient's L2 norm.\n",
    "        mixed_norms = tf.sqrt(\n",
    "            x=tf.reduce_sum(\n",
    "                input_tensor=tf.square(\n",
    "                    x=mixed_gradients,\n",
    "                    name=\"gp_squared_grads\"\n",
    "                ),\n",
    "                axis=[1, 2, 3]\n",
    "            ) + 1e-8\n",
    "        )\n",
    "\n",
    "        # Get squared difference from target of 1.0.\n",
    "        squared_difference = tf.square(\n",
    "            x=mixed_norms - 1.0, name=\"gp_squared_difference\"\n",
    "        )\n",
    "\n",
    "        # Get gradient penalty scalar.\n",
    "        gradient_penalty = tf.reduce_mean(\n",
    "            input_tensor=squared_difference, name=\"gp_gradient_penalty\"\n",
    "        )\n",
    "\n",
    "        # Multiply with lambda to get gradient penalty loss.\n",
    "        gradient_penalty_loss = tf.multiply(\n",
    "            x=self.params[\"discriminator_gradient_penalty_coefficient\"],\n",
    "            y=gradient_penalty,\n",
    "            name=\"gp_gradient_penalty_loss\"\n",
    "        )\n",
    "\n",
    "        return gradient_penalty_loss\n",
    "\n",
    "    def get_discriminator_loss(\n",
    "        self,\n",
    "        global_batch_size,\n",
    "        fake_images,\n",
    "        real_images,\n",
    "        fake_logits,\n",
    "        real_logits,\n",
    "        global_step,\n",
    "        summary_file_writer\n",
    "    ):\n",
    "        \"\"\"Gets discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            global_batch_size: int, global batch size for distribution.\n",
    "            fake_images: tensor, images generated by the generator from random\n",
    "                noise of shape [batch_size, image_size, image_size, 3].\n",
    "            real_images: tensor, real images from input of shape\n",
    "                [batch_size, image_height, image_width, 3].\n",
    "            fake_logits: tensor, output of discriminator using fake images\n",
    "                with shape [batch_size, 1].\n",
    "            real_logits: tensor, output of discriminator using real images\n",
    "                with shape [batch_size, 1].\n",
    "            global_step: int, current global step for training.\n",
    "            summary_file_writer: summary file writer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of discriminator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        if self.params[\"distribution_strategy\"]:\n",
    "            # Calculate base discriminator loss.\n",
    "            discriminator_fake_loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss=fake_logits,\n",
    "                global_batch_size=global_batch_size\n",
    "            )\n",
    "\n",
    "            discriminator_real_loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss=real_logits,\n",
    "                global_batch_size=global_batch_size\n",
    "            )\n",
    "        else:\n",
    "            # Calculate base discriminator loss.\n",
    "            discriminator_fake_loss = tf.reduce_mean(\n",
    "                input_tensor=fake_logits,\n",
    "                name=\"{}_fake_loss\".format(self.name)\n",
    "            )\n",
    "\n",
    "            discriminator_real_loss = tf.reduce_mean(\n",
    "                input_tensor=real_logits,\n",
    "                name=\"{}_real_loss\".format(self.name)\n",
    "            )\n",
    "\n",
    "        discriminator_loss = tf.subtract(\n",
    "            x=discriminator_fake_loss,\n",
    "            y=discriminator_real_loss,\n",
    "            name=\"{}_loss\".format(self.name)\n",
    "        )\n",
    "\n",
    "        # Get discriminator gradient penalty loss.\n",
    "        discriminator_gradient_penalty = self.get_gradient_penalty_loss(\n",
    "            fake_images=fake_images, real_images=real_images\n",
    "        )\n",
    "\n",
    "        # Get discriminator epsilon drift penalty.\n",
    "        epsilon_drift_penalty = tf.multiply(\n",
    "            x=self.params[\"discriminator_epsilon_drift\"],\n",
    "            y=tf.reduce_mean(input_tensor=tf.square(x=real_logits)),\n",
    "            name=\"epsilon_drift_penalty\"\n",
    "        )\n",
    "\n",
    "        # Get discriminator Wasserstein GP loss.\n",
    "        discriminator_wasserstein_gp_loss = tf.add_n(\n",
    "            inputs=[\n",
    "                discriminator_loss,\n",
    "                discriminator_gradient_penalty,\n",
    "                epsilon_drift_penalty\n",
    "            ],\n",
    "            name=\"{}_wasserstein_gp_loss\".format(self.name)\n",
    "        )\n",
    "\n",
    "        if self.params[\"distribution_strategy\"]:\n",
    "            # Get regularization losses.\n",
    "            discriminator_reg_loss = tf.nn.scale_regularization_loss(\n",
    "                regularization_loss=sum(self.model.losses)\n",
    "            )\n",
    "        else:\n",
    "            # Get regularization losses.\n",
    "            discriminator_reg_loss = sum(self.model.losses)\n",
    "\n",
    "        # Combine losses for total loss.\n",
    "        discriminator_total_loss = tf.math.add(\n",
    "            x=discriminator_wasserstein_gp_loss,\n",
    "            y=discriminator_reg_loss,\n",
    "            name=\"discriminator_total_loss\"\n",
    "        )\n",
    "\n",
    "        if self.params[\"write_summaries\"]:\n",
    "            # Add summaries for TensorBoard.\n",
    "            with summary_file_writer.as_default():\n",
    "                with tf.summary.record_if(\n",
    "                    condition=tf.equal(\n",
    "                        x=tf.math.floormod(\n",
    "                            x=global_step,\n",
    "                            y=self.params[\"save_summary_steps\"]\n",
    "                        ), y=0\n",
    "                    )\n",
    "                ):\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/discriminator_real_loss\",\n",
    "                        data=discriminator_real_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/discriminator_fake_loss\",\n",
    "                        data=discriminator_fake_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/discriminator_loss\",\n",
    "                        data=discriminator_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/discriminator_gradient_penalty\",\n",
    "                        data=discriminator_gradient_penalty,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/epsilon_drift_penalty\",\n",
    "                        data=epsilon_drift_penalty,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/discriminator_wasserstein_gp_loss\",\n",
    "                        data=discriminator_wasserstein_gp_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"losses/discriminator_reg_loss\",\n",
    "                        data=discriminator_reg_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        name=\"optimized_losses/discriminator_total_loss\",\n",
    "                        data=discriminator_total_loss,\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    summary_file_writer.flush()\n",
    "\n",
    "        return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/train_and_eval.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TrainAndEval(object):\n",
    "    \"\"\"Class that contains methods used for both training and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `TrainAndEval`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def generator_loss_phase(self, mode, training):\n",
    "        \"\"\"Gets fake logits and loss for generator.\n",
    "\n",
    "        Args:\n",
    "            mode: str, what mode currently in: TRAIN or EVAL.\n",
    "            training: bool, if model should be training.\n",
    "\n",
    "        Returns:\n",
    "            Fake images tensor of shape\n",
    "                [batch_size, iamge_height, image_width, image_depth], fake\n",
    "                logits tensor of shape [batch_size, 1], and generator loss\n",
    "                tensor of shape [].\n",
    "        \"\"\"\n",
    "        batch_size = (\n",
    "            self.params[\"train_batch_size\"]\n",
    "            if mode == \"TRAIN\"\n",
    "            else self.params[\"eval_batch_size\"]\n",
    "        )\n",
    "\n",
    "        # Create random noise latent vector for each batch example.\n",
    "        Z = tf.random.normal(\n",
    "            shape=[batch_size, self.params[\"generator_latent_size\"]],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Get generated image from generator network from gaussian noise.\n",
    "        fake_images = self.network_models[\"generator\"](\n",
    "            inputs=Z, training=training\n",
    "        )\n",
    "\n",
    "        if self.params[\"write_summaries\"] and mode == \"TRAIN\":\n",
    "            # Add summaries for TensorBoard.\n",
    "            with self.summary_file_writer.as_default():\n",
    "                with tf.summary.record_if(\n",
    "                condition=tf.equal(\n",
    "                    x=tf.math.floormod(\n",
    "                        x=self.global_step,\n",
    "                        y=self.params[\"save_summary_steps\"]\n",
    "                    ), y=0\n",
    "                )\n",
    "                ):\n",
    "                    tf.summary.image(\n",
    "                        name=\"fake_images\",\n",
    "                        data=fake_images,\n",
    "                        step=self.global_step,\n",
    "                        max_outputs=5\n",
    "                    )\n",
    "                    self.summary_file_writer.flush()\n",
    "\n",
    "        # Get fake logits from discriminator using generator's output image.\n",
    "        fake_logits = self.network_models[\"discriminator\"](\n",
    "            inputs=fake_images, training=training\n",
    "        )\n",
    "\n",
    "        # Get generator total loss.\n",
    "        generator_total_loss = (\n",
    "            self.network_objects[\"generator\"].get_generator_loss(\n",
    "                global_batch_size=self.global_batch_size,\n",
    "                fake_logits=fake_logits,\n",
    "                global_step=self.global_step,\n",
    "                summary_file_writer=self.summary_file_writer\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return fake_images, fake_logits, generator_total_loss\n",
    "\n",
    "    def discriminator_loss_phase(\n",
    "        self, fake_images, real_images, fake_logits, training\n",
    "    ):\n",
    "        \"\"\"Gets real logits and loss for discriminator.\n",
    "\n",
    "        Args:\n",
    "            fake_images: tensor, images generated by the generator from random\n",
    "                noise of shape [batch_size, image_size, image_size, 3].\n",
    "            real_images: tensor, real images from input of shape\n",
    "                [batch_size, image_height, image_width, 3].\n",
    "            fake_logits: tensor, output of discriminator using fake images\n",
    "                with shape [batch_size, 1].\n",
    "            training: bool, if in training mode.\n",
    "\n",
    "        Returns:\n",
    "            Real logits of shape [batch_size, 1] and discriminator loss of\n",
    "                shape [].\n",
    "        \"\"\"\n",
    "        # Get real logits from discriminator using real image.\n",
    "        real_logits = self.network_models[\"discriminator\"](\n",
    "            inputs=real_images, training=training\n",
    "        )\n",
    "\n",
    "        # Get discriminator total loss.\n",
    "        discriminator_total_loss = (\n",
    "            self.network_objects[\"discriminator\"].get_discriminator_loss(\n",
    "                global_batch_size=self.global_batch_size,\n",
    "                fake_images=fake_images,\n",
    "                real_images=real_images,\n",
    "                fake_logits=fake_logits,\n",
    "                real_logits=real_logits,\n",
    "                global_step=self.global_step,\n",
    "                summary_file_writer=self.summary_file_writer\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return real_logits, discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/train.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Train(object):\n",
    "    \"\"\"Class that contains methods used for only training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `Train`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_variables_and_gradients(self, loss, gradient_tape, scope):\n",
    "        \"\"\"Gets variables and gradients from model wrt. loss.\n",
    "\n",
    "        Args:\n",
    "            loss: tensor, shape of [].\n",
    "            gradient_tape: instance of `GradientTape`.\n",
    "            scope: str, the name of the network of interest.\n",
    "\n",
    "        Returns:\n",
    "            Lists of network's variables and gradients.\n",
    "        \"\"\"\n",
    "        # Get trainable variables.\n",
    "        variables = self.network_models[scope].trainable_variables\n",
    "\n",
    "        # Get gradients from gradient tape.\n",
    "        gradients = gradient_tape.gradient(\n",
    "            target=loss, sources=variables\n",
    "        )\n",
    "\n",
    "        # Clip gradients.\n",
    "        if self.params[\"{}_clip_gradients\".format(scope)]:\n",
    "            gradients, _ = tf.clip_by_global_norm(\n",
    "                t_list=gradients,\n",
    "                clip_norm=self.params[\"{}_clip_gradients\".format(scope)],\n",
    "                name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "            )\n",
    "\n",
    "        # Add variable names back in for identification.\n",
    "        gradients = [\n",
    "            tf.identity(\n",
    "                input=g,\n",
    "                name=\"{}_{}_gradients\".format(scope, v.name[:-2])\n",
    "            )\n",
    "            if tf.is_tensor(x=g) else g\n",
    "            for g, v in zip(gradients, variables)\n",
    "        ]\n",
    "\n",
    "        return variables, gradients\n",
    "\n",
    "    def create_variable_and_gradient_histogram_summaries(\n",
    "        self, variables, gradients, scope\n",
    "    ):\n",
    "        \"\"\"Creates variable and gradient histogram summaries.\n",
    "\n",
    "        Args:\n",
    "            variables: list, network's trainable variables.\n",
    "            gradients: list, gradients of network's trainable variables wrt.\n",
    "                loss.\n",
    "            scope: str, the name of the network of interest.\n",
    "        \"\"\"\n",
    "        if self.params[\"write_summaries\"]:\n",
    "            # Add summaries for TensorBoard.\n",
    "            with self.summary_file_writer.as_default():\n",
    "                with tf.summary.record_if(\n",
    "                    condition=tf.equal(\n",
    "                        x=tf.math.floormod(\n",
    "                            x=self.global_step,\n",
    "                            y=self.params[\"save_summary_steps\"]\n",
    "                        ), y=0\n",
    "                    )\n",
    "                ):\n",
    "                    for v, g in zip(variables, gradients):\n",
    "                        tf.summary.histogram(\n",
    "                            name=\"{}_variables/{}\".format(\n",
    "                                scope, v.name[:-2]\n",
    "                            ),\n",
    "                            data=v,\n",
    "                            step=self.global_step\n",
    "                        )\n",
    "                        if tf.is_tensor(x=g):\n",
    "                            tf.summary.histogram(\n",
    "                                name=\"{}_gradients/{}\".format(\n",
    "                                    scope, v.name[:-2]\n",
    "                                ),\n",
    "                                data=g,\n",
    "                                step=self.global_step\n",
    "                            )\n",
    "                    self.summary_file_writer.flush()\n",
    "\n",
    "    def get_select_loss_variables_and_gradients(self, real_images, scope):\n",
    "        \"\"\"Gets selected network's loss, variables, and gradients.\n",
    "\n",
    "        Args:\n",
    "            real_images: tensor, real images of shape\n",
    "                [batch_size, height * width * depth].\n",
    "            scope: str, the name of the network of interest.\n",
    "\n",
    "        Returns:\n",
    "            Selected network's loss, variables, and gradients.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            # Get fake logits from generator.\n",
    "            (fake_images,\n",
    "             fake_logits,\n",
    "             generator_loss) = self.generator_loss_phase(\n",
    "                mode=\"TRAIN\", training=True\n",
    "            )\n",
    "\n",
    "            # Get discriminator loss.\n",
    "            _, discriminator_loss = self.discriminator_loss_phase(\n",
    "                fake_images, real_images, fake_logits, training=True\n",
    "            )\n",
    "\n",
    "        # Create empty dicts to hold loss, variables, gradients.\n",
    "        loss_dict = {}\n",
    "        vars_dict = {}\n",
    "        grads_dict = {}\n",
    "\n",
    "        # Loop over generator and discriminator.\n",
    "        for (loss, gradient_tape, scope_name) in zip(\n",
    "            [generator_loss, discriminator_loss],\n",
    "            [gen_tape, dis_tape],\n",
    "            [\"generator\", \"discriminator\"]\n",
    "        ):\n",
    "            # Get variables and gradients from generator wrt. loss.\n",
    "            variables, gradients = self.get_variables_and_gradients(\n",
    "                loss, gradient_tape, scope_name\n",
    "            )\n",
    "\n",
    "            # Add loss, variables, and gradients to dictionaries.\n",
    "            loss_dict[scope_name] = loss\n",
    "            vars_dict[scope_name] = variables\n",
    "            grads_dict[scope_name] = gradients\n",
    "\n",
    "            # Create variable and gradient histogram summaries.\n",
    "            self.create_variable_and_gradient_histogram_summaries(\n",
    "                variables, gradients, scope_name\n",
    "            )\n",
    "\n",
    "        return loss_dict[scope], vars_dict[scope], grads_dict[scope]\n",
    "\n",
    "    def train_network(self, variables, gradients, scope):\n",
    "        \"\"\"Trains network variables using gradients with optimizer.\n",
    "\n",
    "        Args:\n",
    "            variables: list, network's trainable variables.\n",
    "            gradients: list, gradients of network's trainable variables wrt.\n",
    "                loss.\n",
    "            scope: str, the name of the network of interest.\n",
    "        \"\"\"\n",
    "        # Zip together gradients and variables.\n",
    "        grads_and_vars = zip(gradients, variables)\n",
    "\n",
    "        # Applying gradients to variables using optimizer.\n",
    "        self.optimizers[scope].apply_gradients(grads_and_vars=grads_and_vars)\n",
    "\n",
    "    def resize_real_images(self, images):\n",
    "        \"\"\"Resizes real images to match the GAN's current size.\n",
    "\n",
    "        Args:\n",
    "            images: tensor, original images.\n",
    "\n",
    "        Returns:\n",
    "            Resized image tensor.\n",
    "        \"\"\"\n",
    "        block_idx = (self.growth_idx + 1) // 2\n",
    "        height, width = self.params[\"generator_projection_dims\"][0:2]\n",
    "        resized_image = tf.image.resize(\n",
    "            images=images,\n",
    "            size=[\n",
    "                height * (2 ** block_idx), width * (2 ** block_idx)\n",
    "            ],\n",
    "            method=\"nearest\",\n",
    "            name=\"resized_image_{}\".format(self.growth_idx)\n",
    "        )\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "    def train_discriminator(self, features):\n",
    "        \"\"\"Trains discriminator network.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Discriminator loss tensor.\n",
    "        \"\"\"\n",
    "        # Extract real images from features dictionary.\n",
    "        real_images = self.resize_real_images(images=features[\"image\"])\n",
    "\n",
    "        # Get gradients for training by running inputs through networks.\n",
    "        loss, variables, gradients = (\n",
    "            self.get_select_loss_variables_and_gradients(\n",
    "                real_images, scope=\"discriminator\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Train discriminator network.\n",
    "        self.train_network(variables, gradients, scope=\"discriminator\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_generator(self, features):\n",
    "        \"\"\"Trains generator network.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Generator loss tensor.\n",
    "        \"\"\"\n",
    "        # Extract real images from features dictionary.\n",
    "        real_images = self.resize_real_images(images=features[\"image\"])\n",
    "\n",
    "        # Get gradients for training by running inputs through networks.\n",
    "        loss, variables, gradients = (\n",
    "            self.get_select_loss_variables_and_gradients(\n",
    "                real_images, scope=\"generator\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Train generator network.\n",
    "        self.train_network(variables, gradients, scope=\"generator\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def create_checkpoint_machinery(self):\n",
    "        \"\"\"Creates checkpoint machinery needed to save & restore checkpoints.\n",
    "        \"\"\"\n",
    "        # Create checkpoint instance.\n",
    "        checkpoint_dir = os.path.join(\n",
    "            self.params[\"output_dir\"], \"checkpoints\"\n",
    "        )\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "        max_growth_idx = (len(self.params[\"conv_num_filters\"]) - 1) * 2\n",
    "        image_multiplier = 2 ** ((max_growth_idx + 1) // 2)\n",
    "        height, width = self.params[\"generator_projection_dims\"][0:2]\n",
    "\n",
    "        checkpoint = tf.train.Checkpoint(\n",
    "            generator_model=self.network_objects[\"generator\"].get_model(\n",
    "                input_shape=(self.params[\"generator_latent_size\"]),\n",
    "                batch_size=1,\n",
    "                growth_idx=max_growth_idx\n",
    "            ),\n",
    "            discriminator_model=(\n",
    "                self.network_objects[\"discriminator\"].get_model(\n",
    "                    input_shape=(\n",
    "                        height * image_multiplier,\n",
    "                        width * image_multiplier,\n",
    "                        self.params[\"depth\"]\n",
    "                    ),\n",
    "                    batch_size=1,\n",
    "                    growth_idx=max_growth_idx\n",
    "                )\n",
    "            ),\n",
    "            generator_optimizer=self.optimizers[\"generator\"],\n",
    "            discriminator_optimizer=self.optimizers[\"discriminator\"]\n",
    "        )\n",
    "\n",
    "        # Create checkpoint manager.\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=checkpoint,\n",
    "            directory=checkpoint_dir,\n",
    "            max_to_keep=self.params[\"keep_checkpoint_max\"],\n",
    "            step_counter=self.global_step,\n",
    "            checkpoint_interval=self.params[\"save_checkpoints_steps\"]\n",
    "        )\n",
    "\n",
    "        # Restore any prior checkpoints.\n",
    "        status = checkpoint.restore(\n",
    "            save_path=self.checkpoint_manager.latest_checkpoint\n",
    "        )\n",
    "\n",
    "    def prepare_training_components(self):\n",
    "        \"\"\"Prepares all components necessary for training.\n",
    "        \"\"\"\n",
    "        # Instantiate model objects.\n",
    "        self.instantiate_model_objects()\n",
    "\n",
    "        # Create checkpoint machinery to save/restore checkpoints.\n",
    "        self.create_checkpoint_machinery()\n",
    "\n",
    "        # Create summary file writer.\n",
    "        self.summary_file_writer = tf.summary.create_file_writer(\n",
    "            logdir=os.path.join(self.params[\"output_dir\"], \"summaries\"),\n",
    "            name=\"summary_file_writer\"\n",
    "        )\n",
    "\n",
    "    def set_active_network_models(self):\n",
    "        \"\"\"Sets active network models for current growth phase.\n",
    "        \"\"\"\n",
    "        self.network_models[\"generator\"] = (\n",
    "            self.network_objects[\"generator\"].get_model(\n",
    "                input_shape=(self.params[\"generator_latent_size\"]),\n",
    "                batch_size=self.params[\"train_batch_size\"],\n",
    "                growth_idx=self.growth_idx\n",
    "            )\n",
    "        )\n",
    "\n",
    "        image_multiplier = 2 ** ((self.growth_idx + 1) // 2)\n",
    "        height = (\n",
    "            self.params[\"generator_projection_dims\"][0] * image_multiplier\n",
    "        )\n",
    "        width = (\n",
    "            self.params[\"generator_projection_dims\"][1] * image_multiplier\n",
    "        )\n",
    "        self.network_models[\"discriminator\"] = (\n",
    "            self.network_objects[\"discriminator\"].get_model(\n",
    "                    input_shape=(height, width, self.params[\"depth\"]\n",
    "                ),\n",
    "                batch_size=self.params[\"train_batch_size\"],\n",
    "                growth_idx=self.growth_idx\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instantiate_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/instantiate_model.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import discriminators\n",
    "from . import generators\n",
    "\n",
    "\n",
    "class InstantiateModel(object):\n",
    "    \"\"\"Class that contains methods used for instantiating model objects.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `InstantiateModel`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def instantiate_network_objects(self):\n",
    "        \"\"\"Instantiates generator and discriminator with parameters.\n",
    "        \"\"\"\n",
    "        # Instantiate generator.\n",
    "        self.network_objects[\"generator\"] = generators.Generator(\n",
    "            kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                l1=self.params[\"generator_l1_regularization_scale\"],\n",
    "                l2=self.params[\"generator_l2_regularization_scale\"]\n",
    "            ),\n",
    "            bias_regularizer=None,\n",
    "            name=\"generator\",\n",
    "            params=self.params,\n",
    "            alpha_var=self.alpha_var\n",
    "        )\n",
    "\n",
    "        # Instantiate discriminator.\n",
    "        self.network_objects[\"discriminator\"] = discriminators.Discriminator(\n",
    "            kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                l1=self.params[\"discriminator_l1_regularization_scale\"],\n",
    "                l2=self.params[\"discriminator_l2_regularization_scale\"]\n",
    "            ),\n",
    "            bias_regularizer=None,\n",
    "            name=\"discriminator\",\n",
    "            params=self.params,\n",
    "            alpha_var=self.alpha_var\n",
    "        )\n",
    "\n",
    "    def instantiate_optimizer(self, scope):\n",
    "        \"\"\"Instantiates optimizer with parameters.\n",
    "\n",
    "        Args:\n",
    "            scope: str, the name of the network of interest.\n",
    "        \"\"\"\n",
    "        # Create optimizer map.\n",
    "        optimizers = {\n",
    "            \"Adadelta\": tf.keras.optimizers.Adadelta,\n",
    "            \"Adagrad\": tf.keras.optimizers.Adagrad,\n",
    "            \"Adam\": tf.keras.optimizers.Adam,\n",
    "            \"Adamax\": tf.keras.optimizers.Adamax,\n",
    "            \"Ftrl\": tf.keras.optimizers.Ftrl,\n",
    "            \"Nadam\": tf.keras.optimizers.Nadam,\n",
    "            \"RMSprop\": tf.keras.optimizers.RMSprop,\n",
    "            \"SGD\": tf.keras.optimizers.SGD\n",
    "        }\n",
    "\n",
    "        # Get optimizer and instantiate it.\n",
    "        if self.params[\"{}_optimizer\".format(scope)] == \"Adam\":\n",
    "            optimizer = optimizers[self.params[\"{}_optimizer\".format(scope)]](\n",
    "                learning_rate=self.params[\"{}_learning_rate\".format(scope)],\n",
    "                beta_1=self.params[\"{}_adam_beta1\".format(scope)],\n",
    "                beta_2=self.params[\"{}_adam_beta2\".format(scope)],\n",
    "                epsilon=self.params[\"{}_adam_epsilon\".format(scope)],\n",
    "                name=\"{}_{}_optimizer\".format(\n",
    "                    scope, self.params[\"{}_optimizer\".format(scope)].lower()\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            optimizer = optimizers[self.params[\"{}_optimizer\".format(scope)]](\n",
    "                learning_rate=self.params[\"{}_learning_rate\".format(scope)],\n",
    "                name=\"{}_{}_optimizer\".format(\n",
    "                    scope, self.params[\"{}_optimizer\".format(scope)].lower()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.optimizers[scope] = optimizer\n",
    "\n",
    "    def instantiate_model_objects(self):\n",
    "        \"\"\"Instantiate model network objects, network models, and optimizers.\n",
    "        \"\"\"\n",
    "        # Instantiate generator and discriminator objects.\n",
    "        self.instantiate_network_objects()\n",
    "\n",
    "        # Instantiate generator optimizer.\n",
    "        self.instantiate_optimizer(scope=\"generator\")\n",
    "\n",
    "        # Instantiate discriminator optimizer.\n",
    "        self.instantiate_optimizer(scope=\"discriminator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_step.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/train_step.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TrainStep(object):\n",
    "    \"\"\"Class that contains methods concerning train steps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `TrainStep`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def distributed_eager_discriminator_train_step(self, features):\n",
    "        \"\"\"Perform one distributed, eager discriminator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        if self.params[\"tf_version\"] > 2.1:\n",
    "            run_function = self.strategy.run\n",
    "        else:\n",
    "            run_function = self.strategy.experimental_run_v2\n",
    "\n",
    "        per_replica_losses = run_function(\n",
    "            fn=self.train_discriminator, kwargs={\"features\": features}\n",
    "        )\n",
    "\n",
    "        return self.strategy.reduce(\n",
    "            reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "            value=per_replica_losses,\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "    def non_distributed_eager_discriminator_train_step(self, features):\n",
    "        \"\"\"Perform one non-distributed, eager discriminator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        return self.train_discriminator(features=features)\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_graph_discriminator_train_step(self, features):\n",
    "        \"\"\"Perform one distributed, graph discriminator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        if self.params[\"tf_version\"] > 2.1:\n",
    "            run_function = self.strategy.run\n",
    "        else:\n",
    "            run_function = self.strategy.experimental_run_v2\n",
    "\n",
    "        per_replica_losses = run_function(\n",
    "            fn=self.train_discriminator, kwargs={\"features\": features}\n",
    "        )\n",
    "\n",
    "        return self.strategy.reduce(\n",
    "            reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "            value=per_replica_losses,\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def non_distributed_graph_discriminator_train_step(self, features):\n",
    "        \"\"\"Perform one non-distributed, graph discriminator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        return self.train_discriminator(features=features)\n",
    "\n",
    "    def distributed_eager_generator_train_step(self, features):\n",
    "        \"\"\"Perform one distributed, eager generator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        if self.params[\"tf_version\"] > 2.1:\n",
    "            run_function = self.strategy.run\n",
    "        else:\n",
    "            run_function = self.strategy.experimental_run_v2\n",
    "\n",
    "        per_replica_losses = run_function(\n",
    "            fn=self.train_generator, kwargs={\"features\": features}\n",
    "        )\n",
    "\n",
    "        return self.strategy.reduce(\n",
    "            reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "            value=per_replica_losses,\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "    def non_distributed_eager_generator_train_step(self, features):\n",
    "        \"\"\"Perform one non-distributed, eager generator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        return self.train_generator(features=features)\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_graph_generator_train_step(self, features):\n",
    "        \"\"\"Perform one distributed, graph generator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        if self.params[\"tf_version\"] > 2.1:\n",
    "            run_function = self.strategy.run\n",
    "        else:\n",
    "            run_function = self.strategy.experimental_run_v2\n",
    "\n",
    "        per_replica_losses = run_function(\n",
    "            fn=self.train_generator, kwargs={\"features\": features}\n",
    "        )\n",
    "\n",
    "        return self.strategy.reduce(\n",
    "            reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "            value=per_replica_losses,\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def non_distributed_graph_generator_train_step(self, features):\n",
    "        \"\"\"Perform one non-distributed, graph generator train step.\n",
    "\n",
    "        Args:\n",
    "            features: dict, feature tensors from input function.\n",
    "\n",
    "        Returns:\n",
    "            Reduced loss tensor for chosen network across replicas.\n",
    "        \"\"\"\n",
    "        return self.train_generator(features=features)\n",
    "\n",
    "    def get_train_step_functions(self):\n",
    "        \"\"\"Gets network model train step functions for strategy and mode.\n",
    "        \"\"\"\n",
    "        if self.strategy:\n",
    "            if self.params[\"use_graph_mode\"]:\n",
    "                self.discriminator_train_step_fn = (\n",
    "                    self.distributed_graph_discriminator_train_step\n",
    "                )\n",
    "                self.generator_train_step_fn = (\n",
    "                    self.distributed_graph_generator_train_step\n",
    "                )\n",
    "            else:\n",
    "                self.discriminator_train_step_fn = (\n",
    "                    self.distributed_eager_discriminator_train_step\n",
    "                )\n",
    "                self.generator_train_step_fn = (\n",
    "                    self.distributed_eager_generator_train_step\n",
    "                )\n",
    "        else:\n",
    "            if self.params[\"use_graph_mode\"]:\n",
    "                self.discriminator_train_step_fn = (\n",
    "                    self.non_distributed_graph_discriminator_train_step\n",
    "                )\n",
    "                self.generator_train_step_fn = (\n",
    "                    self.non_distributed_graph_generator_train_step\n",
    "                )\n",
    "            else:\n",
    "                self.discriminator_train_step_fn = (\n",
    "                    self.non_distributed_eager_discriminator_train_step\n",
    "                )\n",
    "                self.generator_train_step_fn = (\n",
    "                    self.non_distributed_eager_generator_train_step\n",
    "                )\n",
    "\n",
    "    def log_step_loss(self, epoch, loss):\n",
    "        \"\"\"Logs step information and loss.\n",
    "\n",
    "        Args:\n",
    "            epoch: int, current iteration fully through the dataset.\n",
    "            loss: float, the loss of the model at the current step.\n",
    "        \"\"\"\n",
    "        if self.global_step % self.params[\"log_step_count_steps\"] == 0:\n",
    "            start_time = self.previous_timestamp\n",
    "            self.previous_timestamp = tf.timestamp()\n",
    "            elapsed_time = self.previous_timestamp - start_time\n",
    "            print(\n",
    "                \"{} = {}, {} = {}, {} = {}, {} = {}, {} = {}\".format(\n",
    "                    \"epoch\",\n",
    "                    epoch,\n",
    "                    \"global_step\",\n",
    "                    self.global_step.numpy(),\n",
    "                    \"epoch_step\",\n",
    "                    self.epoch_step,\n",
    "                    \"steps/sec\",\n",
    "                    float(self.params[\"log_step_count_steps\"]) / elapsed_time,\n",
    "                    \"loss\",\n",
    "                    loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @tf.function\n",
    "    def increment_global_step(self):\n",
    "        \"\"\"Increments global step variable.\n",
    "        \"\"\"\n",
    "        self.global_step.assign_add(\n",
    "            delta=tf.ones(shape=[], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def increment_alpha_var(self):\n",
    "        \"\"\"Increments alpha variable through range [0., 1.] during transition.\n",
    "        \"\"\"\n",
    "        self.alpha_var.assign(\n",
    "            value=tf.divide(\n",
    "                x=tf.cast(\n",
    "                    x=tf.math.floormod(\n",
    "                        x=self.global_step,\n",
    "                        y=self.params[\"num_steps_until_growth\"]\n",
    "                    ),\n",
    "                    dtype=tf.float32\n",
    "                ),\n",
    "                y=self.params[\"num_steps_until_growth\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def network_model_training_steps(\n",
    "        self,\n",
    "        epoch,\n",
    "        train_step_fn,\n",
    "        train_steps,\n",
    "        train_dataset_iter,\n",
    "        features,\n",
    "        labels\n",
    "    ):\n",
    "        \"\"\"Trains a network model for so many steps given a set of features.\n",
    "\n",
    "        Args:\n",
    "            epoch: int, the current iteration through the dataset.\n",
    "            train_step_fn: unbound function, trains the given network model\n",
    "                given a set of features.\n",
    "            train_steps: int, number of steps to train network model.\n",
    "            train_dataset_iter: iterator, training dataset iterator.\n",
    "            features: dict, feature tensors from input function.\n",
    "            labels: tensor, label tensor from input function.\n",
    "\n",
    "        Returns:\n",
    "            Bool that indicates if current growth phase complete,\n",
    "                dictionary of most recent feature tensors, and most recent\n",
    "                label tensor.\n",
    "        \"\"\"\n",
    "        for _ in range(train_steps):\n",
    "            if features is None:\n",
    "                # Train model on batch of features and get loss.\n",
    "                features, labels = next(train_dataset_iter)\n",
    "\n",
    "            loss = train_step_fn(features=features)\n",
    "\n",
    "            # Log step information and loss.\n",
    "            self.log_step_loss(epoch, loss)\n",
    "\n",
    "            # Checkpoint model every save_checkpoints_steps steps.\n",
    "            self.checkpoint_manager.save(\n",
    "                checkpoint_number=self.global_step, check_interval=True\n",
    "            )\n",
    "\n",
    "            # Increment steps.\n",
    "            self.increment_global_step()\n",
    "            self.epoch_step += 1\n",
    "\n",
    "            # If this is a growth transition phase.\n",
    "            if self.growth_idx % 2 == 1:\n",
    "                # Increment alpha variable.\n",
    "                self.increment_alpha_var()\n",
    "\n",
    "            if self.global_step % self.params[\"num_steps_until_growth\"] == 0:\n",
    "                return True, features, labels\n",
    "        return False, features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/training_loop.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TrainingLoop(object):\n",
    "    \"\"\"Class that contains methods for training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `TrainStep`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def training_loop(self, steps_per_epoch, train_dataset_iter):\n",
    "        \"\"\"Loops through training dataset to train model.\n",
    "\n",
    "        Args:\n",
    "            steps_per_epoch: int, number of steps/batches to take each epoch.\n",
    "            train_dataset_iter: iterator, training dataset iterator.\n",
    "        \"\"\"\n",
    "        # Get correct train function based on parameters.\n",
    "        self.get_train_step_functions()\n",
    "\n",
    "        # Calculate number of growths. Each progression involves 2 growths,\n",
    "        # a transition phase and stablization phase.\n",
    "        num_growths = len(self.params[\"conv_num_filters\"]) * 2 - 1\n",
    "\n",
    "        for self.growth_idx in range(num_growths):\n",
    "            print(\"\\ngrowth_idx = {}\".format(self.growth_idx))\n",
    "\n",
    "            # Set active generator and discriminator `Model`s.\n",
    "            self.set_active_network_models()\n",
    "\n",
    "            for epoch in range(self.params[\"num_epochs\"]):\n",
    "                self.previous_timestamp = tf.timestamp()\n",
    "\n",
    "                self.epoch_step = 0\n",
    "                while self.epoch_step < steps_per_epoch:\n",
    "                    # Train discriminator.\n",
    "                    (growth_phase_complete,\n",
    "                     features,\n",
    "                     labels) = self.network_model_training_steps(\n",
    "                        epoch=epoch,\n",
    "                        train_step_fn=self.discriminator_train_step_fn,\n",
    "                        train_steps=self.params[\"discriminator_train_steps\"],\n",
    "                        train_dataset_iter=train_dataset_iter,\n",
    "                        features=None,\n",
    "                        labels=None\n",
    "                    )\n",
    "\n",
    "                    if growth_phase_complete:\n",
    "                        break  # break while loop\n",
    "\n",
    "                    # Train generator.\n",
    "                    (growth_phase_complete,\n",
    "                     _,\n",
    "                     _) = self.network_model_training_steps(\n",
    "                        epoch=epoch,\n",
    "                        train_step_fn=self.generator_train_step_fn,\n",
    "                        train_steps=self.params[\"generator_train_steps\"],\n",
    "                        train_dataset_iter=None,\n",
    "                        features=features,\n",
    "                        labels=labels\n",
    "                    )\n",
    "\n",
    "                    if growth_phase_complete:\n",
    "                        break  # break while loop\n",
    "\n",
    "                if growth_phase_complete:\n",
    "                    break  # break epoch for loop\n",
    "\n",
    "            if self.params[\"export_every_growth_phase\"]:\n",
    "                self.export_saved_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/export.py\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Export(object):\n",
    "    \"\"\"Class that contains methods used for exporting model objects.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiate instance of `Export`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def export_saved_model(self):\n",
    "        \"\"\"Exports SavedModel to output directory for serving.\n",
    "        \"\"\"\n",
    "        export_path = os.path.join(\n",
    "            self.params[\"output_dir\"],\n",
    "            \"export\",\n",
    "            datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        )\n",
    "\n",
    "        # Signature will be serving_default.\n",
    "        tf.saved_model.save(\n",
    "            obj=self.network_models[\"generator\"], export_dir=export_path\n",
    "        )\n",
    "\n",
    "    def training_loop_end_save_model(self):\n",
    "        \"\"\"Saving model when training loop ends.\n",
    "        \"\"\"\n",
    "        # Write final checkpoint.\n",
    "        self.checkpoint_manager.save(\n",
    "            checkpoint_number=self.global_step, check_interval=False\n",
    "        )\n",
    "\n",
    "        # Export SavedModel for serving.\n",
    "        self.export_saved_model()\n",
    "\n",
    "    def training_loop_end_save_model(self):\n",
    "        \"\"\"Saving model when training loop ends.\n",
    "        \"\"\"\n",
    "        # Write final checkpoint.\n",
    "        self.checkpoint_manager.save(\n",
    "            checkpoint_number=self.global_step, check_interval=False\n",
    "        )\n",
    "\n",
    "        # Export SavedModel for serving.\n",
    "        export_path = os.path.join(\n",
    "            self.params[\"output_dir\"],\n",
    "            \"export\",\n",
    "            datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        )\n",
    "\n",
    "        # Signature will be serving_default.\n",
    "        tf.saved_model.save(\n",
    "            obj=self.network_models[\"generator\"], export_dir=export_path\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/model.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import export\n",
    "from . import inputs\n",
    "from . import instantiate_model\n",
    "from . import train\n",
    "from . import train_and_eval\n",
    "from . import train_step\n",
    "from . import training_loop\n",
    "\n",
    "\n",
    "class TrainAndEvaluateModel(\n",
    "    train_and_eval.TrainAndEval,\n",
    "    train.Train,\n",
    "    instantiate_model.InstantiateModel,\n",
    "    train_step.TrainStep,\n",
    "    training_loop.TrainingLoop,\n",
    "    export.Export\n",
    "):\n",
    "    \"\"\"Train and evaluate loop trainer for model.\n",
    "\n",
    "    Attributes:\n",
    "        params: dict, user passed parameters.\n",
    "        network_objects: dict, instances of `Generator` and `Discriminator`\n",
    "            network objects.\n",
    "        network_models: dict, instances of Keras `Model`s for each network.\n",
    "        optimizers: dict, instances of Keras `Optimizer`s for each network.\n",
    "        strategy: instance of tf.distribute.strategy.\n",
    "        discriminator_train_step_fn: unbound function, function for a\n",
    "            dicriminator train step using correct strategy and mode.\n",
    "        generator_train_step_fn: unbound function, function for a\n",
    "            generator train step using correct strategy and mode.\n",
    "        global_batch_size: int, the global batch size after summing batch\n",
    "            sizes across replicas.\n",
    "        global_step: tf.Variable, the global step counter across epochs and\n",
    "            steps within epoch.\n",
    "        alpha_var: tf.Variable, used in growth transition network's weighted\n",
    "            sum.\n",
    "        summary_file_writer: instance of tf.summary.create_file_writer for\n",
    "            summaries for TensorBoard.\n",
    "        growth_idx: int, current growth index model has progressed to.\n",
    "        epoch_step: int, the current step through current epoch.\n",
    "        previous_timestamp: float, the previous timestamp for profiling the\n",
    "            steps/sec rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        \"\"\"Instantiate trainer.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "\n",
    "        self.network_objects = {}\n",
    "        self.network_models = {}\n",
    "        self.optimizers = {}\n",
    "\n",
    "        self.strategy = None\n",
    "\n",
    "        self.discriminator_train_step_fn = None\n",
    "        self.generator_train_step_fn = None\n",
    "\n",
    "        self.global_batch_size = None\n",
    "\n",
    "        self.global_step = tf.Variable(\n",
    "            initial_value=tf.zeros(shape=[], dtype=tf.int64),\n",
    "            trainable=False,\n",
    "            name=\"global_step\"\n",
    "        )\n",
    "\n",
    "        self.alpha_var = tf.Variable(\n",
    "            initial_value=tf.zeros(shape=[], dtype=tf.float32),\n",
    "            trainable=False,\n",
    "            name=\"alpha_var\"\n",
    "        )\n",
    "\n",
    "        self.summary_file_writer = None\n",
    "\n",
    "        self.growth_idx = 0\n",
    "        self.epoch_step = 0\n",
    "        self.previous_timestamp = 0.0\n",
    "\n",
    "    def get_train_eval_datasets(self, num_replicas):\n",
    "        \"\"\"Gets train and eval datasets.\n",
    "\n",
    "        Args:\n",
    "            num_replicas: int, number of device replicas.\n",
    "\n",
    "        Returns:\n",
    "            Train and eval datasets.\n",
    "        \"\"\"\n",
    "        train_dataset = inputs.read_dataset(\n",
    "            filename=self.params[\"train_file_pattern\"],\n",
    "            batch_size=self.params[\"train_batch_size\"] * num_replicas,\n",
    "            params=self.params,\n",
    "            training=True\n",
    "        )()\n",
    "\n",
    "        eval_dataset = inputs.read_dataset(\n",
    "            filename=self.params[\"eval_file_pattern\"],\n",
    "            batch_size=self.params[\"eval_batch_size\"] * num_replicas,\n",
    "            params=self.params,\n",
    "            training=False\n",
    "        )()\n",
    "        if self.params[\"eval_steps\"]:\n",
    "            eval_dataset = eval_dataset.take(count=self.params[\"eval_steps\"])\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def train_block(self, train_dataset, eval_dataset):\n",
    "        \"\"\"Training block setups training, then loops through datasets.\n",
    "\n",
    "        Args:\n",
    "            train_dataset: instance of `Dataset` for training data.\n",
    "            eval_dataset: instance of `Dataset` for evaluation data.\n",
    "        \"\"\"\n",
    "        # Create iterators of datasets.\n",
    "        train_dataset_iter = iter(train_dataset)\n",
    "        eval_dataset_iter = iter(eval_dataset)\n",
    "\n",
    "        steps_per_epoch = (\n",
    "            self.params[\"train_dataset_length\"] // self.global_batch_size\n",
    "        )\n",
    "\n",
    "        # Instantiate models, create checkpoints, create summary file writer.\n",
    "        self.prepare_training_components()\n",
    "\n",
    "        # Run training loop.\n",
    "        self.training_loop(steps_per_epoch, train_dataset_iter)\n",
    "\n",
    "        # Save model at end of training loop.\n",
    "        self.training_loop_end_save_model()\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        \"\"\"Trains and evaluates Keras model.\n",
    "\n",
    "        Args:\n",
    "            args: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generator's `Model` object for in-memory predictions.\n",
    "        \"\"\"\n",
    "        if self.params[\"distribution_strategy\"]:\n",
    "            # If the list of devices is not specified in the\n",
    "            # Strategy constructor, it will be auto-detected.\n",
    "            if self.params[\"distribution_strategy\"] == \"Mirrored\":\n",
    "                self.strategy = tf.distribute.MirroredStrategy()\n",
    "            print(\n",
    "                \"Number of devices = {}\".format(\n",
    "                    self.strategy.num_replicas_in_sync\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Set global batch size for training.\n",
    "            self.global_batch_size = (\n",
    "                self.params[\"train_batch_size\"] * self.strategy.num_replicas_in_sync\n",
    "            )\n",
    "\n",
    "            # Get input datasets. Batch size is split evenly between replicas.\n",
    "            train_dataset, eval_dataset = self.get_train_eval_datasets(\n",
    "                num_replicas=self.strategy.num_replicas_in_sync\n",
    "            )\n",
    "\n",
    "            with self.strategy.scope():\n",
    "                # Create distributed datasets.\n",
    "                train_dist_dataset = (\n",
    "                    self.strategy.experimental_distribute_dataset(\n",
    "                        dataset=train_dataset\n",
    "                    )\n",
    "                )\n",
    "                eval_dist_dataset = (\n",
    "                    self.strategy.experimental_distribute_dataset(\n",
    "                        dataset=eval_dataset\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Training block setups training, then loops through datasets.\n",
    "                self.train_block(\n",
    "                    train_dataset=train_dist_dataset,\n",
    "                    eval_dataset=eval_dist_dataset\n",
    "                )\n",
    "        else:\n",
    "            # Set global batch size for training.\n",
    "            self.global_batch_size = self.params[\"train_batch_size\"]\n",
    "\n",
    "            # Get input datasets.\n",
    "            train_dataset, eval_dataset = self.get_train_eval_datasets(\n",
    "                num_replicas=1\n",
    "            )\n",
    "\n",
    "            # Training block setups training, then loops through datasets.\n",
    "            self.train_block(\n",
    "                train_dataset=train_dataset, eval_dataset=eval_dataset\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgan_class_ctl_module/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "def calc_generator_discriminator_conv_layer_properties(\n",
    "        conv_num_filters, conv_kernel_sizes, conv_strides, depth):\n",
    "    \"\"\"Calculates generator and discriminator conv layer properties.\n",
    "\n",
    "    Args:\n",
    "        num_filters: list, nested list of ints of the number of filters\n",
    "            for each conv layer.\n",
    "        kernel_sizes: list, nested list of ints of the kernel sizes for\n",
    "            each conv layer.\n",
    "        strides: list, nested list of ints of the strides for each conv\n",
    "            layer.\n",
    "        depth: int, depth dimension of images.\n",
    "\n",
    "    Returns:\n",
    "        Nested lists of conv layer properties for both generator and\n",
    "            discriminator.\n",
    "    \"\"\"\n",
    "    def make_generator(num_filters, kernel_sizes, strides, depth):\n",
    "        \"\"\"Calculates generator conv layer properties.\n",
    "\n",
    "        Args:\n",
    "            num_filters: list, nested list of ints of the number of filters\n",
    "                for each conv layer.\n",
    "            kernel_sizes: list, nested list of ints of the kernel sizes for\n",
    "                each conv layer.\n",
    "            strides: list, nested list of ints of the strides for each conv\n",
    "                layer.\n",
    "            depth: int, depth dimension of images.\n",
    "\n",
    "        Returns:\n",
    "            Nested list of conv layer properties for generator.\n",
    "        \"\"\"\n",
    "        # Get the number of growths.\n",
    "        num_growths = len(num_filters) - 1\n",
    "\n",
    "        # Make base block.\n",
    "        in_out = num_filters[0]\n",
    "        base = [\n",
    "            [kernel_sizes[0][i]] * 2 + in_out + [strides[0][i]] * 2\n",
    "            for i in range(len(num_filters[0]))\n",
    "        ]\n",
    "        blocks = [base]\n",
    "\n",
    "        # Add growth blocks.\n",
    "        for i in range(1, num_growths + 1):\n",
    "            in_out = [[blocks[i - 1][-1][-3], num_filters[i][0]]]\n",
    "            block = [[kernel_sizes[i][0]] * 2 + in_out[0] + [strides[i][0]] * 2]\n",
    "            for j in range(1, len(num_filters[i])):\n",
    "                in_out.append([block[-1][-3], num_filters[i][j]])\n",
    "                block.append(\n",
    "                    [kernel_sizes[i][j]] * 2 + in_out[j] + [strides[i][j]] * 2\n",
    "                )\n",
    "            blocks.append(block)\n",
    "\n",
    "        # Add toRGB conv.\n",
    "        blocks[-1].append([1, 1, blocks[-1][-1][-3], depth] + [1] * 2)\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def make_discriminator(generator):\n",
    "        \"\"\"Calculates discriminator conv layer properties.\n",
    "\n",
    "        Args:\n",
    "            generator: list, nested list of conv layer properties for\n",
    "                generator.\n",
    "\n",
    "        Returns:\n",
    "            Nested list of conv layer properties for discriminator.\n",
    "        \"\"\"\n",
    "        # Reverse generator.\n",
    "        discriminator = generator[::-1]\n",
    "\n",
    "        # Reverse input and output shapes.\n",
    "        discriminator = [\n",
    "            [\n",
    "                conv[0:2] + conv[2:4][::-1] + conv[-2:]\n",
    "                for conv in block[::-1]\n",
    "            ]\n",
    "            for block in discriminator\n",
    "        ]\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    # Calculate conv layer properties for generator using args.\n",
    "    generator = make_generator(\n",
    "        conv_num_filters, conv_kernel_sizes, conv_strides, depth\n",
    "    )\n",
    "\n",
    "    # Calculate conv layer properties for discriminator using generator\n",
    "    # properties.\n",
    "    discriminator = make_discriminator(generator)\n",
    "\n",
    "    return generator, discriminator\n",
    "\n",
    "\n",
    "def split_up_generator_conv_layer_properties(\n",
    "        generator, num_filters, strides, depth):\n",
    "    \"\"\"Splits up generator conv layer properties into lists.\n",
    "\n",
    "    Args:\n",
    "        generator: list, nested list of conv layer properties for\n",
    "            generator.\n",
    "        num_filters: list, nested list of ints of the number of filters\n",
    "            for each conv layer.\n",
    "        strides: list, nested list of ints of the strides for each conv\n",
    "            layer.\n",
    "        depth: int, depth dimension of images.\n",
    "\n",
    "    Returns:\n",
    "        Nested lists of conv layer properties for generator.\n",
    "    \"\"\"\n",
    "    generator_base_conv_blocks = [generator[0][0:len(num_filters[0])]]\n",
    "\n",
    "    generator_growth_conv_blocks = []\n",
    "    if len(num_filters) > 1:\n",
    "        generator_growth_conv_blocks = generator[1:-1] + [generator[-1][:-1]]\n",
    "\n",
    "    generator_to_rgb_layers = [\n",
    "        [[1] * 2 + [num_filters[i][0]] + [depth] + [strides[i][0]] * 2]\n",
    "        for i in range(len(num_filters))\n",
    "    ]\n",
    "\n",
    "    return (generator_base_conv_blocks,\n",
    "            generator_growth_conv_blocks,\n",
    "            generator_to_rgb_layers)\n",
    "\n",
    "\n",
    "def split_up_discriminator_conv_layer_properties(\n",
    "        discriminator, num_filters, strides, depth):\n",
    "    \"\"\"Splits up discriminator conv layer properties into lists.\n",
    "\n",
    "    Args:\n",
    "        discriminator: list, nested list of conv layer properties for\n",
    "            discriminator.\n",
    "        num_filters: list, nested list of ints of the number of filters\n",
    "            for each conv layer.\n",
    "        strides: list, nested list of ints of the strides for each conv\n",
    "            layer.\n",
    "        depth: int, depth dimension of images.\n",
    "\n",
    "    Returns:\n",
    "        Nested lists of conv layer properties for discriminator.\n",
    "    \"\"\"\n",
    "    discriminator_from_rgb_layers = [\n",
    "        [[1] * 2 + [depth] + [num_filters[i][0]] + [strides[i][0]] * 2]\n",
    "        for i in range(len(num_filters))\n",
    "    ]\n",
    "\n",
    "    if len(num_filters) > 1:\n",
    "        discriminator_base_conv_blocks = [discriminator[-1]]\n",
    "    else:\n",
    "        discriminator_base_conv_blocks = [discriminator[-1][1:]]\n",
    "\n",
    "    discriminator_growth_conv_blocks = []\n",
    "    if len(num_filters) > 1:\n",
    "        discriminator_growth_conv_blocks = [discriminator[0][1:]] + discriminator[1:-1]\n",
    "        discriminator_growth_conv_blocks = discriminator_growth_conv_blocks[::-1]\n",
    "\n",
    "    return (discriminator_from_rgb_layers,\n",
    "            discriminator_base_conv_blocks,\n",
    "            discriminator_growth_conv_blocks)\n",
    "\n",
    "\n",
    "def convert_string_to_bool(string):\n",
    "    \"\"\"Converts string to bool.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        Boolean conversion of string.\n",
    "    \"\"\"\n",
    "    return False if string.lower() == \"false\" else True\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_float(string):\n",
    "    \"\"\"Converts string to None or float.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or float conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else float(string)\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_int(string):\n",
    "    \"\"\"Converts string to None or int.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or int conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else int(string)\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_ints(string, sep):\n",
    "    \"\"\"Converts string to list of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of ints conversion of string.\n",
    "    \"\"\"\n",
    "    return [int(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_lists_of_ints(string, outer_sep, inner_sep):\n",
    "    \"\"\"Converts string to list of lists of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        outer_sep: str, separator for outer list string.\n",
    "        inner_sep: str, separator for inner list string.\n",
    "\n",
    "    Returns:\n",
    "        List of lists of ints conversion of string.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        convert_string_to_list_of_ints(x, inner_sep)\n",
    "        for x in string.split(outer_sep)\n",
    "    ]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # File arguments.\n",
    "    parser.add_argument(\n",
    "        \"--train_file_pattern\",\n",
    "        help=\"GCS location to read training data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_file_pattern\",\n",
    "        help=\"GCS location to read evaluation data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud.\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "\n",
    "    # Training parameters.\n",
    "    parser.add_argument(\n",
    "        \"--tf_version\",\n",
    "        help=\"Version of TensorFlow\",\n",
    "        type=float,\n",
    "        default=2.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_graph_mode\",\n",
    "        help=\"Whether to use graph mode or not (eager).\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--distribution_strategy\",\n",
    "        help=\"Which distribution strategy to use, if any.\",\n",
    "        type=str,\n",
    "        default=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--write_summaries\",\n",
    "        help=\"Whether to write summaries for TensorBoard.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train for.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_dataset_length\",\n",
    "        help=\"Number of examples in one epoch of training set\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Number of examples in training batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_fn_autotune\",\n",
    "        help=\"Whether to autotune input function performance.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_step_count_steps\",\n",
    "        help=\"How many steps to train before writing steps and loss to log.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_summary_steps\",\n",
    "        help=\"How many steps to train before saving a summary.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_checkpoints_steps\",\n",
    "        help=\"How many steps to train before saving a checkpoint.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_checkpoint_max\",\n",
    "        help=\"Max number of checkpoints to keep.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--export_every_growth_phase\",\n",
    "        help=\"Whether to export SavedModel every growth phase.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "    # Eval parameters.\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        help=\"Number of examples in evaluation batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Number of steps to evaluate for.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "\n",
    "    # Image parameters.\n",
    "    parser.add_argument(\n",
    "        \"--height\",\n",
    "        help=\"Height of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--width\",\n",
    "        help=\"Width of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--depth\",\n",
    "        help=\"Depth of image.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "\n",
    "    # Shared network parameters.\n",
    "    parser.add_argument(\n",
    "        \"--num_steps_until_growth\",\n",
    "        help=\"Number of steps until layer added to generator & discriminator.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_equalized_learning_rate\",\n",
    "        help=\"If want to scale layer weights to equalize learning rate each forward pass.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--conv_num_filters\",\n",
    "        help=\"Number of filters for conv layers.\",\n",
    "        type=str,\n",
    "        default=\"512,512;512,512\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--conv_kernel_sizes\",\n",
    "        help=\"Kernel sizes for conv layers.\",\n",
    "        type=str,\n",
    "        default=\"3,3;3,3\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--conv_strides\",\n",
    "        help=\"Strides for conv layers.\",\n",
    "        type=str,\n",
    "        default=\"1,1;1,1\"\n",
    "    )\n",
    "\n",
    "    # Generator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_latent_size\",\n",
    "        help=\"The latent size of the noise vector.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_normalize_latents\",\n",
    "        help=\"If want to normalize latent vector before projection.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_use_pixel_norm\",\n",
    "        help=\"If want to use pixel norm op after each convolution.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_pixel_norm_epsilon\",\n",
    "        help=\"Small value to add to denominator for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_projection_dims\",\n",
    "        help=\"The 3D dimensions to project latent noise vector into.\",\n",
    "        type=str,\n",
    "        default=\"4,4,512\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_final_activation\",\n",
    "        help=\"The final activation function of generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_optimizer\",\n",
    "        help=\"Name of optimizer to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for generator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_train_steps\",\n",
    "        help=\"Number of steps to train generator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Discriminator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_use_minibatch_stddev\",\n",
    "        help=\"If want to use minibatch stddev op before first base conv layer.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_minibatch_stddev_group_size\",\n",
    "        help=\"The size of groups to split minibatch examples into.\",\n",
    "        type=int,\n",
    "        default=4\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_minibatch_stddev_averaging\",\n",
    "        help=\"If want to average across feature maps and pixels for minibatch stddev.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of discriminator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_optimizer\",\n",
    "        help=\"Name of optimizer to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_gradient_penalty_coefficient\",\n",
    "        help=\"Coefficient of gradient penalty for discriminator.\",\n",
    "        type=float,\n",
    "        default=10.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_epsilon_drift\",\n",
    "        help=\"Coefficient of epsilon drift penalty for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_train_steps\",\n",
    "        help=\"Number of steps to train discriminator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Parse all arguments.\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service.\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Fix use_graph_mode.\n",
    "    arguments[\"use_graph_mode\"] = convert_string_to_bool(\n",
    "        string=arguments[\"use_graph_mode\"]\n",
    "    )\n",
    "\n",
    "    # Fix write_summaries.\n",
    "    arguments[\"write_summaries\"] = convert_string_to_bool(\n",
    "        string=arguments[\"write_summaries\"]\n",
    "    )\n",
    "\n",
    "    # Fix input_fn_autotune.\n",
    "    arguments[\"input_fn_autotune\"] = convert_string_to_bool(\n",
    "        string=arguments[\"input_fn_autotune\"]\n",
    "    )\n",
    "\n",
    "    # Fix export_every_growth_phase.\n",
    "    arguments[\"export_every_growth_phase\"] = convert_string_to_bool(\n",
    "        string=arguments[\"export_every_growth_phase\"]\n",
    "    )\n",
    "\n",
    "    # Fix eval steps.\n",
    "    arguments[\"eval_steps\"] = convert_string_to_none_or_int(\n",
    "        string=arguments[\"eval_steps\"])\n",
    "\n",
    "    # Fix conv layer property parameters.\n",
    "    arguments[\"conv_num_filters\"] = convert_string_to_list_of_lists_of_ints(\n",
    "        string=arguments[\"conv_num_filters\"], outer_sep=\";\", inner_sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"conv_kernel_sizes\"] = convert_string_to_list_of_lists_of_ints(\n",
    "        string=arguments[\"conv_kernel_sizes\"], outer_sep=\";\", inner_sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"conv_strides\"] = convert_string_to_list_of_lists_of_ints(\n",
    "        string=arguments[\"conv_strides\"], outer_sep=\";\", inner_sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Make some assertions.\n",
    "    assert len(arguments[\"conv_num_filters\"]) > 0\n",
    "    assert len(arguments[\"conv_num_filters\"]) == len(arguments[\"conv_kernel_sizes\"])\n",
    "    assert len(arguments[\"conv_num_filters\"]) == len(arguments[\"conv_strides\"])\n",
    "\n",
    "    # Truncate lists if over the 1024x1024 current limit.\n",
    "    if len(arguments[\"conv_num_filters\"]) > 9:\n",
    "        arguments[\"conv_num_filters\"] = arguments[\"conv_num_filters\"][0:10]\n",
    "        arguments[\"conv_kernel_sizes\"] = arguments[\"conv_kernel_sizes\"][0:10]\n",
    "        arguments[\"conv_strides\"] = arguments[\"conv_strides\"][0:10]\n",
    "\n",
    "    # Get conv layer properties for generator and discriminator.\n",
    "    (generator,\n",
    "     discriminator) = calc_generator_discriminator_conv_layer_properties(\n",
    "        arguments[\"conv_num_filters\"],\n",
    "        arguments[\"conv_kernel_sizes\"],\n",
    "        arguments[\"conv_strides\"],\n",
    "        arguments[\"depth\"]\n",
    "    )\n",
    "\n",
    "    # Split up generator properties into separate lists.\n",
    "    (generator_base_conv_blocks,\n",
    "     generator_growth_conv_blocks,\n",
    "     generator_to_rgb_layers) = split_up_generator_conv_layer_properties(\n",
    "        generator,\n",
    "        arguments[\"conv_num_filters\"],\n",
    "        arguments[\"conv_strides\"],\n",
    "        arguments[\"depth\"]\n",
    "    )\n",
    "    arguments[\"generator_base_conv_blocks\"] = generator_base_conv_blocks\n",
    "    arguments[\"generator_growth_conv_blocks\"] = generator_growth_conv_blocks\n",
    "    arguments[\"generator_to_rgb_layers\"] = generator_to_rgb_layers\n",
    "\n",
    "    # Split up discriminator properties into separate lists.\n",
    "    (discriminator_from_rgb_layers,\n",
    "     discriminator_base_conv_blocks,\n",
    "     discriminator_growth_conv_blocks) = split_up_discriminator_conv_layer_properties(\n",
    "        discriminator,\n",
    "        arguments[\"conv_num_filters\"],\n",
    "        arguments[\"conv_strides\"],\n",
    "        arguments[\"depth\"]\n",
    "    )\n",
    "    arguments[\"discriminator_from_rgb_layers\"] = discriminator_from_rgb_layers\n",
    "    arguments[\"discriminator_base_conv_blocks\"] = discriminator_base_conv_blocks\n",
    "    arguments[\"discriminator_growth_conv_blocks\"] = discriminator_growth_conv_blocks\n",
    "\n",
    "    # Fix generator_normalize_latents.\n",
    "    arguments[\"generator_normalize_latents\"] = convert_string_to_bool(\n",
    "        arguments[\"generator_normalize_latents\"]\n",
    "    )\n",
    "\n",
    "    # Fix generator_use_pixel_norm.\n",
    "    arguments[\"generator_use_pixel_norm\"] = convert_string_to_bool(\n",
    "        arguments[\"generator_use_pixel_norm\"]\n",
    "    )\n",
    "\n",
    "    # Fix generator_projection_dims.\n",
    "    arguments[\"generator_projection_dims\"] = convert_string_to_list_of_ints(\n",
    "        arguments[\"generator_projection_dims\"], \",\"\n",
    "    )\n",
    "\n",
    "    # Fix discriminator_use_minibatch_stddev.\n",
    "    arguments[\"discriminator_use_minibatch_stddev\"] = convert_string_to_bool(\n",
    "        arguments[\"discriminator_use_minibatch_stddev\"]\n",
    "    )\n",
    "\n",
    "    # Fix clip_gradients.\n",
    "    arguments[\"generator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"generator_clip_gradients\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"discriminator_clip_gradients\"]\n",
    "    )\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning.\n",
    "    # This code can be removed if you are not using hyperparameter tuning.\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\n",
    "                \"TF_CONFIG\", \"{}\"\n",
    "            )\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\"))\n",
    "\n",
    "    print(arguments)\n",
    "\n",
    "    # Instantiate instance of model train and evaluate loop.\n",
    "    train_and_evaluate_model = model.TrainAndEvaluateModel(params=arguments)\n",
    "\n",
    "    # Run the training job.\n",
    "    train_and_evaluate_model.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
