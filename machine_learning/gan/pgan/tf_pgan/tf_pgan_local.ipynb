{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2-dlenv_tfe\n",
      "1.18.1\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments.\n",
    "arguments[\"train_file_pattern\"] = \"data/train.tfrecord\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/eval.tfrecord\"\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Training parameters.\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"train_steps\"] = 400\n",
    "\n",
    "# Eval parameters.\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "arguments[\"eval_steps\"] = 10\n",
    "arguments[\"start_delay_secs\"] = 600\n",
    "arguments[\"throttle_secs\"] = 600\n",
    "\n",
    "# Image parameters.\n",
    "arguments[\"height\"] = 32\n",
    "arguments[\"width\"] = 32\n",
    "arguments[\"depth\"] = 3\n",
    "\n",
    "# Shared parameters.\n",
    "arguments[\"num_steps_until_growth\"] = 100\n",
    "\n",
    "arguments[\"conv_num_filters\"] = [[512, 512], [512, 512], [512, 512], [512, 512], [256, 256]]\n",
    "arguments[\"conv_kernel_sizes\"] = [[4, 3], [3, 3], [3, 3], [3, 3], [3, 3]]\n",
    "arguments[\"conv_strides\"] = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n",
    "\n",
    "# Generator parameters.\n",
    "arguments[\"latent_size\"] = 512\n",
    "arguments[\"generator_projection_dims\"] = [4, 4, 512]\n",
    "arguments[\"generator_l1_regularization_scale\"] = 0.01\n",
    "arguments[\"generator_l2_regularization_scale\"] = 0.01\n",
    "arguments[\"generator_optimizer\"] = \"Adam\"\n",
    "arguments[\"generator_learning_rate\"] = 0.0001\n",
    "arguments[\"generator_clip_gradients\"] = 5.0\n",
    "arguments[\"generator_train_steps\"] = 1\n",
    "\n",
    "# Discriminator hyperparameters.\n",
    "arguments[\"discriminator_l1_regularization_scale\"] = 0.01\n",
    "arguments[\"discriminator_l2_regularization_scale\"] = 0.01\n",
    "arguments[\"discriminator_optimizer\"] = \"Adam\"\n",
    "arguments[\"discriminator_learning_rate\"] = 0.0001\n",
    "arguments[\"discriminator_clip_gradients\"] = 5.0\n",
    "arguments[\"discriminator_gradient_penalty_coefficient\"] = 10.0\n",
    "arguments[\"discriminator_train_steps\"] = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_obj(function_name, object_name, object_value):\n",
    "    \"\"\"Prints enclosing function, object name, and object value.\n",
    "\n",
    "    Args:\n",
    "        function_name: str, name of function.\n",
    "        object_name: str, name of object.\n",
    "        object_value: object, value of passed object.\n",
    "    \"\"\"\n",
    "#     pass\n",
    "    print(\"{}: {} = {}\".format(function_name, object_name, object_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "    print_obj(\"\\ndecode_example\", \"features\", features)\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "    print_obj(\"decode_example\", \"label\", label)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    \"\"\"Reads CSV time series data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read CSV dataset\n",
    "    using Dataset API, apply necessary preprocessing, and return an input\n",
    "    function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        mode: The estimator ModeKeys. Can be TRAIN or EVAL.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create list of files that match pattern.\n",
    "        file_list = tf.gfile.Glob(filename=filename)\n",
    "\n",
    "        # Create dataset from file list.\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=file_list, num_parallel_reads=40\n",
    "        )\n",
    "\n",
    "        # Shuffle and repeat if training with fused op.\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(\n",
    "                    buffer_size=50 * batch_size,\n",
    "                    count=None  # indefinitely\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decode CSV file into a features dictionary of tensors, then batch.\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                map_func=lambda x: decode_example(\n",
    "                    protos=x,\n",
    "                    params=params\n",
    "                ),\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=4\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(buffer_size=2)\n",
    "\n",
    "        # Create a iterator, then get batch of features from example queue.\n",
    "        batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return batched_dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_projection(Z, regularizer, params):\n",
    "    \"\"\"Creates generator projection from noise latent vector.\n",
    "\n",
    "    Args:\n",
    "        Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Latent vector projection tensor.\n",
    "    \"\"\"\n",
    "    # Project latent vectors.\n",
    "    projection_height = params[\"generator_projection_dims\"][0]\n",
    "    projection_width = params[\"generator_projection_dims\"][1]\n",
    "    projection_depth = params[\"generator_projection_dims\"][2]\n",
    "\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height * projection_width * projection_depth\n",
    "        # )\n",
    "        projection = tf.layers.dense(\n",
    "            inputs=Z,\n",
    "            units=projection_height * projection_width * projection_depth,\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=\"projection_layer\"\n",
    "        )\n",
    "        print_obj(\"generator_projection\", \"projection\", projection)\n",
    "\n",
    "    # Reshape projection into \"image\".\n",
    "    # shape = (\n",
    "    #     cur_batch_size,\n",
    "    #     projection_height,\n",
    "    #     projection_width,\n",
    "    #     projection_depth\n",
    "    # )\n",
    "    projection = tf.reshape(\n",
    "        tensor=projection,\n",
    "        shape=[-1, projection_height, projection_width, projection_depth],\n",
    "        name=\"projection_reshaped\"\n",
    "    )\n",
    "    print_obj(\"generator_network\", \"projection\", projection)\n",
    "\n",
    "    return projection\n",
    "\n",
    "\n",
    "def create_generator_base_conv_layer_block(regularizer, params):\n",
    "    \"\"\"Creates generator base conv layer block.\n",
    "\n",
    "    Args:\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of base conv layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = params[\"generator_base_conv_blocks\"][0]\n",
    "\n",
    "        # Create list of base conv layers.\n",
    "        base_conv_layers = [\n",
    "            tf.layers.Conv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"generator_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\ncreate_generator_base_conv_layer_block\",\n",
    "            \"base_conv_layers\",\n",
    "            base_conv_layers\n",
    "        )\n",
    "\n",
    "    return base_conv_layers\n",
    "\n",
    "\n",
    "def create_generator_growth_layer_block(block_idx, regularizer, params):\n",
    "    \"\"\"Creates generator growth block.\n",
    "\n",
    "    Args:\n",
    "        block_idx: int, the current growth block's index.\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of growth block layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = params[\"generator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "        # Create new inner convolutional layers.\n",
    "        conv_layers = [\n",
    "            tf.layers.Conv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"generator_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                    block_idx,\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\ncreate_generator_growth_layer_block\", \"conv_layers\", conv_layers\n",
    "        )\n",
    "\n",
    "    return conv_layers\n",
    "\n",
    "\n",
    "def create_generator_to_rgb_layers(regularizer, params):\n",
    "    \"\"\"Creates generator toRGB layers of 1x1 convs.\n",
    "\n",
    "    Args:\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of toRGB 1x1 conv layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # Get toRGB layer properties.\n",
    "        to_rgb = [\n",
    "            params[\"generator_to_rgb_layers\"][i][0][:]\n",
    "            for i in range(len(params[\"generator_to_rgb_layers\"]))\n",
    "        ]\n",
    "\n",
    "        # Create list to hold toRGB 1x1 convs.\n",
    "        to_rgb_conv_layers = [\n",
    "            # Create base toRGB conv 1x1.\n",
    "            tf.layers.Conv2D(\n",
    "                filters=to_rgb[i][3],\n",
    "                kernel_size=to_rgb[i][0:2],\n",
    "                strides=to_rgb[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"generator_to_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    i, to_rgb[i][0], to_rgb[i][1], to_rgb[i][2], to_rgb[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(to_rgb))\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\ncreate_generator_to_rgb_layers\",\n",
    "            \"to_rgb_conv_layers\",\n",
    "            to_rgb_conv_layers\n",
    "        )\n",
    "\n",
    "    return to_rgb_conv_layers\n",
    "\n",
    "\n",
    "def upsample_generator_image(image, original_image_size, block_idx):\n",
    "    \"\"\"Upsamples generator image.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, image created by generator conv block.\n",
    "        original_image_size: list, the height and width dimensions of the\n",
    "            original image before any growth.\n",
    "        block_idx: int, index of the current generator growth block.\n",
    "\n",
    "    Returns:\n",
    "        Upsampled image tensor.\n",
    "    \"\"\"\n",
    "    # Upsample from s X s to 2s X 2s image.\n",
    "    upsampled_image = tf.image.resize(\n",
    "        images=image,\n",
    "        size=tf.convert_to_tensor(\n",
    "            value=original_image_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"upsample_generator_image_original_image_size\"\n",
    "        ) * 2 ** block_idx,\n",
    "        method=\"nearest\",\n",
    "        name=\"generator_growth_upsampled_image_{}_{}x{}_{}x{}\".format(\n",
    "            block_idx,\n",
    "            original_image_size[0] * 2 ** (block_idx - 1),\n",
    "            original_image_size[1] * 2 ** (block_idx - 1),\n",
    "            original_image_size[0] * 2 ** block_idx,\n",
    "            original_image_size[1] * 2 ** block_idx\n",
    "        )\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\nupsample_generator_image\",\n",
    "        \"upsampled_image\",\n",
    "        upsampled_image\n",
    "    )\n",
    "\n",
    "    return upsampled_image\n",
    "\n",
    "\n",
    "def create_base_generator_network(X, to_rgb_conv_layers, blocks):\n",
    "    \"\"\"Creates base generator network.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, input image to generator.\n",
    "        to_rgb_conv_layers: list, toRGB 1x1 conv layers.\n",
    "        blocks: list, lists of block layers for each block.\n",
    "\n",
    "    Returns:\n",
    "        Final network block conv tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\ncreate_base_generator_network\", \"X\", X)\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # Only need the first block and toRGB conv layer for base network.\n",
    "        block_layers = blocks[0]\n",
    "        to_rgb_conv_layer = to_rgb_conv_layers[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = block_layers[0](inputs=X)\n",
    "        print_obj(\"create_base_generator_network\", \"block_conv_0\", block_conv)\n",
    "\n",
    "        for i in range(1, len(block_layers)):\n",
    "            block_conv = block_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_base_generator_network\",\n",
    "                \"block_conv_{}\".format(i),\n",
    "                block_conv\n",
    "            )\n",
    "        to_rgb_conv = to_rgb_conv_layer(inputs=block_conv)\n",
    "        print_obj(\"create_base_generator_network\", \"to_rgb_conv\", to_rgb_conv)\n",
    "\n",
    "    return to_rgb_conv\n",
    "\n",
    "\n",
    "def create_growth_transition_generator_network(\n",
    "        X,\n",
    "        to_rgb_conv_layers,\n",
    "        blocks,\n",
    "        original_image_size,\n",
    "        alpha_var,\n",
    "        trans_idx):\n",
    "    \"\"\"Creates base generator network.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, input image to generator.\n",
    "        to_rgb_conv_layers: list, toRGB 1x1 conv layers.\n",
    "        blocks: list, lists of block layers for each block.\n",
    "        original_image_size: list, the height and width dimensions of the\n",
    "            original image before any growth.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        trans_idx: int, index of current growth transition.\n",
    "\n",
    "    Returns:\n",
    "        Final network block conv tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\n",
    "        \"\\nEntered create_growth_transition_generator_network\",\n",
    "        \"trans_idx\",\n",
    "        trans_idx\n",
    "    )\n",
    "    print_obj(\"create_growth_transition_generator_network\", \"X\", X)\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # Permanent blocks.\n",
    "        permanent_blocks = blocks[0:trans_idx + 1]\n",
    "\n",
    "        # Base block doesn't need any upsampling so it's handled differently.\n",
    "        base_block_conv_layers = permanent_blocks[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = base_block_conv_layers[0](inputs=X)\n",
    "        print_obj(\n",
    "            \"\\ncreate_growth_transition_generator_network\",\n",
    "            \"base_block_conv_{}_0\".format(trans_idx),\n",
    "            block_conv\n",
    "        )\n",
    "        for i in range(1, len(base_block_conv_layers)):\n",
    "            block_conv = base_block_conv_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"base_block_conv_{}_{}\".format(trans_idx, i),\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "        # Growth blocks require first the prev conv layer's image upsampled.\n",
    "        for i in range(1, len(permanent_blocks)):\n",
    "            # Upsample previous block's image.\n",
    "            block_conv = upsample_generator_image(\n",
    "                image=block_conv,\n",
    "                original_image_size=original_image_size,\n",
    "                block_idx=i\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"upsample_generator_image_block_conv_{}_{}\".format(\n",
    "                    trans_idx, i\n",
    "                ),\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "            block_conv_layers = permanent_blocks[i]\n",
    "            for j in range(0, len(block_conv_layers)):\n",
    "                block_conv = block_conv_layers[j](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_generator_network\",\n",
    "                    \"block_conv_{}_{}_{}\".format(trans_idx, i, j),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "        # Upsample most recent block conv image for both side chains.\n",
    "        upsampled_block_conv = upsample_generator_image(\n",
    "            image=block_conv,\n",
    "            original_image_size=original_image_size,\n",
    "            block_idx=len(permanent_blocks)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"create_growth_transition_generator_network\",\n",
    "            \"upsampled_block_conv_{}\".format(trans_idx),\n",
    "            upsampled_block_conv\n",
    "        )\n",
    "\n",
    "        # Growing side chain.\n",
    "        growing_block_layers = blocks[trans_idx + 1]\n",
    "        growing_to_rgb_conv_layer = to_rgb_conv_layers[trans_idx + 1]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = growing_block_layers[0](inputs=upsampled_block_conv)\n",
    "        print_obj(\n",
    "            \"create_growth_transition_generator_network\",\n",
    "            \"growing_block_conv_{}_0\".format(trans_idx),\n",
    "            block_conv\n",
    "        )\n",
    "        for i in range(1, len(growing_block_layers)):\n",
    "            block_conv = growing_block_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"growing_block_conv_{}_{}\".format(trans_idx, i),\n",
    "                block_conv\n",
    "            )\n",
    "        growing_to_rgb_conv = growing_to_rgb_conv_layer(inputs=block_conv)\n",
    "        print_obj(\n",
    "            \"create_growth_transition_generator_network\",\n",
    "            \"growing_to_rgb_conv_{}\".format(trans_idx),\n",
    "            growing_to_rgb_conv\n",
    "        )\n",
    "\n",
    "        # Shrinking side chain.\n",
    "        shrinking_to_rgb_conv_layer = to_rgb_conv_layers[trans_idx]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        shrinking_to_rgb_conv = shrinking_to_rgb_conv_layer(\n",
    "            inputs=upsampled_block_conv\n",
    "        )\n",
    "        print_obj(\n",
    "            \"create_growth_transition_generator_network\",\n",
    "            \"shrinking_to_rgb_conv_{}\".format(trans_idx),\n",
    "            shrinking_to_rgb_conv\n",
    "        )\n",
    "\n",
    "        # Weighted sum.\n",
    "        weighted_sum = tf.add(\n",
    "            x=growing_to_rgb_conv * alpha_var,\n",
    "            y=shrinking_to_rgb_conv * (1.0 - alpha_var),\n",
    "            name=\"growth_transition_weighted_sum_{}\".format(trans_idx)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"create_growth_transition_generator_network\",\n",
    "            \"weighted_sum_{}\".format(trans_idx),\n",
    "            weighted_sum\n",
    "        )\n",
    "\n",
    "    return weighted_sum\n",
    "\n",
    "\n",
    "def create_final_generator_network(\n",
    "        X, to_rgb_conv_layers, blocks, original_image_size):\n",
    "    \"\"\"Creates base generator network.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, input image to generator.\n",
    "        to_rgb_conv_layers: list, toRGB 1x1 conv layers.\n",
    "        blocks: list, lists of block layers for each block.\n",
    "        original_image_size: list, the height and width dimensions of the\n",
    "            original image before any growth.\n",
    "\n",
    "    Returns:\n",
    "        Final network block conv tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\ncreate_final_generator_network\", \"X\", X)\n",
    "    with tf.variable_scope(name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "        # Base block doesn't need any upsampling so it's handled differently.\n",
    "        base_block_conv_layers = blocks[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = base_block_conv_layers[0](inputs=X)\n",
    "        print_obj(\n",
    "            \"\\ncreate_final_generator_network\",\n",
    "            \"base_block_conv\",\n",
    "            block_conv\n",
    "        )\n",
    "\n",
    "        for i in range(1, len(base_block_conv_layers)):\n",
    "            block_conv = base_block_conv_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_final_generator_network\",\n",
    "                \"base_block_conv_{}\".format(i),\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "        # Growth blocks require first the prev conv layer's image upsampled.\n",
    "        for i in range(1, len(blocks)):\n",
    "            # Upsample previous block's image.\n",
    "            block_conv = upsample_generator_image(\n",
    "                image=block_conv,\n",
    "                original_image_size=original_image_size,\n",
    "                block_idx=i\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_final_generator_network\",\n",
    "                \"upsample_generator_image_block_conv_{}\".format(i),\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "            block_conv_layers = blocks[i]\n",
    "            for j in range(0, len(block_conv_layers)):\n",
    "                block_conv = block_conv_layers[j](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_final_generator_network\",\n",
    "                    \"block_conv_{}_{}\".format(i, j),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "        # Only need the last toRGB conv layer.\n",
    "        to_rgb_conv_layer = to_rgb_conv_layers[-1]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        to_rgb_conv = to_rgb_conv_layer(inputs=block_conv)\n",
    "        print_obj(\n",
    "            \"create_final_generator_network\", \"to_rgb_conv\", to_rgb_conv\n",
    "        )\n",
    "\n",
    "    return to_rgb_conv\n",
    "\n",
    "\n",
    "def generator_network(Z, alpha_var, params):\n",
    "    \"\"\"Creates generator network and returns generated output.\n",
    "\n",
    "    Args:\n",
    "        Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Generated outputs tensor of shape\n",
    "            [cur_batch_size, height * width * depth].\n",
    "    \"\"\"\n",
    "    print_obj(\"\\ngenerator_network\", \"Z\", Z)\n",
    "\n",
    "    # Create regularizer for layer kernel weights.\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "        scale_l1=params[\"generator_l1_regularization_scale\"],\n",
    "        scale_l2=params[\"generator_l2_regularization_scale\"]\n",
    "    )\n",
    "\n",
    "    # Project latent vectors.\n",
    "    projection = generator_projection(Z, regularizer, params)\n",
    "    print_obj(\"generator_network\", \"projection\", projection)\n",
    "\n",
    "    # Create empty list to hold generator convolutional layer blocks.\n",
    "    blocks = []\n",
    "\n",
    "    # Create base convolutional layers, for post-growth.\n",
    "    blocks.append(create_generator_base_conv_layer_block(regularizer, params))\n",
    "\n",
    "    # Create growth layer blocks.\n",
    "    for block_idx in range(len(params[\"generator_growth_conv_blocks\"])):\n",
    "        blocks.append(\n",
    "            create_generator_growth_layer_block(\n",
    "                block_idx, regularizer, params\n",
    "            )\n",
    "        )\n",
    "    print_obj(\"generator_network\", \"blocks\", blocks)\n",
    "\n",
    "    # Create list of toRGB 1x1 conv layers.\n",
    "    to_rgb_conv_layers = create_generator_to_rgb_layers(\n",
    "        regularizer, params\n",
    "    )\n",
    "    print_obj(\"generator_network\", \"to_rgb_conv_layers\", to_rgb_conv_layers)\n",
    "\n",
    "    # Get generated outputs.\n",
    "    if (params[\"train_steps\"] // params[\"num_steps_until_growth\"] <= 0 or\n",
    "       len(params[\"conv_num_filters\"]) == 1):\n",
    "        print(\"\\ngenerator_network: NEVER GOING TO GROW, SKIP SWITCH CASE\")\n",
    "        # If we never are going to grow, no sense using the switch case.\n",
    "        # 4x4\n",
    "        generated_outputs = create_base_generator_network(\n",
    "            projection, to_rgb_conv_layers, blocks\n",
    "        )\n",
    "    else:\n",
    "        # Find growth index based on global step and growth frequency.\n",
    "        growth_index = tf.cast(\n",
    "            x=tf.floordiv(\n",
    "                x=tf.train.get_or_create_global_step(),\n",
    "                y=params[\"num_steps_until_growth\"]\n",
    "            ),\n",
    "            dtype=tf.int32,\n",
    "            name=\"generator_growth_index\"\n",
    "        )\n",
    "\n",
    "        # Switch to case based on number of steps for network creation.\n",
    "        generated_outputs = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                # 4x4\n",
    "                lambda: create_base_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks\n",
    "                ),\n",
    "                # 8x8\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    0\n",
    "                ),\n",
    "                # 16x16\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    1\n",
    "                ),\n",
    "                # 32x32\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    2\n",
    "                ),\n",
    "                # 64x64\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    3\n",
    "                ),\n",
    "                # 128x128\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    4\n",
    "                ),\n",
    "                # 256x256\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    5\n",
    "                ),\n",
    "                # 512x512\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    6\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: create_growth_transition_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2],\n",
    "                    alpha_var,\n",
    "                    7\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: create_final_generator_network(\n",
    "                    projection,\n",
    "                    to_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    params[\"generator_projection_dims\"][0:2]\n",
    "                )\n",
    "            ],\n",
    "            name=\"generator_switch_case_generated_outputs\"\n",
    "        )\n",
    "\n",
    "    print_obj(\"generator_network\", \"generated_outputs\", generated_outputs)\n",
    "\n",
    "    return generated_outputs\n",
    "\n",
    "\n",
    "def get_generator_loss(generated_logits):\n",
    "    \"\"\"Gets generator loss.\n",
    "\n",
    "    Args:\n",
    "        generated_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "\n",
    "    Returns:\n",
    "        Tensor of generator's total loss of shape [].\n",
    "    \"\"\"\n",
    "    # Calculate base generator loss.\n",
    "    generator_loss = -tf.reduce_mean(\n",
    "        input_tensor=generated_logits,\n",
    "        name=\"generator_loss\"\n",
    "    )\n",
    "    print_obj(\"\\nget_generator_loss\", \"generator_loss\", generator_loss)\n",
    "\n",
    "    # Get regularization losses.\n",
    "    generator_regularization_loss = tf.losses.get_regularization_loss(\n",
    "        scope=\"generator\",\n",
    "        name=\"generator_regularization_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_generator_loss\",\n",
    "        \"generator_regularization_loss\",\n",
    "        generator_regularization_loss\n",
    "    )\n",
    "\n",
    "    # Combine losses for total losses.\n",
    "    generator_total_loss = tf.math.add(\n",
    "        x=generator_loss,\n",
    "        y=generator_regularization_loss,\n",
    "        name=\"generator_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_generator_loss\", \"generator_total_loss\", generator_total_loss\n",
    "    )\n",
    "\n",
    "    return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator_from_rgb_layers(regularizer, params):\n",
    "    \"\"\"Creates discriminator fromRGB layers of 1x1 convs.\n",
    "\n",
    "    Args:\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of fromRGB 1x1 conv layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Get fromRGB layer properties.\n",
    "        from_rgb = [\n",
    "            params[\"discriminator_from_rgb_layers\"][i][0][:]\n",
    "            for i in range(len(params[\"discriminator_from_rgb_layers\"]))\n",
    "        ]\n",
    "\n",
    "        # Create list to hold toRGB 1x1 convs.\n",
    "        from_rgb_conv_layers = [\n",
    "            # Create base toRGB conv 1x1.\n",
    "            tf.layers.Conv2D(\n",
    "                filters=from_rgb[i][3],\n",
    "                kernel_size=from_rgb[i][0:2],\n",
    "                strides=from_rgb[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"discriminator_from_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    i,\n",
    "                    from_rgb[i][0],\n",
    "                    from_rgb[i][1],\n",
    "                    from_rgb[i][2],\n",
    "                    from_rgb[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(from_rgb))\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\ncreate_discriminator_from_rgb_layers\",\n",
    "            \"from_rgb_conv_layers\",\n",
    "            from_rgb_conv_layers\n",
    "        )\n",
    "\n",
    "    return from_rgb_conv_layers\n",
    "\n",
    "\n",
    "def create_discriminator_base_conv_layer_block(regularizer, params):\n",
    "    \"\"\"Creates discriminator base conv layer block.\n",
    "\n",
    "    Args:\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of base conv layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = params[\"discriminator_base_conv_blocks\"][0]\n",
    "\n",
    "        # Create list of base conv layers.\n",
    "        base_conv_layers = [\n",
    "            tf.layers.Conv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"discriminator_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block) - 1)\n",
    "        ]\n",
    "\n",
    "        # Have valid padding for layer just before flatten and logits.\n",
    "        base_conv_layers.append(\n",
    "            tf.layers.Conv2D(\n",
    "                filters=conv_block[-1][3],\n",
    "                kernel_size=conv_block[-1][0:2],\n",
    "                strides=conv_block[-1][4:6],\n",
    "                padding=\"valid\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"discriminator_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                    len(conv_block) - 1,\n",
    "                    conv_block[-1][0],\n",
    "                    conv_block[-1][1],\n",
    "                    conv_block[-1][2],\n",
    "                    conv_block[-1][3]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\ncreate_discriminator_base_conv_layer_block\",\n",
    "            \"base_conv_layers\",\n",
    "            base_conv_layers\n",
    "        )\n",
    "\n",
    "    return base_conv_layers\n",
    "\n",
    "\n",
    "def create_discriminator_growth_layer_block(\n",
    "        block_idx, regularizer, params):\n",
    "    \"\"\"Creates discriminator growth block.\n",
    "\n",
    "    Args:\n",
    "        block_idx: int, the current growth block's index.\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of growth block layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Get conv block layer properties.\n",
    "        conv_block = params[\"discriminator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "        # Create new inner convolutional layers.\n",
    "        conv_layers = [\n",
    "            tf.layers.Conv2D(\n",
    "                filters=conv_block[i][3],\n",
    "                kernel_size=conv_block[i][0:2],\n",
    "                strides=conv_block[i][4:6],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "#                 kernel_regularizer=regularizer,\n",
    "                name=\"discriminator_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                    block_idx,\n",
    "                    i,\n",
    "                    conv_block[i][0],\n",
    "                    conv_block[i][1],\n",
    "                    conv_block[i][2],\n",
    "                    conv_block[i][3]\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_block))\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\ncreate_discriminator_growth_layer_block\",\n",
    "            \"conv_layers\",\n",
    "            conv_layers\n",
    "        )\n",
    "\n",
    "        # Down sample from 2s X 2s to s X s image.\n",
    "        downsampled_image_layer = tf.layers.AveragePooling2D(\n",
    "            pool_size=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            name=\"discriminator_growth_downsampled_image_{}\".format(\n",
    "                block_idx\n",
    "            )\n",
    "        )\n",
    "        print_obj(\n",
    "            \"create_discriminator_growth_layer_block\",\n",
    "            \"downsampled_image_layer\",\n",
    "            downsampled_image_layer\n",
    "        )\n",
    "\n",
    "    return conv_layers + [downsampled_image_layer]\n",
    "\n",
    "\n",
    "def create_discriminator_growth_transition_downsample_layers(params):\n",
    "    \"\"\"Creates discriminator growth transition downsample layers.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        List of growth transition downsample layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Down sample from 2s X 2s to s X s image.\n",
    "        downsample_layers = [\n",
    "            tf.layers.AveragePooling2D(\n",
    "                pool_size=(2, 2),\n",
    "                strides=(2, 2),\n",
    "                name=\"discriminator_growth_transition_downsample_layer_{}\".format(\n",
    "                    layer_idx\n",
    "                )\n",
    "            )\n",
    "            for layer_idx in range(\n",
    "                1 + len(params[\"discriminator_growth_conv_blocks\"])\n",
    "            )\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\ncreate_discriminator_growth_transition_downsample_layers\",\n",
    "            \"downsample_layers\",\n",
    "            downsample_layers\n",
    "        )\n",
    "\n",
    "    return downsample_layers\n",
    "\n",
    "\n",
    "def create_base_discriminator_network(\n",
    "        X, from_rgb_conv_layers, blocks, params):\n",
    "    \"\"\"Creates base discriminator network.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, input image to discriminator.\n",
    "        from_rgb_conv_layers: list, fromRGB 1x1 conv layers.\n",
    "        blocks: list, lists of block layers for each block.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Last block's last conv layer's tensor.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Only need the first fromRGB conv layer and block for base network.\n",
    "        from_rgb_conv_layer = from_rgb_conv_layers[0]\n",
    "        block_layers = blocks[0]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = from_rgb_conv_layer(inputs=X)\n",
    "        print_obj(\n",
    "            \"\\ncreate_base_discriminator_network\",\n",
    "            \"block_conv\",\n",
    "            block_conv\n",
    "        )\n",
    "\n",
    "        for i in range(len(block_layers)):\n",
    "            block_conv = block_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_base_discriminator_network\", \"block_conv\", block_conv\n",
    "            )\n",
    "\n",
    "    return block_conv\n",
    "\n",
    "\n",
    "def create_growth_transition_discriminator_network(\n",
    "        X,\n",
    "        from_rgb_conv_layers,\n",
    "        blocks,\n",
    "        transition_downsample_layers,\n",
    "        alpha_var,\n",
    "        params,\n",
    "        trans_idx):\n",
    "    \"\"\"Creates base discriminator network.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, input image to discriminator.\n",
    "        from_rgb_conv_layers: list, fromRGB 1x1 conv layers.\n",
    "        blocks: list, lists of block layers for each block.\n",
    "        transition_downsample_layers: list, downsample layers for transition.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "        trans_idx: int, index of current growth transition.\n",
    "\n",
    "    Returns:\n",
    "        Last block's last conv layer's tensor.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Growing side chain.\n",
    "        growing_from_rgb_conv_layer = from_rgb_conv_layers[trans_idx + 1]\n",
    "        growing_block_layers = blocks[trans_idx + 1]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        growing_block_conv = growing_from_rgb_conv_layer(inputs=X)\n",
    "        print_obj(\n",
    "            \"\\ncreate_base_discriminator_network\",\n",
    "            \"growing_block_conv\",\n",
    "            growing_block_conv\n",
    "        )\n",
    "        for i in range(len(growing_block_layers)):\n",
    "            growing_block_conv = growing_block_layers[i](\n",
    "                inputs=growing_block_conv\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_base_discriminator_network\",\n",
    "                \"growing_block_conv\",\n",
    "                growing_block_conv\n",
    "            )\n",
    "\n",
    "        # Shrinking side chain.\n",
    "        transition_downsample_layer = transition_downsample_layers[trans_idx]\n",
    "        shrinking_from_rgb_conv_layer = from_rgb_conv_layers[trans_idx]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        transition_downsample = transition_downsample_layer(inputs=X)\n",
    "        print_obj(\n",
    "            \"create_base_discriminator_network\",\n",
    "            \"transition_downsample\",\n",
    "            transition_downsample\n",
    "        )\n",
    "        shrinking_from_rgb_conv = shrinking_from_rgb_conv_layer(\n",
    "            inputs=transition_downsample\n",
    "        )\n",
    "        print_obj(\n",
    "            \"create_base_discriminator_network\",\n",
    "            \"shrinking_from_rgb_conv\",\n",
    "            shrinking_from_rgb_conv\n",
    "        )\n",
    "\n",
    "        # Weighted sum.\n",
    "        weighted_sum = tf.add(\n",
    "            x=growing_block_conv * alpha_var,\n",
    "            y=shrinking_from_rgb_conv * (1.0 - alpha_var),\n",
    "            name=\"growth_transition_weighted_sum_{}\".format(trans_idx)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"create_base_discriminator_network\",\n",
    "            \"weighted_sum\",\n",
    "            weighted_sum\n",
    "        )\n",
    "\n",
    "        # Permanent blocks.\n",
    "        permanent_blocks = blocks[0:trans_idx + 1]\n",
    "\n",
    "        # Reverse order of blocks and flatten.\n",
    "        permanent_block_layers = [\n",
    "            item for sublist in permanent_blocks[::-1] for item in sublist\n",
    "        ]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = weighted_sum\n",
    "        for i in range(len(permanent_block_layers)):\n",
    "            block_conv = permanent_block_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_growth_transition_discriminator_network\",\n",
    "                \"block_conv\",\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "    return block_conv\n",
    "\n",
    "\n",
    "def create_final_discriminator_network(\n",
    "        X, from_rgb_conv_layers, blocks, params):\n",
    "    \"\"\"Creates base discriminator network.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, input image to discriminator.\n",
    "        from_rgb_conv_layers: list, fromRGB 1x1 conv layers.\n",
    "        blocks: list, lists of block layers for each block.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Last block's last conv layer's tensor.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Only need the last fromRGB conv layer.\n",
    "        from_rgb_conv_layer = from_rgb_conv_layers[-1]\n",
    "\n",
    "        # Reverse order of blocks and flatten.\n",
    "        block_layers = [item for sublist in blocks[::-1] for item in sublist]\n",
    "\n",
    "        # Pass inputs through layer chain.\n",
    "        block_conv = from_rgb_conv_layer(inputs=X)\n",
    "        print_obj(\n",
    "            \"\\ncreate_final_discriminator_network\",\n",
    "            \"block_conv\",\n",
    "            block_conv\n",
    "        )\n",
    "\n",
    "        for i in range(len(block_layers)):\n",
    "            block_conv = block_layers[i](inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"create_final_discriminator_network\", \"block_conv\", block_conv\n",
    "            )\n",
    "\n",
    "    return block_conv\n",
    "\n",
    "\n",
    "def discriminator_logits(block_conv, regularizer):\n",
    "    \"\"\"Finds logits from discriminator's last conv layer.\n",
    "\n",
    "    Args:\n",
    "        block_conv: tensor, output of last conv layer of discriminator.\n",
    "        regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "\n",
    "    Returns:\n",
    "        Final logits tensor of discriminator.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "        # Flatten final block conv tensor.\n",
    "        block_conv_flat = tf.layers.Flatten()(inputs=block_conv)\n",
    "        print_obj(\n",
    "            \"discriminator_network\",\n",
    "            \"block_conv_flat\",\n",
    "            block_conv_flat\n",
    "        )\n",
    "\n",
    "        # Final linear layer for logits.\n",
    "        logits = tf.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "#             kernel_regularizer=regularizer,\n",
    "            name=\"layers_dense_logits\"\n",
    "        )(inputs=block_conv_flat)\n",
    "        print_obj(\n",
    "            \"create_growth_transition_discriminator_network\", \"logits\", logits\n",
    "        )\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def discriminator_network(X, alpha_var, params):\n",
    "    \"\"\"Creates discriminator network and returns logits.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, image tensors of shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Logits tensor of shape [cur_batch_size, 1].\n",
    "    \"\"\"\n",
    "    print_obj(\"\\ndiscriminator_network\", \"X\", X)\n",
    "\n",
    "    # Create regularizer for layer kernel weights.\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "        scale_l1=params[\"discriminator_l1_regularization_scale\"],\n",
    "        scale_l2=params[\"discriminator_l2_regularization_scale\"]\n",
    "    )\n",
    "\n",
    "    # Create list of fromRGB 1x1 conv layers.\n",
    "    from_rgb_conv_layers = create_discriminator_from_rgb_layers(\n",
    "        regularizer, params\n",
    "    )\n",
    "    print_obj(\n",
    "        \"discriminator_network\",\n",
    "        \"from_rgb_conv_layers\",\n",
    "        from_rgb_conv_layers\n",
    "    )\n",
    "\n",
    "    # Create empty list to hold discriminator convolutional layer blocks.\n",
    "    blocks = []\n",
    "\n",
    "    # Create base convolutional layers, for post-growth.\n",
    "    blocks.append(\n",
    "        create_discriminator_base_conv_layer_block(regularizer, params)\n",
    "    )\n",
    "\n",
    "    # Create growth layer blocks.\n",
    "    for block_idx in range(len(params[\"discriminator_growth_conv_blocks\"])):\n",
    "        blocks.append(\n",
    "            create_discriminator_growth_layer_block(\n",
    "                block_idx, regularizer, params\n",
    "            )\n",
    "        )\n",
    "    print_obj(\"discriminator_network\", \"blocks\", blocks)\n",
    "\n",
    "    # Create list of transition downsample layers.\n",
    "    transition_downsample_layers = (\n",
    "        create_discriminator_growth_transition_downsample_layers(params)\n",
    "    )\n",
    "    print_obj(\n",
    "        \"discriminator_network\",\n",
    "        \"transition_downsample_layers\",\n",
    "        transition_downsample_layers\n",
    "    )\n",
    "\n",
    "    # Get final convolutional block's final layer output.\n",
    "    if (params[\"train_steps\"] // params[\"num_steps_until_growth\"] <= 0 or\n",
    "       len(params[\"conv_num_filters\"]) == 1):\n",
    "        print(\"\\ndiscriminator_network: NEVER GOING TO GROW, SKIP SWITCH CASE\")\n",
    "        # If we never are going to grow, no sense using the switch case.\n",
    "        # 4x4\n",
    "        block_conv = create_base_discriminator_network(\n",
    "            X, from_rgb_conv_layers, blocks, params\n",
    "        )\n",
    "    else:\n",
    "        # Find growth index based on global step and growth frequency.\n",
    "        growth_index = tf.cast(\n",
    "            x=tf.floordiv(\n",
    "                x=tf.train.get_or_create_global_step(),\n",
    "                y=params[\"num_steps_until_growth\"]\n",
    "            ),\n",
    "            dtype=tf.int32,\n",
    "            name=\"discriminator_growth_index\"\n",
    "        )\n",
    "\n",
    "        # Switch to case based on number of steps for network creation.\n",
    "        block_conv = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                # 4x4\n",
    "                lambda: create_base_discriminator_network(\n",
    "                    X, from_rgb_conv_layers, blocks, params\n",
    "                ),\n",
    "                # 8x8\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    0\n",
    "                ),\n",
    "                # 16x16\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    1\n",
    "                ),\n",
    "                # 32x32\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    2\n",
    "                ),\n",
    "                # 64x64\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    3\n",
    "                ),\n",
    "                # 128x128\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    4\n",
    "                ),\n",
    "                # 256x256\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    5\n",
    "                ),\n",
    "                # 512x512\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    6\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: create_growth_transition_discriminator_network(\n",
    "                    X,\n",
    "                    from_rgb_conv_layers,\n",
    "                    blocks,\n",
    "                    transition_downsample_layers,\n",
    "                    alpha_var,\n",
    "                    params,\n",
    "                    7\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: create_final_discriminator_network(\n",
    "                    X, from_rgb_conv_layers, blocks, params\n",
    "                )\n",
    "            ],\n",
    "            name=\"discriminator_switch_case_block_conv\"\n",
    "        )\n",
    "\n",
    "    # Set shape to remove ambiguity for dense layer.\n",
    "    block_conv.set_shape(\n",
    "        [\n",
    "            block_conv.get_shape()[0],\n",
    "            params[\"generator_projection_dims\"][0] / 4,\n",
    "            params[\"generator_projection_dims\"][1] / 4,\n",
    "            block_conv.get_shape()[-1]]\n",
    "    )\n",
    "    print_obj(\n",
    "        \"discriminator_network\",\n",
    "        \"block_conv\",\n",
    "        block_conv\n",
    "    )\n",
    "\n",
    "    # Get final logits.\n",
    "    logits = discriminator_logits(block_conv, regularizer)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def get_discriminator_loss(generated_logits, real_logits, params):\n",
    "    \"\"\"Gets discriminator loss.\n",
    "\n",
    "    Args:\n",
    "        generated_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "        real_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of discriminator's total loss of shape [].\n",
    "    \"\"\"\n",
    "    # Calculate base discriminator loss.\n",
    "    discriminator_real_loss = tf.reduce_mean(\n",
    "        input_tensor=real_logits,\n",
    "        name=\"discriminator_real_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\nget_discriminator_loss\",\n",
    "        \"discriminator_real_loss\",\n",
    "        discriminator_real_loss\n",
    "    )\n",
    "\n",
    "    discriminator_generated_loss = tf.reduce_mean(\n",
    "        input_tensor=generated_logits,\n",
    "        name=\"discriminator_generated_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_generated_loss\",\n",
    "        discriminator_generated_loss\n",
    "    )\n",
    "\n",
    "    discriminator_loss = tf.add(\n",
    "        x=discriminator_real_loss, y=-discriminator_generated_loss,\n",
    "        name=\"discriminator_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_loss\",\n",
    "        discriminator_loss\n",
    "    )\n",
    "\n",
    "    # Get discriminator gradient penalty.\n",
    "    discriminator_gradients = tf.gradients(\n",
    "        ys=discriminator_loss,\n",
    "        xs=tf.trainable_variables(scope=\"discriminator\"),\n",
    "        name=\"discriminator_gradients_for_penalty\"\n",
    "    )\n",
    "\n",
    "    discriminator_gradient_penalty = tf.square(\n",
    "        x=tf.multiply(\n",
    "            x=params[\"discriminator_gradient_penalty_coefficient\"],\n",
    "            y=tf.linalg.global_norm(\n",
    "                t_list=discriminator_gradients,\n",
    "                name=\"discriminator_gradients_global_norm\"\n",
    "            ) - 1.0\n",
    "        ),\n",
    "        name=\"discriminator_gradient_penalty\"\n",
    "    )\n",
    "\n",
    "    discriminator_wasserstein_gp_loss = tf.add(\n",
    "        x=discriminator_loss,\n",
    "        y=discriminator_gradient_penalty,\n",
    "        name=\"discriminator_wasserstein_gp_loss\"\n",
    "    )\n",
    "\n",
    "    # Get regularization losses.\n",
    "    discriminator_regularization_loss = tf.losses.get_regularization_loss(\n",
    "        scope=\"discriminator\",\n",
    "        name=\"discriminator_regularization_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_regularization_loss\",\n",
    "        discriminator_regularization_loss\n",
    "    )\n",
    "\n",
    "    # Combine losses for total losses.\n",
    "    discriminator_total_loss = tf.math.add(\n",
    "        x=discriminator_wasserstein_gp_loss,\n",
    "        y=discriminator_regularization_loss,\n",
    "        name=\"discriminator_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_total_loss\",\n",
    "        discriminator_total_loss\n",
    "    )\n",
    "\n",
    "    return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(loss, global_step, alpha_var, params, scope):\n",
    "    \"\"\"Trains network and returns loss and train op.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the variables that to train.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor and training op.\n",
    "    \"\"\"\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adam\": tf.train.AdamOptimizer,\n",
    "        \"Adadelta\": tf.train.AdadeltaOptimizer,\n",
    "        \"AdagradDA\": tf.train.AdagradDAOptimizer,\n",
    "        \"Adagrad\": tf.train.AdagradOptimizer,\n",
    "        \"Ftrl\": tf.train.FtrlOptimizer,\n",
    "        \"GradientDescent\": tf.train.GradientDescentOptimizer,\n",
    "        \"Momentum\": tf.train.MomentumOptimizer,\n",
    "        \"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer,\n",
    "        \"ProximalGradientDescent\": tf.train.ProximalGradientDescentOptimizer,\n",
    "        \"RMSProp\": tf.train.RMSPropOptimizer\n",
    "    }\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=tf.trainable_variables(scope=scope),\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    # Clip gradients.\n",
    "    if params[\"{}_clip_gradients\".format(scope)]:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=params[\"{}_clip_gradients\".format(scope)],\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "\n",
    "    # Zip back together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, tf.trainable_variables(scope=scope))\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "        learning_rate=params[\"{}_learning_rate\".format(scope)]\n",
    "    )\n",
    "\n",
    "    # Create train op by applying gradients to variables and incrementing\n",
    "    # global step.\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        global_step=global_step,\n",
    "        name=\"{}_apply_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    # Update alpha variable to linearly scale from 0 to 1 based on steps.\n",
    "    alpha_var_update_op = tf.assign(\n",
    "        ref=alpha_var,\n",
    "        value=tf.divide(\n",
    "            x=tf.cast(\n",
    "                x=tf.mod(x=global_step, y=params[\"num_steps_until_growth\"]),\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            y=params[\"num_steps_until_growth\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Ensure alpha variable gets updated.\n",
    "    with tf.control_dependencies(control_inputs=[alpha_var_update_op]):\n",
    "        return loss, train_op\n",
    "\n",
    "\n",
    "def resize_real_image(block_idx, image, params):\n",
    "    \"\"\"Resizes real images to match the GAN's current size.\n",
    "\n",
    "    Args:\n",
    "        block_idx: int, index of current block.\n",
    "        image: tensor, original image.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Resized image tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nresize_real_image\", \"block_idx\", block_idx)\n",
    "    print_obj(\"resize_real_image\", \"image\", image)\n",
    "\n",
    "    # Resize image to match GAN size at current block index.\n",
    "    resized_image = tf.image.resize(\n",
    "        images=image,\n",
    "        size=[\n",
    "            params[\"generator_projection_dims\"][0] * (2 ** block_idx),\n",
    "            params[\"generator_projection_dims\"][1] * (2 ** block_idx)\n",
    "        ],\n",
    "        method=\"nearest\",\n",
    "        name=\"resize_real_images_resized_image_{}\".format(block_idx)\n",
    "    )\n",
    "    print_obj(\"resize_real_images\", \"resized_image\", resized_image)\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def resize_real_images(image, params):\n",
    "    \"\"\"Resizes real images to match the GAN's current size.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, original image.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Resized image tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nresize_real_images\", \"image\", image)\n",
    "    # Resize real image for each block.\n",
    "    if (params[\"train_steps\"] // params[\"num_steps_until_growth\"] <= 0 or\n",
    "       len(params[\"conv_num_filters\"]) == 1):\n",
    "        # If we never are going to grow, no sense using the switch case.\n",
    "        # 4x4\n",
    "        resized_image = resize_real_image(0, image, params)\n",
    "        print_obj(\n",
    "            \"resize_real_images\", \"slipped resized_image\", resized_image\n",
    "        )\n",
    "    else:\n",
    "        # Find growth index based on global step and growth frequency.\n",
    "        growth_index = tf.cast(\n",
    "            x=tf.floordiv(\n",
    "                x=tf.train.get_or_create_global_step(),\n",
    "                y=params[\"num_steps_until_growth\"]\n",
    "            ),\n",
    "            dtype=tf.int32,\n",
    "            name=\"resize_real_images_growth_index\"\n",
    "        )\n",
    "\n",
    "        # Switch to case based on number of steps for resized image.\n",
    "        resized_image = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                lambda: resize_real_image(0, image, params),  # 4x4\n",
    "                lambda: resize_real_image(1, image, params),  # 8x8\n",
    "                lambda: resize_real_image(2, image, params),  # 16x16\n",
    "                lambda: resize_real_image(3, image, params),  # 32x32\n",
    "                lambda: resize_real_image(4, image, params),  # 64x64\n",
    "                lambda: resize_real_image(5, image, params),  # 128x128\n",
    "                lambda: resize_real_image(6, image, params),  # 256x256\n",
    "                lambda: resize_real_image(7, image, params),  # 512x512\n",
    "                lambda: resize_real_image(8, image, params),  # 1024x1024\n",
    "            ],\n",
    "            name=\"resize_real_images_switch_case_resized_image\"\n",
    "        )\n",
    "        print_obj(\n",
    "            \"resize_real_images\", \"selected resized_image\", resized_image\n",
    "        )\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def pgan_model(features, labels, mode, params):\n",
    "    \"\"\"Progressively Growing GAN custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `tf.estimator.EstimatorSpec` class.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\npgan_model\", \"features\", features)\n",
    "    print_obj(\"pgan_model\", \"labels\", labels)\n",
    "    print_obj(\"pgan_model\", \"mode\", mode)\n",
    "    print_obj(\"pgan_model\", \"params\", params)\n",
    "\n",
    "    # Loss function, training/eval ops, etc.\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "\n",
    "    # Create alpha variable to use for weighted sum for smooth fade-in.\n",
    "    alpha_var = tf.get_variable(\n",
    "        name=\"alpha_var\",\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros(shape=[], dtype=tf.float32),\n",
    "        trainable=False\n",
    "    )\n",
    "    print_obj(\"pgan_model\", \"alpha_var\", alpha_var)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Extract given latent vectors from features dictionary.\n",
    "        Z = tf.cast(x=features[\"Z\"], dtype=tf.float32)\n",
    "\n",
    "        # Get predictions from generator.\n",
    "        generated_images = generator_network(Z, alpha_var, params)\n",
    "\n",
    "        # Create predictions dictionary.\n",
    "        predictions_dict = {\n",
    "            \"generated_images\": generated_images\n",
    "        }\n",
    "\n",
    "        # Create export outputs.\n",
    "        export_outputs = {\n",
    "            \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "                outputs=predictions_dict)\n",
    "        }\n",
    "    else:\n",
    "        # Extract image from features dictionary.\n",
    "        X = features[\"image\"]\n",
    "\n",
    "        # Get dynamic batch size in case of partial batch.\n",
    "        cur_batch_size = tf.shape(\n",
    "            input=X,\n",
    "            out_type=tf.int32,\n",
    "            name=\"pgan_model_cur_batch_size\"\n",
    "        )[0]\n",
    "\n",
    "        # Create random noise latent vector for each batch example.\n",
    "        Z = tf.random.normal(\n",
    "            shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Establish generator network subgraph with gaussian noise.\n",
    "        print(\"\\nCall generator with Z = {}.\".format(Z))\n",
    "        generator_outputs = generator_network(Z, alpha_var, params)\n",
    "\n",
    "        # Resize real images based on the current size of the GAN.\n",
    "        real_image = resize_real_images(X, params)\n",
    "\n",
    "        # Establish discriminator network subgraph with real data.\n",
    "        print(\"\\nCall discriminator with real_image = {}.\".format(\n",
    "            real_image\n",
    "        ))\n",
    "\n",
    "        real_logits = discriminator_network(\n",
    "            real_image, alpha_var, params\n",
    "        )\n",
    "\n",
    "        # Get generated logits too.\n",
    "        print(\"\\nCall discriminator with generator_outputs = {}.\".format(\n",
    "            generator_outputs\n",
    "        ))\n",
    "\n",
    "        generated_logits = discriminator_network(\n",
    "            generator_outputs, alpha_var, params\n",
    "        )\n",
    "\n",
    "        # Get generator total loss.\n",
    "        generator_total_loss = get_generator_loss(generated_logits)\n",
    "\n",
    "        # Get discriminator total loss.\n",
    "        discriminator_total_loss = get_discriminator_loss(\n",
    "            generated_logits, real_logits, params\n",
    "        )\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Get global step.\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "            # Determine if it is time to train generator or discriminator.\n",
    "            cycle_step = tf.mod(\n",
    "                x=global_step,\n",
    "                y=tf.cast(\n",
    "                    x=tf.add(\n",
    "                        x=params[\"generator_train_steps\"],\n",
    "                        y=params[\"discriminator_train_steps\"]\n",
    "                    ),\n",
    "                    dtype=tf.int64\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create choose generator condition.\n",
    "            condition = tf.less(\n",
    "                x=cycle_step, y=params[\"generator_train_steps\"]\n",
    "            )\n",
    "\n",
    "            # Needed for batch normalization, but has no effect otherwise.\n",
    "            update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(control_inputs=update_ops):\n",
    "                # Conditionally choose to train generator or discriminator.\n",
    "                loss, train_op = tf.cond(\n",
    "                    pred=condition,\n",
    "                    true_fn=lambda: train_network(\n",
    "                        loss=generator_total_loss,\n",
    "                        global_step=global_step,\n",
    "                        alpha_var=alpha_var,\n",
    "                        params=params,\n",
    "                        scope=\"generator\"\n",
    "                    ),\n",
    "                    false_fn=lambda: train_network(\n",
    "                        loss=discriminator_total_loss,\n",
    "                        global_step=global_step,\n",
    "                        alpha_var=alpha_var,\n",
    "                        params=params,\n",
    "                        scope=\"discriminator\"\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            loss = discriminator_total_loss\n",
    "\n",
    "            # Concatenate discriminator logits and labels.\n",
    "            discriminator_logits = tf.concat(\n",
    "                values=[real_logits, generated_logits],\n",
    "                axis=0,\n",
    "                name=\"discriminator_concat_logits\"\n",
    "            )\n",
    "\n",
    "            discriminator_labels = tf.concat(\n",
    "                values=[\n",
    "                    tf.ones_like(tensor=real_logits),\n",
    "                    tf.zeros_like(tensor=generated_logits)\n",
    "                ],\n",
    "                axis=0,\n",
    "                name=\"discriminator_concat_labels\"\n",
    "            )\n",
    "\n",
    "            # Calculate discriminator probabilities.\n",
    "            discriminator_probabilities = tf.nn.sigmoid(\n",
    "                x=discriminator_logits, name=\"discriminator_probabilities\"\n",
    "            )\n",
    "\n",
    "            # Create eval metric ops dictionary.\n",
    "            eval_metric_ops = {\n",
    "                \"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    name=\"pgan_model_accuracy\"\n",
    "                ),\n",
    "                \"precision\": tf.metrics.precision(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    name=\"pgan_model_precision\"\n",
    "                ),\n",
    "                \"recall\": tf.metrics.recall(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    name=\"pgan_model_recall\"\n",
    "                ),\n",
    "                \"auc_roc\": tf.metrics.auc(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    num_thresholds=200,\n",
    "                    curve=\"ROC\",\n",
    "                    name=\"pgan_model_auc_roc\"\n",
    "                ),\n",
    "                \"auc_pr\": tf.metrics.auc(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    num_thresholds=200,\n",
    "                    curve=\"PR\",\n",
    "                    name=\"pgan_model_auc_pr\"\n",
    "                )\n",
    "            }\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs=export_outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn(params):\n",
    "    \"\"\"Serving input function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        ServingInputReceiver object containing features and receiver tensors.\n",
    "    \"\"\"\n",
    "    # Create placeholders to accept data sent to the model at serving time.\n",
    "    # shape = (batch_size,)\n",
    "    feature_placeholders = {\n",
    "        \"Z\": tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, params[\"latent_size\"]],\n",
    "            name=\"serving_input_placeholder_Z\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"feature_placeholders\",\n",
    "        feature_placeholders\n",
    "    )\n",
    "\n",
    "    # Create clones of the feature placeholder tensors so that the SavedModel\n",
    "    # SignatureDef will point to the placeholder.\n",
    "    features = {\n",
    "        key: tf.identity(\n",
    "            input=value,\n",
    "            name=\"serving_input_fn_identity_placeholder_{}\".format(key)\n",
    "        )\n",
    "        for key, value in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"features\",\n",
    "        features\n",
    "    )\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=feature_placeholders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates custom Estimator model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        `Estimator` object.\n",
    "    \"\"\"\n",
    "    # Set logging to be level of INFO.\n",
    "    tf.logging.set_verbosity(v=tf.logging.INFO)\n",
    "\n",
    "    # Create our custom estimator using our model function.\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=pgan_model,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        params=args\n",
    "    )\n",
    "\n",
    "    # Create train spec to read in our training data.\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset(\n",
    "            filename=args[\"train_file_pattern\"],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size=args[\"train_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        max_steps=args[\"train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Create exporter to save out the complete model to disk.\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name=\"exporter\",\n",
    "        serving_input_receiver_fn=lambda: serving_input_fn(args)\n",
    "    )\n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model.\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset(\n",
    "            filename=args[\"eval_file_pattern\"],\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size=args[\"eval_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        steps=args[\"eval_steps\"],\n",
    "        start_delay_secs=args[\"start_delay_secs\"],\n",
    "        throttle_secs=args[\"throttle_secs\"],\n",
    "        exporters=exporter\n",
    "    )\n",
    "\n",
    "    # Create train and evaluate loop to train and evaluate our estimator.\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f00a1dfa4d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:87: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/shuffle_ops.py:54: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:99: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "\n",
      "decode_example: features = {'image_raw': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "decode_example: image = Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "decode_example: image = Tensor(\"Reshape:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "decode_example: image = Tensor(\"sub:0\", shape=(32, 32, 3), dtype=float32)\n",
      "decode_example: label = Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:107: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "pgan_model: features = {'image': <tf.Tensor 'IteratorGetNext:0' shape=(?, 32, 32, 3) dtype=float32>}\n",
      "pgan_model: labels = Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "pgan_model: mode = train\n",
      "pgan_model: params = {'train_file_pattern': 'data/train.tfrecord', 'eval_file_pattern': 'data/eval.tfrecord', 'output_dir': 'trained_model', 'train_batch_size': 32, 'train_steps': 400, 'eval_batch_size': 32, 'eval_steps': 10, 'start_delay_secs': 600, 'throttle_secs': 600, 'height': 32, 'width': 32, 'depth': 3, 'base_num_filters': [512, 512], 'base_kernel_sizes': [4, 3], 'base_strides': [1, 1], 'num_steps_until_growth': 100, 'growth_num_filters': [[512, 512], [512, 512]], 'growth_kernel_sizes': [[3, 3], [3, 3]], 'growth_strides': [[1, 1], [1, 1]], 'latent_size': 512, 'generator_projection_dims': [4, 4, 512], 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_optimizer': 'Adam', 'generator_learning_rate': 0.0001, 'generator_clip_gradients': 5.0, 'generator_train_steps': 1, 'discriminator_l1_regularization_scale': 0.01, 'discriminator_l2_regularization_scale': 0.01, 'discriminator_optimizer': 'Adam', 'discriminator_learning_rate': 0.0001, 'discriminator_clip_gradients': 5.0, 'discriminator_gradient_penalty_coefficient': 10.0, 'discriminator_train_steps': 5}\n",
      "pgan_model: alpha_var = <tf.Variable 'alpha_var:0' shape=() dtype=float32_ref>\n",
      "\n",
      "Call generator with Z = Tensor(\"random_normal:0\", shape=(?, 512), dtype=float32).\n",
      "\n",
      "generator_network: Z = Tensor(\"random_normal:0\", shape=(?, 512), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-5c254a6a8fc5>:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "generator_projection: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 8192), dtype=float32)\n",
      "generator_network: projection = Tensor(\"projection_reshaped:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "generator_network: projection = Tensor(\"projection_reshaped:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "\n",
      "create_generator_base_conv_layer_block: base_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089397c50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089397590>]\n",
      "\n",
      "create_generator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089397610>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008938ea90>]\n",
      "\n",
      "create_generator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089359e50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bd3d0>]\n",
      "generator_network: blocks = [[<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089397c50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089397590>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089397610>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008938ea90>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089359e50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bd3d0>]]\n",
      "\n",
      "create_generator_to_rgb_layers: to_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bd210>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bded0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bdb50>]\n",
      "generator_network: to_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bd210>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bded0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00893bdb50>]\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "create_base_generator_network: to_rgb_conv = Tensor(\"base_to_rgb_layers_conv2d/LeakyRelu:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_1/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_base_generator_network: weighted_sum = Tensor(\"growth_transition_weighted_sum_0:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_1/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_2/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_base_generator_network: weighted_sum = Tensor(\"growth_transition_weighted_sum_1:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_1/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_2/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_final_generator_network: to_rgb_conv = Tensor(\"growth_to_rgb_layers_conv2d_1/LeakyRelu:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator_switch_case_generated_outputs/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "resize_real_images: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "\n",
      "resize_real_image: block_idx = 0\n",
      "resize_real_image: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "resize_real_images: resized_image = Tensor(\"resize_real_images_resized_image_0/ResizeNearestNeighbor:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "\n",
      "resize_real_image: block_idx = 1\n",
      "resize_real_image: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "resize_real_images: resized_image = Tensor(\"resize_real_images_resized_image_1/ResizeNearestNeighbor:0\", shape=(?, 8, 8, 3), dtype=float32)\n",
      "\n",
      "resize_real_image: block_idx = 2\n",
      "resize_real_image: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "resize_real_images: resized_image = Tensor(\"resize_real_images_resized_image_2/ResizeNearestNeighbor:0\", shape=(?, 16, 16, 3), dtype=float32)\n",
      "resize_real_images: selected resized_image = Tensor(\"resize_real_images_switch_case_resized_image/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "Call discriminator with real_image = Tensor(\"resize_real_images_switch_case_resized_image/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32).\n",
      "\n",
      "discriminator_network: X = Tensor(\"resize_real_images_switch_case_resized_image/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "create_discriminator_from_rgb_layers: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00892cef90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008925b910>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278690>]\n",
      "discriminator_network: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00892cef90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008925b910>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278690>]\n",
      "\n",
      "create_discriminator_base_conv_layer_block: base_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278a90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278c10>]\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278e50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278fd0>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089278e10>\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089284310>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089284490>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089284610>\n",
      "discriminator_network: blocks = [[<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278a90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278c10>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278e50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089278fd0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089278e10>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089284310>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089284490>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089284610>]]\n",
      "\n",
      "create_discriminator_growth_transition_downsample_layers: downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f00892847d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089284690>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089284950>]\n",
      "discriminator_network: transition_downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f00892847d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089284690>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089284950>]\n",
      "\n",
      "create_base_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_final_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "discriminator_network: block_conv = Tensor(\"discriminator_switch_case_block_conv/indexed_case/Identity:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "discriminator_network: block_conv_flat = Tensor(\"discriminator_5/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "create_growth_transition_discriminator_network: logits = Tensor(\"discriminator_5/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "Call discriminator with generator_outputs = Tensor(\"generator_switch_case_generated_outputs/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32).\n",
      "\n",
      "discriminator_network: X = Tensor(\"generator_switch_case_generated_outputs/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "create_discriminator_from_rgb_layers: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891cb750>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891cbd10>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891effd0>]\n",
      "discriminator_network: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891cb750>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891cbd10>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891effd0>]\n",
      "\n",
      "create_discriminator_base_conv_layer_block: base_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891e0b90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891f8190>]\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089190290>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089196150>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f008934a250>\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089188ad0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891ba5d0>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089188bd0>\n",
      "discriminator_network: blocks = [[<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891e0b90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891f8190>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089190290>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089196150>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f008934a250>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0089188ad0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00891ba5d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f0089188bd0>]]\n",
      "\n",
      "create_discriminator_growth_transition_downsample_layers: downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f008925b0d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f00891a4ed0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f00892784d0>]\n",
      "discriminator_network: transition_downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f008925b0d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f00891a4ed0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7f00892784d0>]\n",
      "\n",
      "create_base_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_final_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "discriminator_network: block_conv = Tensor(\"discriminator_switch_case_block_conv_1/indexed_case/Identity:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "discriminator_network: block_conv_flat = Tensor(\"discriminator_11/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "create_growth_transition_discriminator_network: logits = Tensor(\"discriminator_11/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "get_generator_loss: generator_loss = Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_regularization_loss = Tensor(\"generator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_total_loss = Tensor(\"generator_total_loss:0\", shape=(), dtype=float32)\n",
      "\n",
      "get_discriminator_loss: discriminator_real_loss = Tensor(\"discriminator_real_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_generated_loss = Tensor(\"discriminator_generated_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_loss = Tensor(\"discriminator_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_regularization_loss = Tensor(\"discriminator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_total_loss = Tensor(\"discriminator_total_loss:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 2235.7012, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.56209\n",
      "INFO:tensorflow:loss = 130404.375, step = 101 (39.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.15809\n",
      "INFO:tensorflow:loss = 47384.1, step = 201 (86.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.464376\n",
      "INFO:tensorflow:loss = 2012.4314, step = 301 (215.339 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into trained_model/model.ckpt.\n",
      "\n",
      "decode_example: features = {'image_raw': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "decode_example: image = Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "decode_example: image = Tensor(\"Reshape:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "decode_example: image = Tensor(\"sub:0\", shape=(32, 32, 3), dtype=float32)\n",
      "decode_example: label = Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "pgan_model: features = {'image': <tf.Tensor 'IteratorGetNext:0' shape=(?, 32, 32, 3) dtype=float32>}\n",
      "pgan_model: labels = Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "pgan_model: mode = eval\n",
      "pgan_model: params = {'train_file_pattern': 'data/train.tfrecord', 'eval_file_pattern': 'data/eval.tfrecord', 'output_dir': 'trained_model', 'train_batch_size': 32, 'train_steps': 400, 'eval_batch_size': 32, 'eval_steps': 10, 'start_delay_secs': 600, 'throttle_secs': 600, 'height': 32, 'width': 32, 'depth': 3, 'base_num_filters': [512, 512], 'base_kernel_sizes': [4, 3], 'base_strides': [1, 1], 'num_steps_until_growth': 100, 'growth_num_filters': [[512, 512], [512, 512]], 'growth_kernel_sizes': [[3, 3], [3, 3]], 'growth_strides': [[1, 1], [1, 1]], 'latent_size': 512, 'generator_projection_dims': [4, 4, 512], 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_optimizer': 'Adam', 'generator_learning_rate': 0.0001, 'generator_clip_gradients': 5.0, 'generator_train_steps': 1, 'discriminator_l1_regularization_scale': 0.01, 'discriminator_l2_regularization_scale': 0.01, 'discriminator_optimizer': 'Adam', 'discriminator_learning_rate': 0.0001, 'discriminator_clip_gradients': 5.0, 'discriminator_gradient_penalty_coefficient': 10.0, 'discriminator_train_steps': 5}\n",
      "pgan_model: alpha_var = <tf.Variable 'alpha_var:0' shape=() dtype=float32_ref>\n",
      "\n",
      "Call generator with Z = Tensor(\"random_normal:0\", shape=(?, 512), dtype=float32).\n",
      "\n",
      "generator_network: Z = Tensor(\"random_normal:0\", shape=(?, 512), dtype=float32)\n",
      "generator_projection: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 8192), dtype=float32)\n",
      "generator_network: projection = Tensor(\"projection_reshaped:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "generator_network: projection = Tensor(\"projection_reshaped:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "\n",
      "create_generator_base_conv_layer_block: base_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00787a1250>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00787a1a90>]\n",
      "\n",
      "create_generator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f008807d410>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083410>]\n",
      "\n",
      "create_generator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083bd0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083a50>]\n",
      "generator_network: blocks = [[<tensorflow.python.layers.convolutional.Conv2D object at 0x7f00787a1250>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f00787a1a90>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f008807d410>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083410>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083bd0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083a50>]]\n",
      "\n",
      "create_generator_to_rgb_layers: to_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083ed0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008808a0d0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008808a250>]\n",
      "generator_network: to_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7f0088083ed0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008808a0d0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7f008808a250>]\n",
      "\n",
      "create_base_generator_network: to_rgb_conv = Tensor(\"base_to_rgb_layers_conv2d/LeakyRelu:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_1/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_base_generator_network: weighted_sum = Tensor(\"growth_transition_weighted_sum_0:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_1/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_2/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_base_generator_network: weighted_sum = Tensor(\"growth_transition_weighted_sum_1:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_1/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "upsample_generator_image: upsampled_image = Tensor(\"growth_upsampled_image_2/ResizeNearestNeighbor:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_final_generator_network: to_rgb_conv = Tensor(\"growth_to_rgb_layers_conv2d_1/LeakyRelu:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator_switch_case_generated_outputs/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "resize_real_images: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "\n",
      "resize_real_image: block_idx = 0\n",
      "resize_real_image: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "resize_real_images: resized_image = Tensor(\"resize_real_images_resized_image_0/ResizeNearestNeighbor:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "\n",
      "resize_real_image: block_idx = 1\n",
      "resize_real_image: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "resize_real_images: resized_image = Tensor(\"resize_real_images_resized_image_1/ResizeNearestNeighbor:0\", shape=(?, 8, 8, 3), dtype=float32)\n",
      "\n",
      "resize_real_image: block_idx = 2\n",
      "resize_real_image: image = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "resize_real_images: resized_image = Tensor(\"resize_real_images_resized_image_2/ResizeNearestNeighbor:0\", shape=(?, 16, 16, 3), dtype=float32)\n",
      "resize_real_images: selected resized_image = Tensor(\"resize_real_images_switch_case_resized_image/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "Call discriminator with real_image = Tensor(\"resize_real_images_switch_case_resized_image/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32).\n",
      "\n",
      "discriminator_network: X = Tensor(\"resize_real_images_switch_case_resized_image/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "create_discriminator_from_rgb_layers: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86bac10>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86bad50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86bac50>]\n",
      "discriminator_network: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86bac10>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86bad50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86bac50>]\n",
      "\n",
      "create_discriminator_base_conv_layer_block: base_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3310>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c34d0>]\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c37d0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3950>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c3bd0>\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3e50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3fd0>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca290>\n",
      "discriminator_network: blocks = [[<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3310>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c34d0>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c37d0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3950>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c3bd0>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3e50>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86c3fd0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca290>]]\n",
      "\n",
      "create_discriminator_growth_transition_downsample_layers: downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca550>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca690>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca7d0>]\n",
      "discriminator_network: transition_downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca550>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca690>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86ca7d0>]\n",
      "\n",
      "create_base_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_final_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "discriminator_network: block_conv = Tensor(\"discriminator_switch_case_block_conv/indexed_case/Identity:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "discriminator_network: block_conv_flat = Tensor(\"discriminator_5/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "create_growth_transition_discriminator_network: logits = Tensor(\"discriminator_5/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "Call discriminator with generator_outputs = Tensor(\"generator_switch_case_generated_outputs/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32).\n",
      "\n",
      "discriminator_network: X = Tensor(\"generator_switch_case_generated_outputs/indexed_case/Identity:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "\n",
      "create_discriminator_from_rgb_layers: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86e0e10>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86d5110>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8690e90>]\n",
      "discriminator_network: from_rgb_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86e0e10>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa86d5110>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8690e90>]\n",
      "\n",
      "create_discriminator_base_conv_layer_block: base_conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8679f90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8696c10>]\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8646890>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa863c550>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa85c2f50>\n",
      "\n",
      "create_discriminator_growth_layer_block: conv_layers = [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa85f2ad0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8607050>]\n",
      "create_discriminator_growth_layer_block: downsampled_image_layer = <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c33d0>\n",
      "discriminator_network: blocks = [[<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8679f90>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8696c10>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8646890>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa863c550>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa85c2f50>], [<tensorflow.python.layers.convolutional.Conv2D object at 0x7effa85f2ad0>, <tensorflow.python.layers.convolutional.Conv2D object at 0x7effa8607050>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c33d0>]]\n",
      "\n",
      "create_discriminator_growth_transition_downsample_layers: downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c38d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c39d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c3c90>]\n",
      "discriminator_network: transition_downsample_layers = [<tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c38d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c39d0>, <tensorflow.python.layers.pooling.AveragePooling2D object at 0x7effa86c3c90>]\n",
      "\n",
      "create_base_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_growth_transition_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "\n",
      "create_final_discriminator_network: block_conv = Tensor(\"base_layers_conv2d_0/LeakyRelu:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "discriminator_network: block_conv = Tensor(\"discriminator_switch_case_block_conv_1/indexed_case/Identity:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "discriminator_network: block_conv_flat = Tensor(\"discriminator_11/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "create_growth_transition_discriminator_network: logits = Tensor(\"discriminator_11/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "get_generator_loss: generator_loss = Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_regularization_loss = Tensor(\"generator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_total_loss = Tensor(\"generator_total_loss:0\", shape=(), dtype=float32)\n",
      "\n",
      "get_discriminator_loss: discriminator_real_loss = Tensor(\"discriminator_real_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_generated_loss = Tensor(\"discriminator_generated_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_loss = Tensor(\"discriminator_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_regularization_loss = Tensor(\"discriminator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_total_loss = Tensor(\"discriminator_total_loss:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-06-02T06:34:22Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(path=arguments[\"output_dir\"], ignore_errors=True)\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'trained_model/export/exporter': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls trained_model/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_fn = tf.contrib.predictor.from_saved_model(\n",
    "#     \"trained_model/export/exporter/1590991040\"\n",
    "# )\n",
    "# predictions = predict_fn(\n",
    "#     {\n",
    "#         \"Z\": np.random.normal(size=(500, 512))\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_images = np.clip(\n",
    "#     a=((predictions[\"generated_images\"] + 1.0) * (255. / 2)).astype(np.int32),\n",
    "#     a_min=0,\n",
    "#     a_max=255\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generated_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(5):\n",
    "#     plt.subplot(1, 5, i + 1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(generated_images[i], cmap=plt.cm.binary)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
