{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model module GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_num_filters = 512,512;512,512;512,512;512,512;256,256\n",
      "conv_kernel_sizes = 4,3;3,3;3,3;3,3;3,3\n",
      "conv_strides = 1,1;1,1;1,1;1,1;1,1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "def convert_conv_layer_property_lists_to_string(property_list, prop_list_len):\n",
    "    \"\"\"Convert conv layer property list to string.\n",
    "\n",
    "    Args:\n",
    "        property_list: list, nested list of blocks of a conv layer property.\n",
    "        prop_list_len: int, length of list to process.\n",
    "\n",
    "    Returns:\n",
    "        Doubly delimited string of conv layer property values.\n",
    "    \"\"\"\n",
    "    return (\";\").join(\n",
    "        [\n",
    "            (\",\").join([str(val) for val in block])\n",
    "            for block in property_list[0:prop_list_len]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "# Import os environment variables for file hyperparameters.\n",
    "os.environ[\"TRAIN_FILE_PATTERN\"] = \"gs://machine-learning-1234-bucket/gan/pgan/data/train*.tfrecord\"\n",
    "os.environ[\"EVAL_FILE_PATTERN\"] = \"gs://machine-learning-1234-bucket/gan/pgan/data/test*.tfrecord\"\n",
    "os.environ[\"OUTPUT_DIR\"] = \"gs://machine-learning-1234-bucket/gan/pgan/trained_model\"\n",
    "\n",
    "# Import os environment variables for train hyperparameters.\n",
    "os.environ[\"TRAIN_BATCH_SIZE\"] = str(32)\n",
    "os.environ[\"TRAIN_STEPS\"] = str(30000)\n",
    "\n",
    "# Import os environment variables for eval hyperparameters.\n",
    "os.environ[\"EVAL_BATCH_SIZE\"] = str(32)\n",
    "os.environ[\"EVAL_STEPS\"] = str(1)\n",
    "os.environ[\"START_DELAY_SECS\"] = str(600000)\n",
    "os.environ[\"THROTTLE_SECS\"] = str(600000)\n",
    "\n",
    "# Import os environment variables for image hyperparameters.\n",
    "os.environ[\"HEIGHT\"] = str(32)\n",
    "os.environ[\"WIDTH\"] = str(32)\n",
    "os.environ[\"DEPTH\"] = str(3)\n",
    "\n",
    "# Import os environment variables for shared hyperparameters.\n",
    "os.environ[\"NUM_STEPS_UNTIL_GROWTH\"] = str(2000)\n",
    "\n",
    "# Full lists for full 1024x1024 network growth.\n",
    "full_conv_num_filters = [[512, 512], [512, 512], [512, 512], [512, 512], [256, 256], [128, 128], [64, 64], [32, 32], [16, 16]]\n",
    "full_conv_kernel_sizes = [[4, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]\n",
    "full_conv_strides = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n",
    "\n",
    "# Set final image size as a multiple of 2, starting at 4.\n",
    "image_size = 64\n",
    "prop_list_len = max(\n",
    "    min(int(math.log(image_size, 2) - 1), len(full_conv_num_filters)), 1\n",
    ")\n",
    "\n",
    "# Get slices of lists.\n",
    "conv_num_filters = convert_conv_layer_property_lists_to_string(\n",
    "    full_conv_num_filters, prop_list_len\n",
    ")\n",
    "print(\"conv_num_filters = {}\".format(conv_num_filters))\n",
    "conv_kernel_sizes = convert_conv_layer_property_lists_to_string(\n",
    "    full_conv_kernel_sizes, prop_list_len\n",
    ")\n",
    "print(\"conv_kernel_sizes = {}\".format(conv_kernel_sizes))\n",
    "conv_strides = convert_conv_layer_property_lists_to_string(\n",
    "    full_conv_strides, prop_list_len\n",
    ")\n",
    "print(\"conv_strides = {}\".format(conv_strides))\n",
    "\n",
    "os.environ[\"CONV_NUM_FILTERS\"] = conv_num_filters\n",
    "os.environ[\"CONV_KERNEL_SIZES\"] = conv_kernel_sizes\n",
    "os.environ[\"CONV_STRIDES\"] = conv_strides\n",
    "\n",
    "# Import os environment variables for generator hyperparameters.\n",
    "os.environ[\"LATENT_SIZE\"] = str(512)\n",
    "os.environ[\"GENERATOR_PROJECTION_DIMS\"] = \"4,4,512\"\n",
    "os.environ[\"GENERATOR_L1_REGULARIZATION_SCALE\"] = str(0.01)\n",
    "os.environ[\"GENERATOR_L2_REGULARIZATION_SCALE\"] = str(0.01)\n",
    "os.environ[\"GENERATOR_OPTIMIZER\"] = \"Adam\"\n",
    "os.environ[\"GENERATOR_LEARNING_RATE\"] = str(0.0001)\n",
    "os.environ[\"GENERATOR_CLIP_GRADIENTS\"] = str(5.0)\n",
    "os.environ[\"GENERATOR_TRAIN_STEPS\"] = str(1)\n",
    "\n",
    "# Import os environment variables for discriminator hyperparameters.\n",
    "os.environ[\"DISCRIMINATOR_L1_REGULARIZATION_SCALE\"] = str(0.01)\n",
    "os.environ[\"DISCRIMINATOR_L2_REGULARIZATION_SCALE\"] = str(0.01)\n",
    "os.environ[\"DISCRIMINATOR_OPTIMIZER\"] = \"Adam\"\n",
    "os.environ[\"DISCRIMINATOR_LEARNING_RATE\"] = str(0.0001)\n",
    "os.environ[\"DISCRIMINATOR_CLIP_GRADIENTS\"] = str(5.0)\n",
    "os.environ[\"DISCRIMINATOR_GRADIENT_PENALTY_COEFFICIENT\"] = str(10.0)\n",
    "os.environ[\"DISCRIMINATOR_TRAIN_STEPS\"] = str(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"machine-learning-1234\"\n",
    "BUCKET = \"machine-learning-1234-bucket\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Import os environment variables\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] =  BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train pGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://machine-learning-1234-bucket/gan/pgan/trained_model us-central1 pgan_64x64_200603_111313\n",
      "jobId: pgan_64x64_200603_111313\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "Job [pgan_64x64_200603_111313] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe pgan_64x64_200603_111313\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs pgan_64x64_200603_111313\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JOBNAME=pgan_64x64_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTPUT_DIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTPUT_DIR}\n",
    "gcloud ml-engine jobs submit training ${JOBNAME} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$PWD/pgan_module/trainer \\\n",
    "    --job-dir=${OUTPUT_DIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --scale-tier=PREMIUM_1 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    -- \\\n",
    "    --train_file_pattern=${TRAIN_FILE_PATTERN} \\\n",
    "    --eval_file_pattern=${EVAL_FILE_PATTERN} \\\n",
    "    --output_dir=${OUTPUT_DIR} \\\n",
    "    --job-dir=./tmp \\\n",
    "    \\\n",
    "    --train_batch_size=${TRAIN_BATCH_SIZE} \\\n",
    "    --train_steps=${TRAIN_STEPS} \\\n",
    "    \\\n",
    "    --eval_batch_size=${EVAL_BATCH_SIZE} \\\n",
    "    --eval_steps=${EVAL_STEPS} \\\n",
    "    --start_delay_secs=${START_DELAY_SECS} \\\n",
    "    --throttle_secs=${THROTTLE_SECS} \\\n",
    "    \\\n",
    "    --height=${HEIGHT} \\\n",
    "    --width=${WIDTH} \\\n",
    "    --depth=${DEPTH} \\\n",
    "    \\\n",
    "    --num_steps_until_growth=${NUM_STEPS_UNTIL_GROWTH} \\\n",
    "    --conv_num_filters=${CONV_NUM_FILTERS} \\\n",
    "    --conv_kernel_sizes=${CONV_KERNEL_SIZES} \\\n",
    "    --conv_strides=${CONV_STRIDES} \\\n",
    "    \\\n",
    "    --latent_size=${LATENT_SIZE} \\\n",
    "    --generator_projection_dims=${GENERATOR_PROJECTION_DIMS} \\\n",
    "    --generator_l1_regularization_scale=${GENERATOR_L1_REGULARIZATION_SCALE} \\\n",
    "    --generator_l2_regularization_scale=${GENERATOR_L2_REGULARIZATION_SCALE} \\\n",
    "    --generator_optimizer=${GENERATOR_OPTIMIZER} \\\n",
    "    --generator_learning_rate=${GENERATOR_LEARNING_RATE} \\\n",
    "    --generator_clip_gradients=${GENERATOR_CLIP_GRADIENTS} \\\n",
    "    --generator_train_steps=${GENERATOR_TRAIN_STEPS} \\\n",
    "    \\\n",
    "    --discriminator_l1_regularization_scale=${DISCRIMINATOR_L1_REGULARIZATION_SCALE} \\\n",
    "    --discriminator_l2_regularization_scale=${DISCRIMINATOR_L2_REGULARIZATION_SCALE} \\\n",
    "    --discriminator_optimizer=${DISCRIMINATOR_OPTIMIZER} \\\n",
    "    --discriminator_learning_rate=${DISCRIMINATOR_LEARNING_RATE} \\\n",
    "    --discriminator_clip_gradients=${DISCRIMINATOR_CLIP_GRADIENTS} \\\n",
    "    --discriminator_gradient_penalty_coefficient=${DISCRIMINATOR_GRADIENT_PENALTY_COEFFICIENT} \\\n",
    "    --discriminator_train_steps=${DISCRIMINATOR_TRAIN_STEPS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
