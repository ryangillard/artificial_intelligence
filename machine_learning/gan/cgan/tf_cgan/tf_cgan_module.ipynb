{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/print_object.py\n",
    "def print_obj(function_name, object_name, object_value):\n",
    "    \"\"\"Prints enclosing function, object name, and object value.\n",
    "\n",
    "    Args:\n",
    "        function_name: str, name of function.\n",
    "        object_name: str, name of object.\n",
    "        object_value: object, value of passed object.\n",
    "    \"\"\"\n",
    "#     pass\n",
    "    print(\"{}: {} = {}\".format(function_name, object_name, object_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/input.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def preprocess_image(image, params):\n",
    "    \"\"\"Preprocess image tensor.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, input image with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image tensor with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "    \"\"\"\n",
    "    func_name = \"preprocess_image\"\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    func_name = \"decode_example\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "    print_obj(\"\\n\" + func_name, \"features\", features)\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    # Preprocess image.\n",
    "    image = preprocess_image(image=image, params=params)\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "    print_obj(func_name, \"label\", label)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    \"\"\"Reads CSV time series data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read CSV dataset\n",
    "    using Dataset API, apply necessary preprocessing, and return an input\n",
    "    function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        mode: The estimator ModeKeys. Can be TRAIN or EVAL.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create list of files that match pattern.\n",
    "        file_list = tf.gfile.Glob(filename=filename)\n",
    "\n",
    "        # Create dataset from file list.\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=file_list, num_parallel_reads=tf.contrib.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        # Shuffle and repeat if training with fused op.\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(\n",
    "                    buffer_size=50 * batch_size,\n",
    "                    count=None  # indefinitely\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decode CSV file into a features dictionary of tensors, then batch.\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                map_func=lambda x: decode_example(\n",
    "                    protos=x,\n",
    "                    params=params\n",
    "                ),\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=tf.contrib.data.AUTOTUNE\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\n",
    "\n",
    "        # Create a iterator, then get batch of features from example queue.\n",
    "        batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return batched_dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## networks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/networks.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    \"\"\"Network that could be for generator or discriminator.\n",
    "    Fields:\n",
    "        name: str, name of `Generator` or `Discriminator`.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, name):\n",
    "        \"\"\"Instantiates network.\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of generator or discriminator.\n",
    "        \"\"\"\n",
    "        # Set name of generator.\n",
    "        self.name = name\n",
    "\n",
    "        # Regularizer for kernel weights.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        # Regularizer for bias weights.\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "    def embed_labels(self, labels, params, scope):\n",
    "        \"\"\"Embeds labels from integer indices to float vectors.\n",
    "\n",
    "        Args:\n",
    "            labels: tensor, labels to condition on of shape\n",
    "                [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "            scope: str, variable scope.\n",
    "\n",
    "        Returns:\n",
    "            Embedded labels tensor of shape\n",
    "                [cur_batch_size, label_embedding_dimension].\n",
    "        \"\"\"\n",
    "        func_name = \"{}_embed_labels\".format(scope)\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            # Create trainable label embedding matrix.\n",
    "            label_embedding_matrix = tf.get_variable(\n",
    "                name=\"label_embedding_matrix\",\n",
    "                shape=[\n",
    "                    params[\"num_classes\"],\n",
    "                    params[\"label_embedding_dimension\"]\n",
    "                ],\n",
    "                dtype=tf.float32,\n",
    "                initializer=None,\n",
    "                regularizer=None,\n",
    "                trainable=True\n",
    "            )\n",
    "\n",
    "            # Get embedding vectors for integer label index.\n",
    "            label_embeddings = tf.nn.embedding_lookup(\n",
    "                params=label_embedding_matrix,\n",
    "                ids=labels,\n",
    "                name=\"embedding_lookup\"\n",
    "            )\n",
    "\n",
    "            # Flatten back into a rank 2 tensor.\n",
    "            label_vectors = tf.reshape(\n",
    "                tensor=label_embeddings,\n",
    "                shape=[-1, params[\"label_embedding_dimension\"]],\n",
    "                name=\"label_vectors\"\n",
    "            )\n",
    "            print_obj(func_name, \"label_vectors\", label_vectors)\n",
    "\n",
    "        return label_vectors\n",
    "\n",
    "    def use_labels(self, features, labels, params, scope):\n",
    "        \"\"\"Conditions features using label data.\n",
    "\n",
    "        Args:\n",
    "            features: tensor, features tensor, either Z for generator or X for\n",
    "                discriminator.\n",
    "            labels: tensor, labels to condition on of shape\n",
    "                [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "            scope: str, variable scope.\n",
    "\n",
    "        Returns:\n",
    "            Feature tensor conditioned on labels.\n",
    "        \"\"\"\n",
    "        func_name = \"{}_use_labels\".format(scope)\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            if params[\"{}_embed_labels\".format(scope)]:\n",
    "                label_vectors = self.embed_labels(\n",
    "                    labels=labels, params=params, scope=scope\n",
    "                )\n",
    "            else:\n",
    "                label_vectors = tf.one_hot(\n",
    "                    indices=tf.squeeze(input=labels, axis=-1),\n",
    "                    depth=params[\"num_classes\"],\n",
    "                    axis=-1,\n",
    "                    name=\"label_vectors_one_hot\"\n",
    "                )\n",
    "            print_obj(func_name, \"label_vectors\", label_vectors)\n",
    "\n",
    "            if params[\"{}_concatenate_labels\".format(scope)]:\n",
    "                if params[\"{}_dense_before_concatenate\".format(scope)]:\n",
    "                    label_vectors = tf.layers.dense(\n",
    "                        inputs=label_vectors,\n",
    "                        units=params[\"label_embedding_dimension\"],\n",
    "                        activation=None,\n",
    "                        kernel_regularizer=self.kernel_regularizer,\n",
    "                        bias_regularizer=self.bias_regularizer,\n",
    "                        name=\"labels_dense_concat\"\n",
    "                    )\n",
    "\n",
    "                # Concatenate labels & features along feature map dimension.\n",
    "                network = tf.concat(\n",
    "                    values=[features, label_vectors], axis=-1,\n",
    "                    name=\"concat_labels\"\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "            else:\n",
    "                label_vectors = tf.layers.dense(\n",
    "                    inputs=label_vectors,\n",
    "                    units=params[\"latent_size\"],\n",
    "                    activation=None,\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"labels_dense_multiply\"\n",
    "                )\n",
    "                print_obj(func_name, \"label_vectors\", label_vectors)\n",
    "\n",
    "                # Element-wise multiply label vectors with latent vectors.\n",
    "                network = tf.multiply(\n",
    "                    x=features, y=label_vectors, name=\"multiply_labels\"\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/generator.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import networks\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Generator(networks.Network):\n",
    "    \"\"\"Generator that takes latent vector input and outputs image.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, name):\n",
    "        \"\"\"Instantiates and builds generator network.\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of generator.\n",
    "        \"\"\"\n",
    "        # Initialize base class.\n",
    "        super().__init__(kernel_regularizer, bias_regularizer, name)\n",
    "\n",
    "    def get_fake_images(self, Z, labels, params):\n",
    "        \"\"\"Creates generator network and returns generated images.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            labels: tensor, labels to condition on of shape\n",
    "                [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated image tensor of shape\n",
    "                [cur_batch_size, height * width * depth].\n",
    "        \"\"\"\n",
    "        func_name = \"get_fake_images\"\n",
    "        print_obj(\"\\n\" + func_name, \"Z\", Z)\n",
    "        print_obj(func_name, \"labels\", labels)\n",
    "\n",
    "        # Dictionary containing possible final activations.\n",
    "        final_activation_dict = {\n",
    "            \"sigmoid\": tf.nn.sigmoid, \"relu\": tf.nn.relu, \"tanh\": tf.nn.tanh\n",
    "        }\n",
    "\n",
    "        with tf.variable_scope(\n",
    "                name_or_scope=\"generator\", reuse=tf.AUTO_REUSE):\n",
    "            if params[\"generator_use_labels\"]:\n",
    "                network = self.use_labels(\n",
    "                    features=Z,\n",
    "                    labels=labels,\n",
    "                    params=params,\n",
    "                    scope=\"generator\"\n",
    "                )\n",
    "            else:\n",
    "                network = Z\n",
    "\n",
    "            # Add hidden layers with given number of units/neurons per layer.\n",
    "            for i, units in enumerate(params[\"generator_hidden_units\"]):\n",
    "                # shape = (cur_batch_size, generator_hidden_units[i])\n",
    "                network = tf.layers.dense(\n",
    "                    inputs=network,\n",
    "                    units=units,\n",
    "                    activation=None,\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"layers_dense_{}\".format(i)\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                network = tf.nn.leaky_relu(\n",
    "                    features=network,\n",
    "                    alpha=params[\"generator_leaky_relu_alpha\"],\n",
    "                    name=\"leaky_relu_{}\".format(i)\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "            # Final linear layer for outputs.\n",
    "            # shape = (cur_batch_size, height * width * depth)\n",
    "            generated_outputs = tf.layers.dense(\n",
    "                inputs=network,\n",
    "                units=params[\"height\"] * params[\"width\"] * params[\"depth\"],\n",
    "                activation=final_activation_dict.get(\n",
    "                    params[\"generator_final_activation\"].lower(), None\n",
    "                ),\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"layers_dense_generated_outputs\"\n",
    "            )\n",
    "            print_obj(func_name, \"generated_outputs\", generated_outputs)\n",
    "\n",
    "        return generated_outputs\n",
    "\n",
    "    def get_generator_loss(self, fake_logits):\n",
    "        \"\"\"Gets generator loss.\n",
    "\n",
    "        Args:\n",
    "            fake_logits: tensor, shape of\n",
    "                [cur_batch_size, 1].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of generator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        func_name = \"get_generator_loss\"\n",
    "        # Calculate base generator loss.\n",
    "        generator_loss = tf.reduce_mean(\n",
    "            input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=fake_logits,\n",
    "                labels=tf.ones_like(tensor=fake_logits)\n",
    "            ),\n",
    "            name=\"generator_loss\"\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"generator_loss\", generator_loss)\n",
    "\n",
    "        # Get regularization losses.\n",
    "        generator_reg_loss = tf.losses.get_regularization_loss(\n",
    "            scope=\"generator\",\n",
    "            name=\"generator_regularization_loss\"\n",
    "        )\n",
    "        print_obj(func_name, \"generator_reg_loss\", generator_reg_loss)\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        generator_total_loss = tf.math.add(\n",
    "            x=generator_loss,\n",
    "            y=generator_reg_loss,\n",
    "            name=\"generator_total_loss\"\n",
    "        )\n",
    "        print_obj(func_name, \"generator_total_loss\", generator_total_loss)\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        tf.summary.scalar(\n",
    "            name=\"generator_loss\", tensor=generator_loss, family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"generator_reg_loss\",\n",
    "            tensor=generator_reg_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"generator_total_loss\",\n",
    "            tensor=generator_total_loss,\n",
    "            family=\"total_losses\"\n",
    "        )\n",
    "\n",
    "        return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/discriminator.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import networks\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Discriminator(networks.Network):\n",
    "    \"\"\"Discriminator that takes image input and outputs logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, name):\n",
    "        \"\"\"Instantiates discriminator network.\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of discriminator.\n",
    "        \"\"\"\n",
    "        # Initialize base class.\n",
    "        super().__init__(kernel_regularizer, bias_regularizer, name)\n",
    "\n",
    "    def get_discriminator_logits(self, X, labels, params):\n",
    "        \"\"\"Creates discriminator network and returns logits.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image tensors of shape\n",
    "                [cur_batch_size, height * width * depth].\n",
    "            labels: tensor, labels to condition on of shape\n",
    "                [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, 1].\n",
    "        \"\"\"\n",
    "        func_name = \"get_discriminator_logits\"\n",
    "        print_obj(\"\\n\" + func_name, \"X\", X)\n",
    "        print_obj(func_name, \"labels\", labels)\n",
    "\n",
    "        with tf.variable_scope(\n",
    "                name_or_scope=\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "            if params[\"discriminator_use_labels\"]:\n",
    "                network = self.use_labels(\n",
    "                    features=X,\n",
    "                    labels=labels,\n",
    "                    params=params,\n",
    "                    scope=\"discriminator\"\n",
    "                )\n",
    "            else:\n",
    "                network = X\n",
    "\n",
    "            # Add hidden layers with given number of units/neurons per layer.\n",
    "            for i, units in enumerate(params[\"discriminator_hidden_units\"]):\n",
    "                # shape = (cur_batch_size, discriminator_hidden_units[i])\n",
    "                network = tf.layers.dense(\n",
    "                    inputs=network,\n",
    "                    units=units,\n",
    "                    activation=None,\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"layers_dense_{}\".format(i)\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                network = tf.nn.leaky_relu(\n",
    "                    features=network,\n",
    "                    alpha=params[\"discriminator_leaky_relu_alpha\"],\n",
    "                    name=\"leaky_relu_{}\".format(i)\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "            # Final linear layer for logits.\n",
    "            # shape = (cur_batch_size, 1)\n",
    "            logits = tf.layers.dense(\n",
    "                inputs=network,\n",
    "                units=1,\n",
    "                activation=None,\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"layers_dense_logits\"\n",
    "            )\n",
    "            print_obj(func_name, \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_discriminator_loss(self, fake_logits, real_logits, params):\n",
    "        \"\"\"Gets discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            fake_logits: tensor, shape of\n",
    "                [cur_batch_size, 1].\n",
    "            real_logits: tensor, shape of\n",
    "                [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of discriminator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        func_name = \"get_discriminator_loss\"\n",
    "        # Calculate base discriminator loss.\n",
    "        discriminator_real_loss = tf.reduce_mean(\n",
    "            input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=real_logits,\n",
    "                labels=tf.multiply(\n",
    "                    x=tf.ones_like(tensor=real_logits),\n",
    "                    y=params[\"label_smoothing\"]\n",
    "                )\n",
    "            ),\n",
    "            name=\"discriminator_real_loss\"\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\n\" + func_name,\n",
    "            \"discriminator_real_loss\",\n",
    "            discriminator_real_loss\n",
    "        )\n",
    "\n",
    "        discriminator_fake_loss = tf.reduce_mean(\n",
    "            input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=fake_logits,\n",
    "                labels=tf.zeros_like(tensor=fake_logits)\n",
    "            ),\n",
    "            name=\"discriminator_fake_loss\"\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"discriminator_fake_loss\", discriminator_fake_loss\n",
    "        )\n",
    "\n",
    "        discriminator_loss = tf.add(\n",
    "            x=discriminator_real_loss,\n",
    "            y=discriminator_fake_loss,\n",
    "            name=\"discriminator_loss\"\n",
    "        )\n",
    "        print_obj(func_name, \"discriminator_loss\", discriminator_loss)\n",
    "\n",
    "        # Get regularization losses.\n",
    "        discriminator_reg_loss = tf.losses.get_regularization_loss(\n",
    "            scope=\"discriminator\",\n",
    "            name=\"discriminator_reg_loss\"\n",
    "        )\n",
    "        print_obj(func_name, \"discriminator_reg_loss\", discriminator_reg_loss)\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        discriminator_total_loss = tf.math.add(\n",
    "            x=discriminator_loss,\n",
    "            y=discriminator_reg_loss,\n",
    "            name=\"discriminator_total_loss\"\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"discriminator_total_loss\", discriminator_total_loss\n",
    "        )\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        tf.summary.scalar(\n",
    "            name=\"discriminator_real_loss\",\n",
    "            tensor=discriminator_real_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"discriminator_fake_loss\",\n",
    "            tensor=discriminator_fake_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"discriminator_loss\",\n",
    "            tensor=discriminator_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"discriminator_reg_loss\",\n",
    "            tensor=discriminator_reg_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"discriminator_total_loss\",\n",
    "            tensor=discriminator_total_loss,\n",
    "            family=\"total_losses\"\n",
    "        )\n",
    "\n",
    "        return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/train_and_eval.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_logits_and_losses(features, labels, generator, discriminator, params):\n",
    "    \"\"\"Gets logits and losses for both train and eval modes.\n",
    "\n",
    "    Args:\n",
    "        features: dict, feature tensors from serving input function.\n",
    "        labels: tensor, labels to condition on of shape\n",
    "            [cur_batch_size, 1].\n",
    "        generator: instance of generator.`Generator`.\n",
    "        discriminator: instance of discriminator.`Discriminator`.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Real and fake logits and generator and discriminator losses.\n",
    "    \"\"\"\n",
    "    func_name = \"get_logits_and_losses\"\n",
    "\n",
    "    # For training discriminator.\n",
    "    print(\"\\nTraining discriminator.\")\n",
    "\n",
    "    # Extract real images from features dictionary.\n",
    "    real_images = tf.reshape(\n",
    "        tensor=features[\"image\"],\n",
    "        shape=[-1, params[\"height\"] * params[\"width\"] * params[\"depth\"]],\n",
    "        name=\"real_images\"\n",
    "    )\n",
    "    print_obj(\"\\n\" + func_name, \"real_images\", real_images)\n",
    "\n",
    "    # Get dynamic batch size in case of partial batch.\n",
    "    cur_batch_size = tf.shape(\n",
    "        input=real_images,\n",
    "        out_type=tf.int32,\n",
    "        name=\"{}_cur_batch_size\".format(func_name)\n",
    "    )[0]\n",
    "\n",
    "    # Create random noise latent vector for each batch example.\n",
    "    Z = tf.random.normal(\n",
    "        shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "        mean=0.0,\n",
    "        stddev=1.0,\n",
    "        dtype=tf.float32,\n",
    "        name=\"discriminator_Z\"\n",
    "    )\n",
    "    print_obj(func_name, \"Z\", Z)\n",
    "\n",
    "    # Get generated image from generator network from gaussian noise.\n",
    "    print(\"\\nCall generator with Z = {}.\".format(Z))\n",
    "    fake_images = generator.get_fake_images(Z=Z, labels=labels, params=params)\n",
    "\n",
    "    # Get fake logits from discriminator using generator's output image.\n",
    "    print(\"\\nCall discriminator with fake_images = {}.\".format(fake_images))\n",
    "    fake_logits = discriminator.get_discriminator_logits(\n",
    "        X=fake_images, labels=labels, params=params\n",
    "    )\n",
    "\n",
    "    # Get real logits from discriminator using real image.\n",
    "    print(\n",
    "        \"\\nCall discriminator with real_images = {}.\".format(real_images)\n",
    "    )\n",
    "    real_logits = discriminator.get_discriminator_logits(\n",
    "        X=real_images, labels=labels, params=params\n",
    "    )\n",
    "\n",
    "    # Get discriminator total loss.\n",
    "    discriminator_total_loss = discriminator.get_discriminator_loss(\n",
    "        fake_logits=fake_logits, real_logits=real_logits, params=params\n",
    "    )\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    # For training generator.\n",
    "    print(\"\\nTraining generator.\")\n",
    "\n",
    "    # Create random noise latent vector for each batch example.\n",
    "    fake_Z = tf.random.normal(\n",
    "        shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "        mean=0.0,\n",
    "        stddev=1.0,\n",
    "        dtype=tf.float32,\n",
    "        name=\"generator_Z\"\n",
    "    )\n",
    "\n",
    "    # Create random (fake) labels.\n",
    "    fake_labels = tf.random.uniform(\n",
    "        shape=[cur_batch_size, 1],\n",
    "        minval=0,\n",
    "        maxval=params[\"num_classes\"],\n",
    "        dtype=tf.int32,\n",
    "        name=\"fake_labels\"\n",
    "    )\n",
    "    print_obj(func_name, \"fake_labels\", fake_labels)\n",
    "\n",
    "    # Get generated image from generator network from gaussian noise.\n",
    "    print(\"\\nCall generator with fake_Z = {}.\".format(fake_Z))\n",
    "    fake_fake_images = generator.get_fake_images(\n",
    "        Z=fake_Z, labels=fake_labels, params=params\n",
    "    )\n",
    "\n",
    "    # Get fake logits from discriminator using generator's output image.\n",
    "    print(\n",
    "        \"\\nCall discriminator with fake_fake_images = {}.\".format(\n",
    "            fake_fake_images\n",
    "        )\n",
    "    )\n",
    "    fake_fake_logits = discriminator.get_discriminator_logits(\n",
    "        X=fake_fake_images, labels=fake_labels, params=params\n",
    "    )\n",
    "\n",
    "    # Get generator total loss.\n",
    "    generator_total_loss = generator.get_generator_loss(\n",
    "        fake_logits=fake_fake_logits\n",
    "    )\n",
    "\n",
    "    # Add summaries for TensorBoard.\n",
    "    tf.summary.image(\n",
    "        name=\"fake_images\",\n",
    "        tensor=tf.reshape(\n",
    "            tensor=generator.get_fake_images(\n",
    "                Z=tf.random.normal(\n",
    "                    shape=[params[\"num_classes\"], params[\"latent_size\"]],\n",
    "                    mean=0.0,\n",
    "                    stddev=1.0,\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"image_summary_Z\"\n",
    "                ),\n",
    "                labels=tf.expand_dims(\n",
    "                    input=tf.range(\n",
    "                        start=0, limit=params[\"num_classes\"], dtype=tf.int32\n",
    "                    ),\n",
    "                    axis=-1,\n",
    "                    name=\"image_summary_fake_labels\"\n",
    "                ),\n",
    "                params=params\n",
    "            ),\n",
    "            shape=[-1, params[\"height\"], params[\"width\"], params[\"depth\"]],\n",
    "            name=\"image_summary_fake_fake_images\"\n",
    "        ),\n",
    "        max_outputs=params[\"num_classes\"],\n",
    "    )\n",
    "\n",
    "    return (real_logits,\n",
    "            fake_logits,\n",
    "            generator_total_loss,\n",
    "            discriminator_total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/train.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_variables_and_gradients(loss, scope):\n",
    "    \"\"\"Gets variables and their gradients wrt. loss.\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        scope: str, the network's name to find its variables to train.\n",
    "    Returns:\n",
    "        Lists of variables and their gradients.\n",
    "    \"\"\"\n",
    "    func_name = \"get_variables_and_gradients\"\n",
    "    # Get trainable variables.\n",
    "    variables = tf.trainable_variables(scope=scope)\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"variables\", variables)\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=variables,\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    # Add variable names back in for identification.\n",
    "    gradients = [\n",
    "        tf.identity(\n",
    "            input=g,\n",
    "            name=\"{}_{}_gradients\".format(func_name, v.name[:-2])\n",
    "        )\n",
    "        if tf.is_tensor(x=g) else g\n",
    "        for g, v in zip(gradients, variables)\n",
    "    ]\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    return variables, gradients\n",
    "\n",
    "\n",
    "def create_variable_and_gradient_histogram_summaries(loss_dict, params):\n",
    "    \"\"\"Creates variable and gradient histogram summaries.\n",
    "    Args:\n",
    "        loss_dict: dict, keys are scopes and values are scalar loss tensors\n",
    "            for each network kind.\n",
    "        params: dict, user passed parameters.\n",
    "    \"\"\"\n",
    "    for scope, loss in loss_dict.items():\n",
    "        # Get variables and their gradients wrt. loss.\n",
    "        variables, gradients = get_variables_and_gradients(loss, scope)\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        for g, v in zip(gradients, variables):\n",
    "            tf.summary.histogram(\n",
    "                name=\"{}\".format(v.name[:-2]),\n",
    "                values=v,\n",
    "                family=\"{}_variables\".format(scope)\n",
    "            )\n",
    "            if tf.is_tensor(x=g):\n",
    "                tf.summary.histogram(\n",
    "                    name=\"{}\".format(v.name[:-2]),\n",
    "                    values=g,\n",
    "                    family=\"{}_gradients\".format(scope)\n",
    "                )\n",
    "\n",
    "\n",
    "def train_network(loss, global_step, params, scope):\n",
    "    \"\"\"Trains network and returns loss and train op.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the variables that to train.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor and training op.\n",
    "    \"\"\"\n",
    "    func_name = \"train_network\"\n",
    "    print_obj(\"\\n\" + func_name, \"scope\", scope)\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adam\": tf.train.AdamOptimizer,\n",
    "        \"Adadelta\": tf.train.AdadeltaOptimizer,\n",
    "        \"AdagradDA\": tf.train.AdagradDAOptimizer,\n",
    "        \"Adagrad\": tf.train.AdagradOptimizer,\n",
    "        \"Ftrl\": tf.train.FtrlOptimizer,\n",
    "        \"GradientDescent\": tf.train.GradientDescentOptimizer,\n",
    "        \"Momentum\": tf.train.MomentumOptimizer,\n",
    "        \"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer,\n",
    "        \"ProximalGradientDescent\": tf.train.ProximalGradientDescentOptimizer,\n",
    "        \"RMSProp\": tf.train.RMSPropOptimizer\n",
    "    }\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    if params[\"{}_optimizer\".format(scope)] == \"Adam\":\n",
    "        optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "            learning_rate=params[\"{}_learning_rate\".format(scope)],\n",
    "            beta1=params[\"{}_adam_beta1\".format(scope)],\n",
    "            beta2=params[\"{}_adam_beta2\".format(scope)],\n",
    "            epsilon=params[\"{}_adam_epsilon\".format(scope)],\n",
    "            name=\"{}_{}_optimizer\".format(\n",
    "                scope, params[\"{}_optimizer\".format(scope)].lower()\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "            learning_rate=params[\"{}_learning_rate\".format(scope)],\n",
    "            name=\"{}_{}_optimizer\".format(\n",
    "                scope, params[\"{}_optimizer\".format(scope)].lower()\n",
    "            )\n",
    "        )\n",
    "    print_obj(\"{}_{}\".format(func_name, scope), \"optimizer\", optimizer)\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=tf.trainable_variables(scope=scope),\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    # Clip gradients.\n",
    "    if params[\"{}_clip_gradients\".format(scope)]:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=params[\"{}_clip_gradients\".format(scope)],\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "        print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    # Zip back together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, tf.trainable_variables(scope=scope))\n",
    "    print_obj(\n",
    "        \"{}_{}\".format(func_name, scope), \"grads_and_vars\", grads_and_vars\n",
    "    )\n",
    "\n",
    "    # Create train op by applying gradients to variables and incrementing\n",
    "    # global step.\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        global_step=global_step,\n",
    "        name=\"{}_apply_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    return loss, train_op\n",
    "\n",
    "\n",
    "def get_loss_and_train_op(\n",
    "        generator_total_loss, discriminator_total_loss, params):\n",
    "    \"\"\"Gets loss and train op for train mode.\n",
    "    Args:\n",
    "        generator_total_loss: tensor, scalar total loss of generator.\n",
    "        discriminator_total_loss: tensor, scalar total loss of discriminator.\n",
    "        params: dict, user passed parameters.\n",
    "    Returns:\n",
    "        Loss scalar tensor and train_op to be used by the EstimatorSpec.\n",
    "    \"\"\"\n",
    "    func_name = \"get_loss_and_train_op\"\n",
    "    # Get global step.\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Determine if it is time to train generator or discriminator.\n",
    "    cycle_step = tf.mod(\n",
    "        x=global_step,\n",
    "        y=tf.cast(\n",
    "            x=tf.add(\n",
    "                x=params[\"discriminator_train_steps\"],\n",
    "                y=params[\"generator_train_steps\"]\n",
    "            ),\n",
    "            dtype=tf.int64\n",
    "        ),\n",
    "        name=\"{}_cycle_step\".format(func_name)\n",
    "    )\n",
    "\n",
    "    # Create choose discriminator condition.\n",
    "    condition = tf.less(\n",
    "        x=cycle_step, y=params[\"discriminator_train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Conditionally choose to train generator or discriminator subgraph.\n",
    "    loss, train_op = tf.cond(\n",
    "        pred=condition,\n",
    "        true_fn=lambda: train_network(\n",
    "            loss=discriminator_total_loss,\n",
    "            global_step=global_step,\n",
    "            params=params,\n",
    "            scope=\"discriminator\"\n",
    "        ),\n",
    "        false_fn=lambda: train_network(\n",
    "            loss=generator_total_loss,\n",
    "            global_step=global_step,\n",
    "            params=params,\n",
    "            scope=\"generator\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return loss, train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval_metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/eval_metrics.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_eval_metric_ops(fake_logits, real_logits, params):\n",
    "    \"\"\"Gets eval metric ops.\n",
    "\n",
    "    Args:\n",
    "        fake_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "            discriminator having processed generator's output image.\n",
    "        real_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "            discriminator having processed real image.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of eval metric ops.\n",
    "    \"\"\"\n",
    "    func_name = \"get_eval_metric_ops\"\n",
    "    # Concatenate discriminator logits and labels.\n",
    "    discriminator_logits = tf.concat(\n",
    "        values=[real_logits, fake_logits],\n",
    "        axis=0,\n",
    "        name=\"discriminator_concat_logits\"\n",
    "    )\n",
    "    print_obj(\"\\n\" + func_name, \"discriminator_logits\", discriminator_logits)\n",
    "\n",
    "    discriminator_labels = tf.concat(\n",
    "        values=[\n",
    "            tf.ones_like(tensor=real_logits) * params[\"label_smoothing\"],\n",
    "            tf.zeros_like(tensor=fake_logits)\n",
    "        ],\n",
    "        axis=0,\n",
    "        name=\"discriminator_concat_labels\"\n",
    "    )\n",
    "    print_obj(func_name, \"discriminator_labels\", discriminator_labels)\n",
    "\n",
    "    # Calculate discriminator probabilities.\n",
    "    discriminator_probabilities = tf.nn.sigmoid(\n",
    "        x=discriminator_logits, name=\"discriminator_probabilities\"\n",
    "    )\n",
    "    print_obj(\n",
    "        func_name, \"discriminator_probabilities\", discriminator_probabilities\n",
    "    )\n",
    "\n",
    "    # Create eval metric ops dictionary.\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_accuracy\"\n",
    "        ),\n",
    "        \"precision\": tf.metrics.precision(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_precision\"\n",
    "        ),\n",
    "        \"recall\": tf.metrics.recall(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_recall\"\n",
    "        ),\n",
    "        \"auc_roc\": tf.metrics.auc(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            num_thresholds=200,\n",
    "            curve=\"ROC\",\n",
    "            name=\"discriminator_auc_roc\"\n",
    "        ),\n",
    "        \"auc_pr\": tf.metrics.auc(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            num_thresholds=200,\n",
    "            curve=\"PR\",\n",
    "            name=\"discriminator_auc_pr\"\n",
    "        )\n",
    "    }\n",
    "    print_obj(func_name, \"eval_metric_ops\", eval_metric_ops)\n",
    "\n",
    "    return eval_metric_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/predict.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_predictions_and_export_outputs(features, generator, params):\n",
    "    \"\"\"Gets predictions and serving export outputs.\n",
    "\n",
    "    Args:\n",
    "        features: dict, feature tensors from serving input function.\n",
    "        generator: instance of `Generator`.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Predictions dictionary and export outputs dictionary.\n",
    "    \"\"\"\n",
    "    func_name = \"get_predictions_and_export_outputs\"\n",
    "    # Extract latent vectors from features dictionary.\n",
    "    Z = tf.cast(x=features[\"Z\"], dtype=tf.float32)\n",
    "    print_obj(\"\\n\" + func_name, \"Z\", Z)\n",
    "\n",
    "    # Extract labels from features dictionary & expand from vector to matrix.\n",
    "    labels = tf.expand_dims(input=features[\"label\"], axis=-1)\n",
    "    print_obj(func_name, \"labels\", labels)\n",
    "\n",
    "    # Establish generator network subgraph.\n",
    "    fake_images = generator.get_fake_images(Z=Z, labels=labels, params=params)\n",
    "    print_obj(func_name, \"fake_images\", fake_images)\n",
    "\n",
    "    # Reshape into a rank 4 image.\n",
    "    generated_images = tf.reshape(\n",
    "        tensor=fake_images,\n",
    "        shape=[-1, params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "    print_obj(func_name, \"generated_images\", generated_images)\n",
    "\n",
    "    # Create predictions dictionary.\n",
    "    predictions_dict = {\n",
    "        \"generated_images\": generated_images\n",
    "    }\n",
    "    print_obj(func_name, \"predictions_dict\", predictions_dict)\n",
    "\n",
    "    # Create export outputs.\n",
    "    export_outputs = {\n",
    "        \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "            outputs=predictions_dict)\n",
    "    }\n",
    "    print_obj(func_name, \"export_outputs\", export_outputs)\n",
    "\n",
    "    return predictions_dict, export_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/cgan.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import discriminator\n",
    "from . import eval_metrics\n",
    "from . import generator\n",
    "from . import predict\n",
    "from . import train\n",
    "from . import train_and_eval\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def cgan_model(features, labels, mode, params):\n",
    "    \"\"\"Conditional GAN custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `tf.estimator.EstimatorSpec` class.\n",
    "    \"\"\"\n",
    "    func_name = \"cgan_model\"\n",
    "    print_obj(\"\\n\" + func_name, \"features\", features)\n",
    "    print_obj(func_name, \"labels\", labels)\n",
    "    print_obj(func_name, \"mode\", mode)\n",
    "    print_obj(func_name, \"params\", params)\n",
    "\n",
    "    # Loss function, training/eval ops, etc.\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "\n",
    "    # Instantiate generator.\n",
    "    cgan_generator = generator.Generator(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"generator_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"generator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        name=\"generator\"\n",
    "    )\n",
    "\n",
    "    # Instantiate discriminator.\n",
    "    cgan_discriminator = discriminator.Discriminator(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"discriminator_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"discriminator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        name=\"discriminator\"\n",
    "    )\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Get predictions and export outputs.\n",
    "        (predictions_dict,\n",
    "         export_outputs) = predict.get_predictions_and_export_outputs(\n",
    "            features=features, generator=cgan_generator, params=params\n",
    "        )\n",
    "    else:\n",
    "        # Expand labels from vector to matrix.\n",
    "        labels = tf.expand_dims(input=labels, axis=-1)\n",
    "        print_obj(func_name, \"labels\", labels)\n",
    "\n",
    "        # Get logits and losses from networks for train and eval modes.\n",
    "        (real_logits,\n",
    "         fake_logits,\n",
    "         generator_total_loss,\n",
    "         discriminator_total_loss) = train_and_eval.get_logits_and_losses(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            generator=cgan_generator,\n",
    "            discriminator=cgan_discriminator,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Create variable and gradient histogram summaries.\n",
    "            train.create_variable_and_gradient_histogram_summaries(\n",
    "                loss_dict={\n",
    "                    \"generator\": generator_total_loss,\n",
    "                    \"discriminator\": discriminator_total_loss\n",
    "                },\n",
    "                params=params\n",
    "            )\n",
    "\n",
    "            # Get loss and train op for EstimatorSpec.\n",
    "            loss, train_op = train.get_loss_and_train_op(\n",
    "                generator_total_loss=generator_total_loss,\n",
    "                discriminator_total_loss=discriminator_total_loss,\n",
    "                params=params\n",
    "            )\n",
    "        else:\n",
    "            # Set eval loss.\n",
    "            loss = discriminator_total_loss\n",
    "\n",
    "            # Get eval metrics.\n",
    "            eval_metric_ops = eval_metrics.get_eval_metric_ops(\n",
    "                real_logits=real_logits,\n",
    "                fake_logits=fake_logits,\n",
    "                params=params\n",
    "            )\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs=export_outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/serving.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def serving_input_fn(params):\n",
    "    \"\"\"Serving input function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        ServingInputReceiver object containing features and receiver tensors.\n",
    "    \"\"\"\n",
    "    func_name = \"serving_input_fn\"\n",
    "    # Create placeholders to accept data sent to the model at serving time.\n",
    "    # shape = (batch_size,)\n",
    "    feature_placeholders = {\n",
    "        \"Z\": tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, params[\"latent_size\"]],\n",
    "            name=\"serving_input_placeholder_Z\"\n",
    "        ),\n",
    "        \"label\": tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None],\n",
    "            name=\"serving_input_placeholder_label\"\n",
    "        )\n",
    "    }\n",
    "    print_obj(\"\\n\" + func_name, \"feature_placeholders\", feature_placeholders)\n",
    "\n",
    "    # Create clones of the feature placeholder tensors so that the SavedModel\n",
    "    # SignatureDef will point to the placeholder.\n",
    "    features = {\n",
    "        key: tf.identity(\n",
    "            input=value,\n",
    "            name=\"{}_identity_placeholder_{}\".format(func_name, key)\n",
    "        )\n",
    "        for key, value in feature_placeholders.items()\n",
    "    }\n",
    "    print_obj(func_name, \"features\", features)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=feature_placeholders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/model.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import input\n",
    "from . import serving\n",
    "from . import cgan\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates custom Estimator model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        `Estimator` object.\n",
    "    \"\"\"\n",
    "    func_name = \"train_and_evaluate\"\n",
    "    print_obj(\"\\n\" + func_name, \"args\", args)\n",
    "    # Ensure filewriter cache is clear for TensorBoard events file.\n",
    "    tf.summary.FileWriterCache.clear()\n",
    "\n",
    "    # Set logging to be level of INFO.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Create a RunConfig for Estimator.\n",
    "    config = tf.estimator.RunConfig(\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        save_summary_steps=args[\"save_summary_steps\"],\n",
    "        save_checkpoints_steps=args[\"save_checkpoints_steps\"],\n",
    "        keep_checkpoint_max=args[\"keep_checkpoint_max\"]\n",
    "    )\n",
    "\n",
    "    # Create our custom estimator using our model function.\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=cgan.cgan_model,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        config=config,\n",
    "        params=args\n",
    "    )\n",
    "\n",
    "    # Create train spec to read in our training data.\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=input.read_dataset(\n",
    "            filename=args[\"train_file_pattern\"],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size=args[\"train_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        max_steps=args[\"train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Create exporter to save out the complete model to disk.\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name=\"exporter\",\n",
    "        serving_input_receiver_fn=lambda: serving.serving_input_fn(args)\n",
    "    )\n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model.\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=input.read_dataset(\n",
    "            filename=args[\"eval_file_pattern\"],\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size=args[\"eval_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        steps=args[\"eval_steps\"],\n",
    "        start_delay_secs=args[\"start_delay_secs\"],\n",
    "        throttle_secs=args[\"throttle_secs\"],\n",
    "        exporters=exporter\n",
    "    )\n",
    "\n",
    "    # Create train and evaluate loop to train and evaluate our estimator.\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cgan_module/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "def convert_string_to_bool(string):\n",
    "    \"\"\"Converts string to bool.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        Boolean conversion of string.\n",
    "    \"\"\"\n",
    "    return False if string.lower() == \"false\" else True\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_float(string):\n",
    "    \"\"\"Converts string to None or float.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or float conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else float(string)\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_int(string):\n",
    "    \"\"\"Converts string to None or int.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or int conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else int(string)\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_ints(string, sep):\n",
    "    \"\"\"Converts string to list of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of ints conversion of string.\n",
    "    \"\"\"\n",
    "    return [int(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # File arguments.\n",
    "    parser.add_argument(\n",
    "        \"--train_file_pattern\",\n",
    "        help=\"GCS location to read training data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_file_pattern\",\n",
    "        help=\"GCS location to read evaluation data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud.\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "\n",
    "    # Training parameters.\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Number of examples in training batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Number of steps to train for.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_summary_steps\",\n",
    "        help=\"How many steps to train before saving a summary.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_checkpoints_steps\",\n",
    "        help=\"How many steps to train before saving a checkpoint.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_checkpoint_max\",\n",
    "        help=\"Max number of checkpoints to keep.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Eval parameters.\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        help=\"Number of examples in evaluation batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Number of steps to evaluate for.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--start_delay_secs\",\n",
    "        help=\"Number of seconds to wait before first evaluation.\",\n",
    "        type=int,\n",
    "        default=60\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--throttle_secs\",\n",
    "        help=\"Number of seconds to wait between evaluations.\",\n",
    "        type=int,\n",
    "        default=120\n",
    "    )\n",
    "\n",
    "    # Image parameters.\n",
    "    parser.add_argument(\n",
    "        \"--height\",\n",
    "        help=\"Height of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--width\",\n",
    "        help=\"Width of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--depth\",\n",
    "        help=\"Depth of image.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "\n",
    "    # Label parameters.\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        help=\"Number of image classes.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--label_embedding_dimension\",\n",
    "        help=\"Number of dimensions to embed label classes into.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "\n",
    "    # Generator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--latent_size\",\n",
    "        help=\"The latent size of the noise vector.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_use_labels\",\n",
    "        help=\"Whether to condition on labels in generator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_embed_labels\",\n",
    "        help=\"Whether to embed labels in generator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_concatenate_labels\",\n",
    "        help=\"Whether to concatenate labels in generator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_dense_before_concatenate\",\n",
    "        help=\"Whether to have a dense layer before concatenation of labels in generator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_hidden_units\",\n",
    "        help=\"Hidden layer sizes to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"2,4,8\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_final_activation\",\n",
    "        help=\"The final activation function of generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_optimizer\",\n",
    "        help=\"Name of optimizer to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for generator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_train_steps\",\n",
    "        help=\"Number of steps to train generator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Discriminator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_use_labels\",\n",
    "        help=\"Whether to condition on labels in discriminator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_embed_labels\",\n",
    "        help=\"Whether to embed labels in discriminator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_concatenate_labels\",\n",
    "        help=\"Whether to concatenate labels in discriminator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_dense_before_concatenate\",\n",
    "        help=\"Whether to have a dense layer before concatenation of labels in discriminator.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_hidden_units\",\n",
    "        help=\"Hidden layer sizes to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"2,4,8\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of discriminator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_optimizer\",\n",
    "        help=\"Name of optimizer to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_train_steps\",\n",
    "        help=\"Number of steps to train discriminator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--label_smoothing\",\n",
    "        help=\"Multiplier when making real labels instead of all ones.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "\n",
    "    # Parse all arguments.\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service.\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Fix eval steps.\n",
    "    arguments[\"eval_steps\"] = convert_string_to_none_or_int(\n",
    "        string=arguments[\"eval_steps\"])\n",
    "\n",
    "    # Fix use_labels.\n",
    "    arguments[\"generator_use_labels\"] = convert_string_to_bool(\n",
    "        arguments[\"generator_use_labels\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_use_labels\"] = convert_string_to_bool(\n",
    "        arguments[\"discriminator_use_labels\"]\n",
    "    )\n",
    "\n",
    "    # Fix embed_labels.\n",
    "    arguments[\"generator_embed_labels\"] = convert_string_to_bool(\n",
    "        arguments[\"generator_embed_labels\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_embed_labels\"] = convert_string_to_bool(\n",
    "        arguments[\"discriminator_embed_labels\"]\n",
    "    )\n",
    "\n",
    "    # Fix concatenate_labels.\n",
    "    arguments[\"generator_concatenate_labels\"] = convert_string_to_bool(\n",
    "        arguments[\"generator_concatenate_labels\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_concatenate_labels\"] = convert_string_to_bool(\n",
    "        arguments[\"discriminator_concatenate_labels\"]\n",
    "    )\n",
    "\n",
    "    # Fix dense_before_concatenate.\n",
    "    arguments[\"generator_dense_before_concatenate\"] = convert_string_to_bool(\n",
    "        arguments[\"generator_dense_before_concatenate\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_dense_before_concatenate\"] = convert_string_to_bool(\n",
    "        arguments[\"discriminator_dense_before_concatenate\"]\n",
    "    )\n",
    "\n",
    "    # Fix hidden_units.\n",
    "    arguments[\"generator_hidden_units\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_hidden_units\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_hidden_units\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"discriminator_hidden_units\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix clip_gradients.\n",
    "    arguments[\"generator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"generator_clip_gradients\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"discriminator_clip_gradients\"]\n",
    "    )\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning.\n",
    "    # This code can be removed if you are not using hyperparameter tuning.\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\n",
    "                \"TF_CONFIG\", \"{}\"\n",
    "            )\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\"))\n",
    "\n",
    "    # Run the training job.\n",
    "    model.train_and_evaluate(arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
