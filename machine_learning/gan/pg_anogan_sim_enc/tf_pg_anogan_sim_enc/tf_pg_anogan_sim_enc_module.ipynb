{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/print_object.py\n",
    "def print_obj(function_name, object_name, object_value):\n",
    "    \"\"\"Prints enclosing function, object name, and object value.\n",
    "\n",
    "    Args:\n",
    "        function_name: str, name of function.\n",
    "        object_name: str, name of object.\n",
    "        object_value: object, value of passed object.\n",
    "    \"\"\"\n",
    "#     pass\n",
    "    print(\"{}: {} = {}\".format(function_name, object_name, object_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/image_utils.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def preprocess_image(image, params):\n",
    "    \"\"\"Preprocess image tensor.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, input image with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image tensor with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "    \"\"\"\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "    print_obj(\"preprocess_image\", \"image\", image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_real_image(image, params, block_idx):\n",
    "    \"\"\"Resizes real images to match the GAN's current size.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, original image.\n",
    "        params: dict, user passed parameters.\n",
    "        block_idx: int, index of current block.\n",
    "\n",
    "    Returns:\n",
    "        Resized image tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nresize_real_image\", \"block_idx\", block_idx)\n",
    "    print_obj(\"resize_real_image\", \"image\", image)\n",
    "\n",
    "    # Resize image to match GAN size at current block index.\n",
    "    resized_image = tf.image.resize(\n",
    "        images=image,\n",
    "        size=[\n",
    "            params[\"generator_projection_dims\"][0] * (2 ** block_idx),\n",
    "            params[\"generator_projection_dims\"][1] * (2 ** block_idx)\n",
    "        ],\n",
    "        method=\"nearest\",\n",
    "        name=\"resize_real_images_resized_image_{}\".format(block_idx)\n",
    "    )\n",
    "    print_obj(\"resize_real_images\", \"resized_image\", resized_image)\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def resize_real_images(image, params):\n",
    "    \"\"\"Resizes real images to match the GAN's current size.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, original image.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Resized image tensor.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nresize_real_images\", \"image\", image)\n",
    "    # Resize real image for each block.\n",
    "    train_steps = params[\"train_steps\"] + params[\"prev_train_steps\"]\n",
    "    num_steps_until_growth = params[\"num_steps_until_growth\"]\n",
    "    num_stages = train_steps // num_steps_until_growth\n",
    "    if (num_stages <= 0 or len(params[\"conv_num_filters\"]) == 1):\n",
    "        print(\n",
    "            \"\\nresize_real_images: NEVER GOING TO GROW, SKIP SWITCH CASE!\"\n",
    "        )\n",
    "        # If we never are going to grow, no sense using the switch case.\n",
    "        # 4x4\n",
    "        resized_image = resize_real_image(\n",
    "            image=image, params=params, block_idx=0\n",
    "        )\n",
    "    else:\n",
    "        # Find growth index based on global step and growth frequency.\n",
    "        growth_index = tf.cast(\n",
    "            x=tf.floordiv(\n",
    "                x=tf.train.get_or_create_global_step(),\n",
    "                y=params[\"num_steps_until_growth\"],\n",
    "                name=\"resize_real_images_global_step_floordiv\"\n",
    "            ),\n",
    "            dtype=tf.int32,\n",
    "            name=\"resize_real_images_growth_index\"\n",
    "        )\n",
    "\n",
    "        # Switch to case based on number of steps for resized image.\n",
    "        resized_image = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                # 4x4\n",
    "                lambda: resize_real_image(\n",
    "                    image=image, params=params, block_idx=0\n",
    "                ),\n",
    "                # 8x8\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(1, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 16x16\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(2, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 32x32\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(3, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 64x64\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(4, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 128x128\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(5, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 256x256\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(6, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 512x512\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(7, len(params[\"conv_num_filters\"]) - 1)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: resize_real_image(\n",
    "                    image=image,\n",
    "                    params=params,\n",
    "                    block_idx=min(8, len(params[\"conv_num_filters\"]) - 1)\n",
    "                )\n",
    "            ],\n",
    "            name=\"resize_real_images_switch_case_resized_image\"\n",
    "        )\n",
    "        print_obj(\n",
    "            \"resize_real_images\", \"selected resized_image\", resized_image\n",
    "        )\n",
    "\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/input.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "    print_obj(\"\\ndecode_example\", \"features\", features)\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Preprocess image.\n",
    "    image = image_utils.preprocess_image(image=image, params=params)\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "    print_obj(\"decode_example\", \"label\", label)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def set_static_shape(features, labels, batch_size):\n",
    "    features[\"image\"].set_shape(\n",
    "        features[\"image\"].get_shape().merge_with(\n",
    "            tf.TensorShape([batch_size, None, None, None])\n",
    "        )\n",
    "    )\n",
    "    labels.set_shape(\n",
    "        labels.get_shape().merge_with(tf.TensorShape([batch_size]))\n",
    "    )\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    \"\"\"Reads CSV time series data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read CSV dataset\n",
    "    using Dataset API, apply necessary preprocessing, and return an input\n",
    "    function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        mode: The estimator ModeKeys. Can be TRAIN or EVAL.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn(params):\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        # read the dataset\n",
    "        dataset = tf.data.Dataset.list_files(filename, shuffle=is_training)\n",
    "        if is_training:\n",
    "            dataset = dataset.repeat()\n",
    "        def fetch_dataset(filename):\n",
    "            buffer_size = 8 * 1024 * 1024 # 8 MiB per file\n",
    "            dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n",
    "            return dataset\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.parallel_interleave(\n",
    "                fetch_dataset, cycle_length=64, sloppy=True))\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(1024)\n",
    "\n",
    "        # augment and batch\n",
    "#         dataset = dataset.apply(\n",
    "#             tf.contrib.data.map_and_batch(\n",
    "#                 read_and_preprocess, batch_size=batch_size,\n",
    "#                 num_parallel_batches=num_cores, drop_remainder=True\n",
    "#             )\n",
    "#         )\n",
    "#         # Create list of files that match pattern.\n",
    "#         file_list = tf.gfile.Glob(filename=filename)\n",
    "\n",
    "#         # Create dataset from file list.\n",
    "#         dataset = tf.data.TFRecordDataset(\n",
    "#             filenames=file_list, num_parallel_reads=tf.contrib.data.AUTOTUNE\n",
    "#         )\n",
    "\n",
    "#         # Shuffle and repeat if training with fused op.\n",
    "#         if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#             dataset = dataset.apply(\n",
    "#                 tf.contrib.data.shuffle_and_repeat(\n",
    "#                     buffer_size=50 * params[\"batch_size\"],\n",
    "#                     count=None  # indefinitely\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "        # Decode CSV file into a features dictionary of tensors, then batch.\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                map_func=lambda x: decode_example(\n",
    "                    protos=x,\n",
    "                    params=params\n",
    "                ),\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                num_parallel_batches=8,\n",
    "                drop_remainder=True,\n",
    "#                 num_parallel_calls=tf.contrib.data.AUTOTUNE\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Assign static shape.\n",
    "        dataset = dataset.map(lambda x, y: set_static_shape(features=x, labels=y, batch_size=batch_size))\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\n",
    "\n",
    "        # Create a iterator, then get batch of features from example queue.\n",
    "        batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return batched_dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/generator.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import regularization\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    \"\"\"Generator that takes latent vector input and outputs image.\n",
    "\n",
    "    Fields:\n",
    "        name: str, name of `Generator`.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "        projection_layer: `Dense` layer for projection of noise to image.\n",
    "        conv_layer_blocks: list, lists of block layers for each block.\n",
    "        to_rgb_conv_layers: list, toRGB 1x1 conv layers.\n",
    "        build_generator_tensors: list, tensors used to build layer internals.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, params, name):\n",
    "        \"\"\"Instantiates and builds generator network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            params: dict, user passed parameters.\n",
    "            name: str, name of generator.\n",
    "        \"\"\"\n",
    "        # Set name of generator.\n",
    "        self.name = name\n",
    "\n",
    "        # Regularizer for kernel weights.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        # Regularizer for bias weights.\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        # Instantiate generator layers.\n",
    "        (self.projection_layer,\n",
    "         self.conv_layer_blocks,\n",
    "         self.to_rgb_conv_layers) = self.instantiate_generator_layers(params)\n",
    "\n",
    "        # Build generator layer internals.\n",
    "        self.build_generator_tensors = self.build_generator_layers(\n",
    "            params\n",
    "        )\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def instantiate_generator_projection_layer(self, params):\n",
    "        \"\"\"Instantiates generator projection layer.\n",
    "\n",
    "        Projection layer projects latent noise vector into an image.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Latent vector projection `Dense` layer.\n",
    "        \"\"\"\n",
    "        # Project latent vectors.\n",
    "        projection_height = params[\"generator_projection_dims\"][0]\n",
    "        projection_width = params[\"generator_projection_dims\"][1]\n",
    "        projection_depth = params[\"generator_projection_dims\"][2]\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     projection_height * projection_width * projection_depth\n",
    "            # )\n",
    "            projection_layer = tf.layers.Dense(\n",
    "                units=projection_height * projection_width * projection_depth,\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"{}_projection_layer\".format(self.name)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_generator_projection_layer\",\n",
    "                \"projection_layer\",\n",
    "                projection_layer\n",
    "            )\n",
    "\n",
    "        return projection_layer\n",
    "\n",
    "    def instantiate_generator_base_conv_layer_block(self, params):\n",
    "        \"\"\"Instantiates generator base conv layer block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of base block conv layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"generator_base_conv_blocks\"][0]\n",
    "\n",
    "            # Create list of base conv layers.\n",
    "            base_conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[i][3],\n",
    "                    kernel_size=conv_block[i][0:2],\n",
    "                    strides=conv_block[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        i,\n",
    "                        conv_block[i][0],\n",
    "                        conv_block[i][1],\n",
    "                        conv_block[i][2],\n",
    "                        conv_block[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_generator_base_conv_layer_block\",\n",
    "                \"base_conv_layers\",\n",
    "                base_conv_layers\n",
    "            )\n",
    "\n",
    "        return base_conv_layers\n",
    "\n",
    "    def instantiate_generator_growth_layer_block(self, params, block_idx):\n",
    "        \"\"\"Instantiates generator growth layer block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of growth block conv layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"generator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "            # Create new inner convolutional layers.\n",
    "            conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[i][3],\n",
    "                    kernel_size=conv_block[i][0:2],\n",
    "                    strides=conv_block[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        block_idx,\n",
    "                        i,\n",
    "                        conv_block[i][0],\n",
    "                        conv_block[i][1],\n",
    "                        conv_block[i][2],\n",
    "                        conv_block[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_generator_growth_layer_block\",\n",
    "                \"conv_layers\",\n",
    "                conv_layers\n",
    "            )\n",
    "\n",
    "        return conv_layers\n",
    "\n",
    "    def instantiate_generator_to_rgb_layers(self, params):\n",
    "        \"\"\"Instantiates generator toRGB layers of 1x1 convs.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of toRGB 1x1 conv layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get toRGB layer properties.\n",
    "            to_rgb = [\n",
    "                params[\"generator_to_rgb_layers\"][i][0][:]\n",
    "                for i in range(len(params[\"generator_to_rgb_layers\"]))\n",
    "            ]\n",
    "\n",
    "            # Create list to hold toRGB 1x1 convs.\n",
    "            to_rgb_conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=to_rgb[i][3],\n",
    "                    kernel_size=to_rgb[i][0:2],\n",
    "                    strides=to_rgb[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    # Notice there is no activation for toRGB conv layers.\n",
    "                    activation=None,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_to_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        i,\n",
    "                        to_rgb[i][0],\n",
    "                        to_rgb[i][1],\n",
    "                        to_rgb[i][2],\n",
    "                        to_rgb[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(to_rgb))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_generator_to_rgb_layers\",\n",
    "                \"to_rgb_conv_layers\",\n",
    "                to_rgb_conv_layers\n",
    "            )\n",
    "\n",
    "        return to_rgb_conv_layers\n",
    "\n",
    "    def instantiate_generator_layers(self, params):\n",
    "        \"\"\"Instantiates layers of generator network.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            projection_layer: `Dense` layer for projection of noise to image.\n",
    "            conv_layer_blocks: list, lists of block layers for each block.\n",
    "            to_rgb_conv_layers: list, toRGB 1x1 conv layers.\n",
    "        \"\"\"\n",
    "        # Instantiate noise-image projection `Dense` layer.\n",
    "        projection_layer = self.instantiate_generator_projection_layer(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\ninstantiate_generator_layers\",\n",
    "            \"projection_layer\",\n",
    "            projection_layer\n",
    "        )\n",
    "\n",
    "        # Instantiate base convolutional `Conv2D` layers, for post-growth.\n",
    "        conv_layer_blocks = [\n",
    "            self.instantiate_generator_base_conv_layer_block(\n",
    "                params=params\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Instantiate growth block `Conv2D` layers.\n",
    "        conv_layer_blocks.extend(\n",
    "            [\n",
    "                self.instantiate_generator_growth_layer_block(\n",
    "                    params=params, block_idx=block_idx\n",
    "                )\n",
    "                for block_idx in range(\n",
    "                    len(params[\"generator_growth_conv_blocks\"])\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_generator_layers\",\n",
    "            \"conv_layer_blocks\",\n",
    "            conv_layer_blocks\n",
    "        )\n",
    "\n",
    "        # Instantiate toRGB 1x1 `Conv2D` layers.\n",
    "        to_rgb_conv_layers = self.instantiate_generator_to_rgb_layers(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_generator_layers\",\n",
    "            \"to_rgb_conv_layers\",\n",
    "            to_rgb_conv_layers\n",
    "        )\n",
    "\n",
    "        return projection_layer, conv_layer_blocks, to_rgb_conv_layers\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def build_generator_projection_layer(self, params):\n",
    "        \"\"\"Builds generator projection layer internals using call.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Latent vector projection tensor.\n",
    "        \"\"\"\n",
    "        # Project latent vectors.\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     projection_height * projection_width * projection_depth\n",
    "            # )\n",
    "            projection_tensor = self.projection_layer(\n",
    "                inputs=tf.zeros(\n",
    "                    shape=[1, params[\"latent_size\"]], dtype=tf.float32\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\nbuild_generator_projection_layer\",\n",
    "                \"projection_tensor\",\n",
    "                projection_tensor\n",
    "            )\n",
    "\n",
    "        return projection_tensor\n",
    "\n",
    "    def build_generator_base_conv_layer_block(self, params):\n",
    "        \"\"\"Builds generator base conv layer block internals using call.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of base conv tensors.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"generator_base_conv_blocks\"][0]\n",
    "\n",
    "            # Create list of base conv layers.\n",
    "            base_conv_tensors = [\n",
    "                # The base conv block is always the 0th one.\n",
    "                self.conv_layer_blocks[0][i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + conv_block[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_generator_base_conv_layer_block\",\n",
    "                \"base_conv_tensors\",\n",
    "                base_conv_tensors\n",
    "            )\n",
    "\n",
    "        return base_conv_tensors\n",
    "\n",
    "    def build_generator_growth_layer_block(\n",
    "            self, params, growth_block_idx):\n",
    "        \"\"\"Builds generator growth block internals through call.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "            growth_block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of growth block tensors.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"generator_growth_conv_blocks\"][growth_block_idx]\n",
    "\n",
    "            # Create new inner convolutional layers.\n",
    "            conv_tensors = [\n",
    "                self.conv_layer_blocks[1 + growth_block_idx][i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + conv_block[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_generator_growth_layer_block\",\n",
    "                \"conv_tensors\",\n",
    "                conv_tensors\n",
    "            )\n",
    "\n",
    "        return conv_tensors\n",
    "\n",
    "    def build_generator_to_rgb_layers(self, params):\n",
    "        \"\"\"Builds generator toRGB layers of 1x1 convs internals through call.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of toRGB 1x1 conv tensors.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get toRGB layer properties.\n",
    "            to_rgb = [\n",
    "                params[\"generator_to_rgb_layers\"][i][0][:]\n",
    "                for i in range(len(params[\"generator_to_rgb_layers\"]))\n",
    "            ]\n",
    "\n",
    "            # Create list to hold toRGB 1x1 convs.\n",
    "            to_rgb_conv_tensors = [\n",
    "                self.to_rgb_conv_layers[i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + to_rgb[i][0:3], dtype=tf.float32)\n",
    "                    )\n",
    "                for i in range(len(to_rgb))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_generator_to_rgb_layers\",\n",
    "                \"to_rgb_conv_tensors\",\n",
    "                to_rgb_conv_tensors\n",
    "            )\n",
    "\n",
    "        return to_rgb_conv_tensors\n",
    "\n",
    "    def build_generator_layers(self, params):\n",
    "        \"\"\"Builds generator layer internals.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of toRGB tensors.\n",
    "        \"\"\"\n",
    "        # Build projection layer internals using call.\n",
    "        projection_tensor = self.build_generator_projection_layer(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\nbuild_generator_layers\",\n",
    "            \"projection_tensor\",\n",
    "            projection_tensor\n",
    "        )\n",
    "\n",
    "        with tf.control_dependencies(control_inputs=[projection_tensor]):\n",
    "            # Build base convolutional layer block's internals using call.\n",
    "            conv_block_tensors = [\n",
    "                self.build_generator_base_conv_layer_block(\n",
    "                    params=params\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Build growth block layer internals through call.\n",
    "            conv_block_tensors.extend(\n",
    "                [\n",
    "                    self.build_generator_growth_layer_block(\n",
    "                        params=params,\n",
    "                        growth_block_idx=growth_block_idx\n",
    "                    )\n",
    "                    for growth_block_idx in range(\n",
    "                        len(params[\"generator_growth_conv_blocks\"])\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            print_obj(\n",
    "                \"build_generator_layers\",\n",
    "                \"conv_block_tensors\",\n",
    "                conv_block_tensors\n",
    "            )\n",
    "\n",
    "            # Flatten block tensor lists of lists into list.\n",
    "            conv_block_tensors = [\n",
    "                item for sublist in conv_block_tensors for item in sublist\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"build_generator_layers\",\n",
    "                \"conv_block_tensors\",\n",
    "                conv_block_tensors\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies(\n",
    "                    control_inputs=conv_block_tensors):\n",
    "                # Build toRGB 1x1 conv layer internals through call.\n",
    "                to_rgb_conv_tensors = self.build_generator_to_rgb_layers(\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"build_generator_layers\",\n",
    "                    \"to_rgb_conv_tensors\",\n",
    "                    to_rgb_conv_tensors\n",
    "                )\n",
    "\n",
    "        return to_rgb_conv_tensors\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def pixel_norm(self, X, epsilon=1e-8):\n",
    "        \"\"\"Normalizes the feature vector in each pixel to unit length.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image feature vectors.\n",
    "            epsilon: float, small value to add to denominator for numerical\n",
    "                stability.\n",
    "\n",
    "        Returns:\n",
    "            Pixel normalized feature vectors.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"{}/pixel_norm\".format(self.name)):\n",
    "            return X * tf.rsqrt(\n",
    "                x=tf.add(\n",
    "                    x=tf.reduce_mean(\n",
    "                        input_tensor=tf.square(x=X), axis=1, keepdims=True\n",
    "                    ),\n",
    "                    y=epsilon\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def use_pixel_norm(self, X, params, epsilon=1e-8):\n",
    "        \"\"\"Decides based on user parameter whether to use pixel norm or not.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image feature vectors.\n",
    "            params: dict, user passed parameters.\n",
    "            epsilon: float, small value to add to denominator for numerical\n",
    "                stability.\n",
    "\n",
    "        Returns:\n",
    "            Pixel normalized feature vectors if using pixel norm, else\n",
    "                original feature vectors.\n",
    "        \"\"\"\n",
    "        if params[\"use_pixel_norm\"]:\n",
    "            return self.pixel_norm(X=X, epsilon=epsilon)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def use_generator_projection_layer(self, Z, params):\n",
    "        \"\"\"Uses projection layer to convert random noise vector into an image.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Latent vector projection tensor.\n",
    "        \"\"\"\n",
    "        # Project latent vectors.\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            if params[\"normalize_latent\"]:\n",
    "                # shape = (cur_batch_size, latent_size)\n",
    "                Z = self.pixel_norm(X=Z, epsilon=params[\"pixel_norm_epsilon\"])\n",
    "\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     projection_height * projection_width * projection_depth\n",
    "            # )\n",
    "            projection_tensor = self.projection_layer(inputs=Z)\n",
    "            print_obj(\n",
    "                \"\\nuse_generator_projection_layer\",\n",
    "                \"projection_tensor\",\n",
    "                projection_tensor\n",
    "            )\n",
    "\n",
    "        # Reshape projection into \"image\".\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height,\n",
    "        #     projection_width,\n",
    "        #     projection_depth\n",
    "        # )\n",
    "        projection_tensor_reshaped = tf.reshape(\n",
    "            tensor=projection_tensor,\n",
    "            shape=[-1] + params[\"generator_projection_dims\"],\n",
    "            name=\"{}_projection_reshaped\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"use_generator_projection_layer\",\n",
    "            \"projection_tensor_reshaped\",\n",
    "            projection_tensor_reshaped\n",
    "        )\n",
    "\n",
    "        return projection_tensor_reshaped\n",
    "\n",
    "    def fused_conv2d_pixel_norm(self, input_image, conv2d_layer, params):\n",
    "        \"\"\"Fused `Conv2D` layer and pixel norm operation.\n",
    "\n",
    "        Args:\n",
    "            input_image: tensor, input image of rank 4.\n",
    "            conv2d_layer: `Conv2D` layer.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            New image tensor of rank 4.\n",
    "        \"\"\"\n",
    "        conv_output = conv2d_layer(inputs=input_image)\n",
    "        print_obj(\"\\nfused_conv2d_pixel_norm\", \"conv_output\", conv_output)\n",
    "\n",
    "        pixel_norm_output = self.use_pixel_norm(\n",
    "            X=conv_output,\n",
    "            params=params,\n",
    "            epsilon=params[\"pixel_norm_epsilon\"]\n",
    "        )\n",
    "        print_obj(\n",
    "            \"fused_conv2d_pixel_norm\", \"pixel_norm_output\", pixel_norm_output\n",
    "        )\n",
    "\n",
    "        return pixel_norm_output\n",
    "\n",
    "    def upsample_generator_image(self, image, original_image_size, block_idx):\n",
    "        \"\"\"Upsamples generator image.\n",
    "\n",
    "        Args:\n",
    "            image: tensor, image created by generator conv block.\n",
    "            original_image_size: list, the height and width dimensions of the\n",
    "                original image before any growth.\n",
    "            block_idx: int, index of the current generator growth block.\n",
    "\n",
    "        Returns:\n",
    "            Upsampled image tensor.\n",
    "        \"\"\"\n",
    "        # Upsample from s X s to 2s X 2s image.\n",
    "        upsampled_image = tf.image.resize(\n",
    "            images=image,\n",
    "            size=tf.convert_to_tensor(\n",
    "                value=original_image_size,\n",
    "                dtype=tf.int32,\n",
    "                name=\"{}_upsample_generator_image_original_image_size\".format(\n",
    "                    self.name\n",
    "                )\n",
    "            ) * 2 ** block_idx,\n",
    "            method=\"nearest\",\n",
    "            name=\"{}_growth_upsampled_image_{}_{}x{}_{}x{}\".format(\n",
    "                self.name,\n",
    "                block_idx,\n",
    "                original_image_size[0] * 2 ** (block_idx - 1),\n",
    "                original_image_size[1] * 2 ** (block_idx - 1),\n",
    "                original_image_size[0] * 2 ** block_idx,\n",
    "                original_image_size[1] * 2 ** block_idx\n",
    "            )\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\nupsample_generator_image\",\n",
    "            \"upsampled_image\",\n",
    "            upsampled_image\n",
    "        )\n",
    "\n",
    "        return upsampled_image\n",
    "\n",
    "    def create_base_generator_network(self, Z, params):\n",
    "        \"\"\"Creates base generator network.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            projection_layer: `Dense` layer for projection of noise into image.\n",
    "            to_rgb_conv_layers: list, toRGB 1x1 conv layers.\n",
    "            blocks: list, lists of block layers for each block.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final network block conv tensor.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\ncreate_base_generator_network\", \"Z\", Z)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Project latent noise vectors into image.\n",
    "            projection = self.use_generator_projection_layer(\n",
    "                Z=Z, params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_base_generator_network\", \"projection\", projection\n",
    "            )\n",
    "\n",
    "            # Only need the first block and toRGB conv layer for base network.\n",
    "            block_layers = self.conv_layer_blocks[0]\n",
    "            to_rgb_conv_layer = self.to_rgb_conv_layers[0]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = projection\n",
    "            for i in range(0, len(block_layers)):\n",
    "                block_conv = self.fused_conv2d_pixel_norm(\n",
    "                    input_image=block_conv,\n",
    "                    conv2d_layer=block_layers[i],\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_base_generator_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Convert convolution to RGB image.\n",
    "            to_rgb_conv = self.fused_conv2d_pixel_norm(\n",
    "                input_image=block_conv,\n",
    "                conv2d_layer=to_rgb_conv_layer,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_base_generator_network\", \"to_rgb_conv\", to_rgb_conv\n",
    "            )\n",
    "\n",
    "        return to_rgb_conv\n",
    "\n",
    "    def create_growth_transition_generator_network(\n",
    "            self, Z, original_image_size, alpha_var, params, trans_idx):\n",
    "        \"\"\"Creates growth transition generator network.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            original_image_size: list, the height and width dimensions of the\n",
    "                original image before any growth.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "            trans_idx: int, index of current growth transition.\n",
    "\n",
    "        Returns:\n",
    "            Weighted sum tensor of growing and shrinking network paths.\n",
    "        \"\"\"\n",
    "        print_obj(\n",
    "            \"\\nEntered create_growth_transition_generator_network\",\n",
    "            \"trans_idx\",\n",
    "            trans_idx\n",
    "        )\n",
    "        print_obj(\"create_growth_transition_generator_network\", \"Z\", Z)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Project latent noise vectors into image.\n",
    "            projection = self.use_generator_projection_layer(\n",
    "                Z=Z, params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"projection\",\n",
    "                projection\n",
    "            )\n",
    "\n",
    "            # Permanent blocks.\n",
    "            permanent_blocks = self.conv_layer_blocks[0:trans_idx + 1]\n",
    "\n",
    "            # Base block doesn't need any upsampling so handle differently.\n",
    "            base_block_conv_layers = permanent_blocks[0]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = projection\n",
    "            for i in range(0, len(base_block_conv_layers)):\n",
    "                block_conv = self.fused_conv2d_pixel_norm(\n",
    "                    input_image=block_conv,\n",
    "                    conv2d_layer=base_block_conv_layers[i],\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_generator_network\",\n",
    "                    \"base_block_conv_{}_{}\".format(trans_idx, i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Growth blocks require first prev conv layer's image upsampled.\n",
    "            for i in range(1, len(permanent_blocks)):\n",
    "                # Upsample previous block's image.\n",
    "                block_conv = self.upsample_generator_image(\n",
    "                    image=block_conv,\n",
    "                    original_image_size=original_image_size,\n",
    "                    block_idx=i\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_generator_network\",\n",
    "                    \"upsample_generator_image_block_conv_{}_{}\".format(\n",
    "                        trans_idx, i\n",
    "                    ),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "                block_conv_layers = permanent_blocks[i]\n",
    "                for j in range(0, len(block_conv_layers)):\n",
    "                    block_conv = self.fused_conv2d_pixel_norm(\n",
    "                        input_image=block_conv,\n",
    "                        conv2d_layer=block_conv_layers[j],\n",
    "                        params=params\n",
    "                    )\n",
    "                    print_obj(\n",
    "                        \"create_growth_transition_generator_network\",\n",
    "                        \"block_conv_{}_{}_{}\".format(trans_idx, i, j),\n",
    "                        block_conv\n",
    "                    )\n",
    "\n",
    "            # Upsample most recent block conv image for both side chains.\n",
    "            upsampled_block_conv = self.upsample_generator_image(\n",
    "                image=block_conv,\n",
    "                original_image_size=original_image_size,\n",
    "                block_idx=len(permanent_blocks)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"upsampled_block_conv_{}\".format(trans_idx),\n",
    "                upsampled_block_conv\n",
    "            )\n",
    "\n",
    "            # Growing side chain.\n",
    "            growing_block_layers = self.conv_layer_blocks[trans_idx + 1]\n",
    "            growing_to_rgb_conv_layer = self.to_rgb_conv_layers[trans_idx + 1]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = upsampled_block_conv\n",
    "            for i in range(0, len(growing_block_layers)):\n",
    "                block_conv = self.fused_conv2d_pixel_norm(\n",
    "                    input_image=block_conv,\n",
    "                    conv2d_layer=growing_block_layers[i],\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_generator_network\",\n",
    "                    \"growing_block_conv_{}_{}\".format(trans_idx, i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            growing_to_rgb_conv = self.fused_conv2d_pixel_norm(\n",
    "                input_image=block_conv,\n",
    "                conv2d_layer=growing_to_rgb_conv_layer,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"growing_to_rgb_conv_{}\".format(trans_idx),\n",
    "                growing_to_rgb_conv\n",
    "            )\n",
    "\n",
    "            # Shrinking side chain.\n",
    "            shrinking_to_rgb_conv_layer = self.to_rgb_conv_layers[trans_idx]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            shrinking_to_rgb_conv = self.fused_conv2d_pixel_norm(\n",
    "                input_image=upsampled_block_conv,\n",
    "                conv2d_layer=shrinking_to_rgb_conv_layer,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"shrinking_to_rgb_conv_{}\".format(trans_idx),\n",
    "                shrinking_to_rgb_conv\n",
    "            )\n",
    "\n",
    "            # Weighted sum.\n",
    "            weighted_sum = tf.add(\n",
    "                x=growing_to_rgb_conv * alpha_var,\n",
    "                y=shrinking_to_rgb_conv * (1.0 - alpha_var),\n",
    "                name=\"growth_transition_weighted_sum_{}\".format(trans_idx)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_generator_network\",\n",
    "                \"weighted_sum_{}\".format(trans_idx),\n",
    "                weighted_sum\n",
    "            )\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "    def create_final_generator_network(self, Z, original_image_size, params):\n",
    "        \"\"\"Creates final generator network.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            original_image_size: list, the height and width dimensions of the\n",
    "                original image before any growth.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final network block conv tensor.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\ncreate_final_generator_network\", \"Z\", Z)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Project latent noise vectors into image.\n",
    "            projection = self.use_generator_projection_layer(\n",
    "                Z=Z, params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_final_generator_network\", \"projection\", projection\n",
    "            )\n",
    "\n",
    "            # Base block doesn't need any upsampling so handle differently.\n",
    "            base_block_conv_layers = self.conv_layer_blocks[0]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = projection\n",
    "            for i in range(0, len(base_block_conv_layers)):\n",
    "                block_conv = self.fused_conv2d_pixel_norm(\n",
    "                    input_image=block_conv,\n",
    "                    conv2d_layer=base_block_conv_layers[i],\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_final_generator_network\",\n",
    "                    \"base_block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Growth blocks require first prev conv layer's image upsampled.\n",
    "            for i in range(1, len(self.conv_layer_blocks)):\n",
    "                # Upsample previous block's image.\n",
    "                block_conv = self.upsample_generator_image(\n",
    "                    image=block_conv,\n",
    "                    original_image_size=original_image_size,\n",
    "                    block_idx=i\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_final_generator_network\",\n",
    "                    \"upsample_generator_image_block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "                block_conv_layers = self.conv_layer_blocks[i]\n",
    "                for j in range(0, len(block_conv_layers)):\n",
    "                    block_conv = self.fused_conv2d_pixel_norm(\n",
    "                        input_image=block_conv,\n",
    "                        conv2d_layer=block_conv_layers[j],\n",
    "                        params=params\n",
    "                    )\n",
    "                    print_obj(\n",
    "                        \"create_final_generator_network\",\n",
    "                        \"block_conv_{}_{}\".format(i, j),\n",
    "                        block_conv\n",
    "                    )\n",
    "\n",
    "            # Only need the last toRGB conv layer.\n",
    "            to_rgb_conv_layer = self.to_rgb_conv_layers[-1]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            to_rgb_conv = self.fused_conv2d_pixel_norm(\n",
    "                input_image=block_conv,\n",
    "                conv2d_layer=to_rgb_conv_layer,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_final_generator_network\", \"to_rgb_conv\", to_rgb_conv\n",
    "            )\n",
    "\n",
    "        return to_rgb_conv\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def switch_case_generator_outputs(\n",
    "            self, Z, original_image_size, alpha_var, params, growth_index):\n",
    "        \"\"\"Uses switch case to use the correct network to generate images.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            original_image_size: list, the height and width dimensions of the\n",
    "                original image before any growth.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "            growth_index: int, current growth stage.\n",
    "\n",
    "        Returns:\n",
    "            Generated image output tensor.\n",
    "        \"\"\"\n",
    "        # Switch to case based on number of steps for gen outputs.\n",
    "        generated_outputs = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                # 4x4\n",
    "                lambda: self.create_base_generator_network(\n",
    "                    Z=Z, params=params\n",
    "                ),\n",
    "                # 8x8\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(0, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 16x16\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(1, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 32x32\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(2, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 64x64\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(3, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 128x128\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(4, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 256x256\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(5, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 512x512\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(6, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: self.create_growth_transition_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(7, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: self.create_final_generator_network(\n",
    "                    Z=Z,\n",
    "                    original_image_size=original_image_size,\n",
    "                    params=params\n",
    "                )\n",
    "            ],\n",
    "            name=\"{}_switch_case_generated_outputs\".format(self.name)\n",
    "        )\n",
    "\n",
    "        return generated_outputs\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def get_train_eval_generator_outputs(self, Z, alpha_var, params):\n",
    "        \"\"\"Uses generator network and returns image for train/eval.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated image output tensor of shape\n",
    "                [cur_batch_size, image_size, image_size, depth].\n",
    "        \"\"\"\n",
    "        print_obj(\"\\nget_train_eval_generator_outputs\", \"Z\", Z)\n",
    "\n",
    "        # Get generator's output image tensor.\n",
    "        train_steps = params[\"train_steps\"] + params[\"prev_train_steps\"]\n",
    "        num_steps_until_growth = params[\"num_steps_until_growth\"]\n",
    "        num_stages = train_steps // num_steps_until_growth\n",
    "        if (num_stages <= 0 or len(params[\"conv_num_filters\"]) == 1):\n",
    "            print(\n",
    "                \"\\nget_train_eval_generator_outputs: NOT GOING TO GROW, SKIP SWITCH CASE!\"\n",
    "            )\n",
    "            # If never going to grow, no sense using the switch case.\n",
    "            # 4x4\n",
    "            generated_outputs = self.create_base_generator_network(\n",
    "                Z=Z, params=params\n",
    "            )\n",
    "        else:\n",
    "            # Find growth index based on global step and growth frequency.\n",
    "            growth_index = tf.cast(\n",
    "                x=tf.floordiv(\n",
    "                    x=tf.train.get_or_create_global_step(),\n",
    "                    y=params[\"num_steps_until_growth\"],\n",
    "                    name=\"{}_global_step_floordiv\".format(self.name)\n",
    "                ),\n",
    "                dtype=tf.int32,\n",
    "                name=\"{}_growth_index\".format(self.name)\n",
    "            )\n",
    "\n",
    "            # Switch to case based on number of steps for gen outputs.\n",
    "            generated_outputs = self.switch_case_generator_outputs(\n",
    "                Z=Z,\n",
    "                original_image_size=params[\"generator_projection_dims\"][0:2],\n",
    "                alpha_var=alpha_var,\n",
    "                params=params,\n",
    "                growth_index=growth_index\n",
    "            )\n",
    "\n",
    "        print_obj(\n",
    "            \"\\nget_train_eval_generator_outputs\",\n",
    "            \"generated_outputs\",\n",
    "            generated_outputs\n",
    "        )\n",
    "\n",
    "        # Wrap generated outputs in a control dependency for the build\n",
    "        # generator tensors to ensure generator internals are built.\n",
    "        with tf.control_dependencies(\n",
    "                control_inputs=self.build_generator_tensors):\n",
    "            generated_outputs = tf.identity(\n",
    "                input=generated_outputs,\n",
    "                name=\"{}_generated_outputs_identity\".format(self.name)\n",
    "            )\n",
    "\n",
    "        return generated_outputs\n",
    "\n",
    "    def get_predict_generator_outputs(self, Z, params, block_idx):\n",
    "        \"\"\"Uses generator network and returns image for predict.\n",
    "\n",
    "        Args:\n",
    "            Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, current conv layer block's index.\n",
    "\n",
    "        Returns:\n",
    "            Generated image output tensor of shape\n",
    "                [cur_batch_size, image_size, image_size, depth] or list of\n",
    "                them for each resolution.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\nget_predict_generator_outputs\", \"Z\", Z)\n",
    "\n",
    "        # Get generator's generated image.\n",
    "        if block_idx == 0:\n",
    "            # 4x4\n",
    "            generated_outputs = self.create_base_generator_network(\n",
    "                Z=Z, params=params\n",
    "            )\n",
    "        elif block_idx < len(params[\"conv_num_filters\"]) - 1:\n",
    "            # 8x8 through 512x512\n",
    "            generated_outputs = self.create_growth_transition_generator_network(\n",
    "                Z=Z,\n",
    "                original_image_size=params[\"generator_projection_dims\"][0:2],\n",
    "                alpha_var=tf.ones(shape=[], dtype=tf.float32),\n",
    "                params=params,\n",
    "                trans_idx=block_idx - 1\n",
    "            )\n",
    "        else:\n",
    "            # 1024x1024\n",
    "            generated_outputs = self.create_final_generator_network(\n",
    "                Z=Z,\n",
    "                original_image_size=params[\"generator_projection_dims\"][0:2],\n",
    "                params=params\n",
    "            )\n",
    "        print_obj(\n",
    "            \"get_predict_generator_outputs\",\n",
    "            \"generated_outputs\",\n",
    "            generated_outputs\n",
    "        )\n",
    "\n",
    "        return generated_outputs\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def get_generator_loss(self, fake_logits, params):\n",
    "        \"\"\"Gets generator loss.\n",
    "\n",
    "        Args:\n",
    "            fake_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "                discriminator having processed generator's output image.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generator's total loss tensor of shape [].\n",
    "        \"\"\"\n",
    "        # Calculate base generator loss.\n",
    "        generator_loss = -tf.reduce_mean(\n",
    "            input_tensor=fake_logits,\n",
    "            name=\"{}_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\"\\nget_generator_loss\", \"generator_loss\", generator_loss)\n",
    "\n",
    "        # Get generator regularization losses.\n",
    "        generator_reg_loss = regularization.get_regularization_loss(\n",
    "            lambda1=params[\"generator_l1_regularization_scale\"],\n",
    "            lambda2=params[\"generator_l2_regularization_scale\"],\n",
    "            scope=self.name\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_generator_loss\",\n",
    "            \"generator_reg_loss\",\n",
    "            generator_reg_loss\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        generator_total_loss = tf.math.add(\n",
    "            x=generator_loss,\n",
    "            y=generator_reg_loss,\n",
    "            name=\"{}_total_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_generator_loss\", \"generator_total_loss\", generator_total_loss\n",
    "        )\n",
    "\n",
    "        return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/discriminator.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import regularization\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Discriminator(object):\n",
    "    \"\"\"Discriminator that takes image input and outputs logits.\n",
    "\n",
    "    Fields:\n",
    "        name: str, name of `Discriminator`.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "        from_rgb_conv_layers: list, fromRGB 1x1 `Conv2D` layers.\n",
    "        conv_layer_blocks: list, lists of `Conv2D` block layers for each\n",
    "            block.\n",
    "        transition_downsample_layers: list, `AveragePooling2D` layers for\n",
    "            downsampling shrinking transition paths.\n",
    "        flatten_layer: `Flatten` layer prior to logits layer.\n",
    "        logits_layer: `Dense` layer for logits.\n",
    "        build_discriminator_tensors: list, tensors used to build layer\n",
    "            internals.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, params, name):\n",
    "        \"\"\"Instantiates and builds discriminator network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            params: dict, user passed parameters.\n",
    "            name: str, name of discriminator.\n",
    "        \"\"\"\n",
    "        # Set name of discriminator.\n",
    "        self.name = name\n",
    "\n",
    "        # Regularizer for kernel weights.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        # Regularizer for bias weights.\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        # Instantiate discriminator layers.\n",
    "        (self.from_rgb_conv_layers,\n",
    "         self.conv_layer_blocks,\n",
    "         self.transition_downsample_layers,\n",
    "         self.flatten_layer,\n",
    "         self.logits_layer) = self.instantiate_discriminator_layers(\n",
    "            params\n",
    "        )\n",
    "\n",
    "        # Build discriminator layer internals.\n",
    "        self.build_discriminator_tensors = self.build_discriminator_layers(\n",
    "            params\n",
    "        )\n",
    "\n",
    "    def instantiate_discriminator_from_rgb_layers(self, params):\n",
    "        \"\"\"Instantiates discriminator fromRGB layers of 1x1 convs.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of fromRGB 1x1 Conv2D layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get fromRGB layer properties.\n",
    "            from_rgb = [\n",
    "                params[\"discriminator_from_rgb_layers\"][i][0][:]\n",
    "                for i in range(len(params[\"discriminator_from_rgb_layers\"]))\n",
    "            ]\n",
    "\n",
    "            # Create list to hold toRGB 1x1 convs.\n",
    "            from_rgb_conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=from_rgb[i][3],\n",
    "                    kernel_size=from_rgb[i][0:2],\n",
    "                    strides=from_rgb[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_from_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        i,\n",
    "                        from_rgb[i][0],\n",
    "                        from_rgb[i][1],\n",
    "                        from_rgb[i][2],\n",
    "                        from_rgb[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(from_rgb))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_discriminator_from_rgb_layers\",\n",
    "                \"from_rgb_conv_layers\",\n",
    "                from_rgb_conv_layers\n",
    "            )\n",
    "\n",
    "        return from_rgb_conv_layers\n",
    "\n",
    "    def instantiate_discriminator_base_conv_layer_block(self, params):\n",
    "        \"\"\"Instantiates discriminator base conv layer block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of base conv layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"discriminator_base_conv_blocks\"][0]\n",
    "\n",
    "            # Create list of base conv layers.\n",
    "            base_conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[i][3],\n",
    "                    kernel_size=conv_block[i][0:2],\n",
    "                    strides=conv_block[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        i,\n",
    "                        conv_block[i][0],\n",
    "                        conv_block[i][1],\n",
    "                        conv_block[i][2],\n",
    "                        conv_block[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block) - 1)\n",
    "            ]\n",
    "\n",
    "            # Have valid padding for layer just before flatten and logits.\n",
    "            base_conv_layers.append(\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[-1][3],\n",
    "                    kernel_size=conv_block[-1][0:2],\n",
    "                    strides=conv_block[-1][4:6],\n",
    "                    padding=\"valid\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        len(conv_block) - 1,\n",
    "                        conv_block[-1][0],\n",
    "                        conv_block[-1][1],\n",
    "                        conv_block[-1][2],\n",
    "                        conv_block[-1][3]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_discriminator_base_conv_layer_block\",\n",
    "                \"base_conv_layers\",\n",
    "                base_conv_layers\n",
    "            )\n",
    "\n",
    "        return base_conv_layers\n",
    "\n",
    "    def instantiate_discriminator_growth_layer_block(self, params, block_idx):\n",
    "        \"\"\"Instantiates discriminator growth block layers.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of growth block layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"discriminator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "            # Create new inner convolutional layers.\n",
    "            conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[i][3],\n",
    "                    kernel_size=conv_block[i][0:2],\n",
    "                    strides=conv_block[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        block_idx,\n",
    "                        i,\n",
    "                        conv_block[i][0],\n",
    "                        conv_block[i][1],\n",
    "                        conv_block[i][2],\n",
    "                        conv_block[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_discriminator_growth_layer_block\",\n",
    "                \"conv_layers\",\n",
    "                conv_layers\n",
    "            )\n",
    "\n",
    "            # Down sample from 2s X 2s to s X s image.\n",
    "            downsampled_image_layer = tf.layers.AveragePooling2D(\n",
    "                pool_size=(2, 2),\n",
    "                strides=(2, 2),\n",
    "                name=\"{}_growth_downsampled_image_{}\".format(\n",
    "                    self.name,\n",
    "                    block_idx\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"instantiate_discriminator_growth_layer_block\",\n",
    "                \"downsampled_image_layer\",\n",
    "                downsampled_image_layer\n",
    "            )\n",
    "\n",
    "        return conv_layers + [downsampled_image_layer]\n",
    "\n",
    "    def instantiate_discriminator_growth_transition_downsample_layers(\n",
    "            self, params):\n",
    "        \"\"\"Instantiates discriminator growth transition downsample layers.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of growth transition downsample layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Down sample from 2s X 2s to s X s image.\n",
    "            downsample_layers = [\n",
    "                tf.layers.AveragePooling2D(\n",
    "                    pool_size=(2, 2),\n",
    "                    strides=(2, 2),\n",
    "                    name=\"{}_growth_transition_downsample_layer_{}\".format(\n",
    "                        self.name,\n",
    "                        layer_idx\n",
    "                    )\n",
    "                )\n",
    "                for layer_idx in range(\n",
    "                    1 + len(params[\"discriminator_growth_conv_blocks\"])\n",
    "                )\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_discriminator_growth_transition_downsample_layers\",\n",
    "                \"downsample_layers\",\n",
    "                downsample_layers\n",
    "            )\n",
    "\n",
    "        return downsample_layers\n",
    "\n",
    "    def instantiate_discriminator_logits_layer(self):\n",
    "        \"\"\"Instantiates discriminator flatten and logits layers.\n",
    "\n",
    "        Returns:\n",
    "            Flatten and logits layers of discriminator.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Flatten layer to ready final block conv tensor for dense layer.\n",
    "            flatten_layer = tf.layers.Flatten(\n",
    "                name=\"{}_flatten_layer\".format(self.name)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\ncreate_discriminator_logits_layer\",\n",
    "                \"flatten_layer\",\n",
    "                flatten_layer\n",
    "            )\n",
    "\n",
    "            # Final linear layer for logits.\n",
    "            logits_layer = tf.layers.Dense(\n",
    "                units=1,\n",
    "                activation=None,\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"{}_layers_dense_logits\".format(self.name)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_discriminator_network\",\n",
    "                \"logits_layer\",\n",
    "                logits_layer\n",
    "            )\n",
    "\n",
    "        return flatten_layer, logits_layer\n",
    "\n",
    "    def instantiate_discriminator_layers(self, params):\n",
    "        \"\"\"Instantiates layers of discriminator network.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            from_rgb_conv_layers: list, fromRGB 1x1 `Conv2D` layers.\n",
    "            conv_layer_blocks: list, lists of `Conv2D` block layers for each\n",
    "                block.\n",
    "            transition_downsample_layers: list, `AveragePooling2D` layers for\n",
    "                downsampling shrinking transition paths.\n",
    "            flatten_layer: `Flatten` layer prior to logits layer.\n",
    "            logits_layer: `Dense` layer for logits.\n",
    "        \"\"\"\n",
    "        # Instantiate fromRGB 1x1 `Conv2D` layers.\n",
    "        from_rgb_conv_layers = self.instantiate_discriminator_from_rgb_layers(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_discriminator_layers\",\n",
    "            \"from_rgb_conv_layers\",\n",
    "            from_rgb_conv_layers\n",
    "        )\n",
    "\n",
    "        # Instantiate base conv block's `Conv2D` layers, for post-growth.\n",
    "        conv_layer_blocks = [\n",
    "            self.instantiate_discriminator_base_conv_layer_block(\n",
    "                params=params\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Instantiate growth `Conv2D` layer blocks.\n",
    "        conv_layer_blocks.extend(\n",
    "            [\n",
    "                self.instantiate_discriminator_growth_layer_block(\n",
    "                    params=params,\n",
    "                    block_idx=block_idx\n",
    "                )\n",
    "                for block_idx in range(\n",
    "                    len(params[\"discriminator_growth_conv_blocks\"])\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_discriminator_layers\",\n",
    "            \"conv_layer_blocks\",\n",
    "            conv_layer_blocks\n",
    "        )\n",
    "\n",
    "        # Instantiate transition downsample `AveragePooling2D` layers.\n",
    "        transition_downsample_layers = (\n",
    "            self.instantiate_discriminator_growth_transition_downsample_layers(\n",
    "                params=params\n",
    "            )\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_discriminator_layers\",\n",
    "            \"transition_downsample_layers\",\n",
    "            transition_downsample_layers\n",
    "        )\n",
    "\n",
    "        # Instantiate `Flatten` and `Dense` logits layers.\n",
    "        (flatten_layer,\n",
    "         logits_layer) = self.instantiate_discriminator_logits_layer()\n",
    "        print_obj(\n",
    "            \"instantiate_discriminator_layers\",\n",
    "            \"flatten_layer\",\n",
    "            flatten_layer\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_discriminator_layers\",\n",
    "            \"logits_layer\",\n",
    "            logits_layer\n",
    "        )\n",
    "\n",
    "        return (from_rgb_conv_layers,\n",
    "                conv_layer_blocks,\n",
    "                transition_downsample_layers,\n",
    "                flatten_layer,\n",
    "                logits_layer)\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def build_discriminator_from_rgb_layers(self, params):\n",
    "        \"\"\"Creates discriminator fromRGB layers of 1x1 convs.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of tensors from fromRGB 1x1 `Conv2D` layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get fromRGB layer properties.\n",
    "            from_rgb = [\n",
    "                params[\"discriminator_from_rgb_layers\"][i][0][:]\n",
    "                for i in range(len(params[\"discriminator_from_rgb_layers\"]))\n",
    "            ]\n",
    "\n",
    "            # Create list to hold fromRGB 1x1 convs.\n",
    "            from_rgb_conv_tensors = [\n",
    "                self.from_rgb_conv_layers[i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + from_rgb[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(from_rgb))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_discriminator_from_rgb_layers\",\n",
    "                \"from_rgb_conv_tensors\",\n",
    "                from_rgb_conv_tensors\n",
    "            )\n",
    "\n",
    "        return from_rgb_conv_tensors\n",
    "\n",
    "    def build_discriminator_base_conv_layer_block(self, params):\n",
    "        \"\"\"Creates discriminator base conv layer block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of tensors from base `Conv2D` layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"discriminator_base_conv_blocks\"][0]\n",
    "\n",
    "            # The base conv block is always the 0th one.\n",
    "            base_conv_layer_block = self.conv_layer_blocks[0]\n",
    "\n",
    "            # batch_batch stddev comes before first base conv layer,\n",
    "            # creating 1 extra feature map.\n",
    "            if params[\"use_minibatch_stddev\"]:\n",
    "                # Therefore, the number of input channels will be 1 higher\n",
    "                # for first base conv block.\n",
    "                num_in_channels = conv_block[0][3] + 1\n",
    "            else:\n",
    "                num_in_channels = conv_block[0][3]\n",
    "\n",
    "            # Get first base conv layer from list.\n",
    "            first_base_conv_layer = base_conv_layer_block[0]\n",
    "\n",
    "            # Build first layer with bigger tensor.\n",
    "            base_conv_tensors = [\n",
    "                first_base_conv_layer(\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + conv_block[0][0:2] + [num_in_channels],\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Now build the rest of the base conv block layers, store in list.\n",
    "            base_conv_tensors.extend(\n",
    "                [\n",
    "                    base_conv_layer_block[i](\n",
    "                        inputs=tf.zeros(\n",
    "                            shape=[1] + conv_block[i][0:3], dtype=tf.float32\n",
    "                        )\n",
    "                    )\n",
    "                    for i in range(1, len(conv_block))\n",
    "                ]\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\nbuild_discriminator_base_conv_layer_block\",\n",
    "                \"base_conv_tensors\",\n",
    "                base_conv_tensors\n",
    "            )\n",
    "\n",
    "        return base_conv_tensors\n",
    "\n",
    "    def build_discriminator_growth_layer_block(self, params, block_idx):\n",
    "        \"\"\"Creates discriminator growth block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of tensors from growth block `Conv2D` layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"discriminator_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "            # Create new inner convolutional layers.\n",
    "            conv_tensors = [\n",
    "                self.conv_layer_blocks[1 + block_idx][i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + conv_block[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_discriminator_growth_layer_block\",\n",
    "                \"conv_tensors\",\n",
    "                conv_tensors\n",
    "            )\n",
    "\n",
    "        return conv_tensors\n",
    "\n",
    "    def build_discriminator_logits_layer(self, params):\n",
    "        \"\"\"Builds flatten and logits layer internals using call.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            block_conv_size = params[\"discriminator_base_conv_blocks\"][-1][-1][3]\n",
    "\n",
    "            # Flatten final block conv tensor.\n",
    "            block_conv_flat = self.flatten_layer(\n",
    "                inputs=tf.zeros(\n",
    "                    shape=[1, 1, 1, block_conv_size],\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\nbuild_discriminator_logits_layer\",\n",
    "                \"block_conv_flat\",\n",
    "                block_conv_flat\n",
    "            )\n",
    "\n",
    "            # Final linear layer for logits.\n",
    "            logits = self.logits_layer(inputs=block_conv_flat)\n",
    "            print_obj(\"build_discriminator_logits_layer\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def build_discriminator_layers(self, params):\n",
    "        \"\"\"Builds discriminator layer internals.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor.\n",
    "        \"\"\"\n",
    "        # Build fromRGB 1x1 `Conv2D` layers internals through call.\n",
    "        from_rgb_conv_tensors = self.build_discriminator_from_rgb_layers(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\nbuild_discriminator_layers\",\n",
    "            \"from_rgb_conv_tensors\",\n",
    "            from_rgb_conv_tensors\n",
    "        )\n",
    "\n",
    "        with tf.control_dependencies(control_inputs=from_rgb_conv_tensors):\n",
    "            # Create base convolutional block's layer internals using call.\n",
    "            conv_block_tensors = [\n",
    "                self.build_discriminator_base_conv_layer_block(\n",
    "                    params=params\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Build growth `Conv2D` layer block internals through call.\n",
    "            conv_block_tensors.extend(\n",
    "                [\n",
    "                    self.build_discriminator_growth_layer_block(\n",
    "                        params=params, block_idx=block_idx\n",
    "                    )\n",
    "                    for block_idx in range(\n",
    "                       len(params[\"discriminator_growth_conv_blocks\"])\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Flatten conv block tensor lists of lists into list.\n",
    "            conv_block_tensors = [\n",
    "                item for sublist in conv_block_tensors for item in sublist\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"build_discriminator_layers\",\n",
    "                \"conv_block_tensors\",\n",
    "                conv_block_tensors\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies(control_inputs=conv_block_tensors):\n",
    "                # Build logits layer internals using call.\n",
    "                logits_tensor = self.build_discriminator_logits_layer(\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"build_discriminator_layers\",\n",
    "                    \"logits_tensor\",\n",
    "                    logits_tensor\n",
    "                )\n",
    "\n",
    "        return logits_tensor\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def minibatch_stddev_common(\n",
    "            self,\n",
    "            variance,\n",
    "            tile_multiples,\n",
    "            params,\n",
    "            caller):\n",
    "        \"\"\"Adds minibatch stddev feature map to image using grouping.\n",
    "\n",
    "        This is the code that is common between the grouped and ungroup\n",
    "        minibatch stddev functions.\n",
    "\n",
    "        Args:\n",
    "            variance: tensor, variance of minibatch or minibatch groups.\n",
    "            tile_multiples: list, length 4, used to tile input to final shape\n",
    "                input_dims[i] * mutliples[i].\n",
    "            params: dict, user passed parameters.\n",
    "            caller: str, name of the calling function.\n",
    "\n",
    "        Returns:\n",
    "            Minibatch standard deviation feature map image added to\n",
    "                channels of shape\n",
    "                [cur_batch_size, image_size, image_size, 1].\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\n",
    "                \"{}/{}_minibatch_stddev\".format(self.name, caller)):\n",
    "            # Calculate standard deviation over the group plus small epsilon.\n",
    "            # shape = (\n",
    "            #     {\"grouped\": cur_batch_size / group_size, \"ungrouped\": 1},\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            stddev = tf.sqrt(\n",
    "                x=variance + 1e-8, name=\"{}_stddev\".format(caller)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"minibatch_stddev_common\", \"{}_stddev\".format(caller), stddev\n",
    "            )\n",
    "\n",
    "            # Take average over feature maps and pixels.\n",
    "            if params[\"minibatch_stddev_averaging\"]:\n",
    "                # grouped shape = (cur_batch_size / group_size, 1, 1, 1)\n",
    "                # ungrouped shape = (1, 1, 1, 1)\n",
    "                stddev = tf.reduce_mean(\n",
    "                    input_tensor=stddev,\n",
    "                    axis=[1, 2, 3],\n",
    "                    keepdims=True,\n",
    "                    name=\"{}_stddev_average\".format(caller)\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"minibatch_stddev_common\",\n",
    "                    \"{}_stddev_average\".format(caller),\n",
    "                    stddev\n",
    "                )\n",
    "\n",
    "            # Replicate over group and pixels.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     1\n",
    "            # )\n",
    "            stddev_feature_map = tf.tile(\n",
    "                input=stddev,\n",
    "                multiples=tile_multiples,\n",
    "                name=\"{}_stddev_feature_map\".format(caller)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"minibatch_stddev_common\",\n",
    "                \"{}_stddev_feature_map\".format(caller),\n",
    "                stddev_feature_map\n",
    "            )\n",
    "\n",
    "        return stddev_feature_map\n",
    "\n",
    "    def grouped_minibatch_stddev(\n",
    "            self,\n",
    "            X,\n",
    "            cur_batch_size,\n",
    "            static_image_shape,\n",
    "            params,\n",
    "            group_size):\n",
    "        \"\"\"Adds minibatch stddev feature map to image using grouping.\n",
    "\n",
    "        Args:\n",
    "            X: tf.float32 tensor, image of shape\n",
    "                [cur_batch_size, image_size, image_size, num_channels].\n",
    "            cur_batch_size: tf.int64 tensor, the dynamic batch size (in case\n",
    "                of partial batch).\n",
    "            static_image_shape: list, the static shape of each image.\n",
    "            params: dict, user passed parameters.\n",
    "            group_size: int, size of image groups.\n",
    "\n",
    "        Returns:\n",
    "            Minibatch standard deviation feature map image added to\n",
    "                channels of shape\n",
    "                [cur_batch_size, image_size, image_size, 1].\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\n",
    "                \"{}/grouped_minibatch_stddev\".format(self.name)):\n",
    "            # The group size should be less than or equal to the batch size.\n",
    "            if params[\"use_tpu\"]:\n",
    "                group_size = min(group_size, cur_batch_size)\n",
    "            else:\n",
    "                # shape = ()\n",
    "                group_size = tf.minimum(\n",
    "                    x=group_size, y=cur_batch_size, name=\"group_size\"\n",
    "                )\n",
    "            print_obj(\"grouped_minibatch_stddev\", \"group_size\", group_size)\n",
    "\n",
    "            # Split minibatch into M groups of size group_size, rank 5 tensor.\n",
    "            # shape = (\n",
    "            #     group_size,\n",
    "            #     cur_batch_size / group_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            grouped_image = tf.reshape(\n",
    "                tensor=X,\n",
    "                shape=[group_size, -1] + static_image_shape,\n",
    "                name=\"grouped_image\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"grouped_minibatch_stddev\",\n",
    "                \"grouped_image\",\n",
    "                grouped_image\n",
    "            )\n",
    "\n",
    "            # Find the mean of each group.\n",
    "            # shape = (\n",
    "            #     1,\n",
    "            #     cur_batch_size / group_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            grouped_mean = tf.reduce_mean(\n",
    "                input_tensor=grouped_image,\n",
    "                axis=0,\n",
    "                keepdims=True,\n",
    "                name=\"grouped_mean\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"grouped_minibatch_stddev\", \"grouped_mean\", grouped_mean\n",
    "            )\n",
    "\n",
    "            # Center each group using the mean.\n",
    "            # shape = (\n",
    "            #     group_size,\n",
    "            #     cur_batch_size / group_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            centered_grouped_image = tf.subtract(\n",
    "                x=grouped_image, y=grouped_mean, name=\"centered_grouped_image\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"grouped_minibatch_stddev\",\n",
    "                \"centered_grouped_image\",\n",
    "                centered_grouped_image\n",
    "            )\n",
    "\n",
    "            # Calculate variance over group.\n",
    "            # shape = (\n",
    "            #     cur_batch_size / group_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            grouped_variance = tf.reduce_mean(\n",
    "                input_tensor=tf.square(x=centered_grouped_image),\n",
    "                axis=0,\n",
    "                name=\"grouped_variance\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"grouped_minibatch_stddev\",\n",
    "                \"grouped_variance\",\n",
    "                grouped_variance\n",
    "            )\n",
    "\n",
    "            # Get stddev image using ops common to both grouped & ungrouped.\n",
    "            stddev_feature_map = self.minibatch_stddev_common(\n",
    "                variance=grouped_variance,\n",
    "                tile_multiples=[group_size] + static_image_shape[0:2] + [1],\n",
    "                params=params,\n",
    "                caller=\"grouped\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"grouped_minibatch_stddev\",\n",
    "                \"stddev_feature_map\",\n",
    "                stddev_feature_map\n",
    "            )\n",
    "\n",
    "        return stddev_feature_map\n",
    "\n",
    "    def ungrouped_minibatch_stddev(\n",
    "            self,\n",
    "            X,\n",
    "            cur_batch_size,\n",
    "            static_image_shape,\n",
    "            params):\n",
    "        \"\"\"Adds minibatch stddev feature map added to image channels.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image of shape\n",
    "                [cur_batch_size, image_size, image_size, num_channels].\n",
    "            cur_batch_size: tf.int64 tensor, the dynamic batch size (in case\n",
    "                of partial batch).\n",
    "            static_image_shape: list, the static shape of each image.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Minibatch standard deviation feature map image added to\n",
    "                channels of shape\n",
    "                [cur_batch_size, image_size, image_size, 1].\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\n",
    "                \"{}/ungrouped_minibatch_stddev\".format(self.name)):\n",
    "            # Find the mean of each group.\n",
    "            # shape = (\n",
    "            #     1,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            mean = tf.reduce_mean(\n",
    "                input_tensor=X, axis=0, keepdims=True, name=\"mean\"\n",
    "            )\n",
    "            print_obj(\"ungrouped_minibatch_stddev\", \"mean\", mean)\n",
    "\n",
    "            # Center each group using the mean.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            centered_image = tf.subtract(\n",
    "                x=X, y=mean, name=\"centered_image\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"ungrouped_minibatch_stddev\",\n",
    "                \"centered_image\",\n",
    "                centered_image\n",
    "            )\n",
    "\n",
    "            # Calculate variance over group.\n",
    "            # shape = (\n",
    "            #     1,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels\n",
    "            # )\n",
    "            variance = tf.reduce_mean(\n",
    "                input_tensor=tf.square(x=centered_image),\n",
    "                axis=0,\n",
    "                keepdims=True,\n",
    "                name=\"variance\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"ungrouped_minibatch_stddev\",\n",
    "                \"variance\",\n",
    "                variance\n",
    "            )\n",
    "\n",
    "            # Get stddev image using ops common to both grouped & ungrouped.\n",
    "            stddev_feature_map = self.minibatch_stddev_common(\n",
    "                variance=variance,\n",
    "                tile_multiples=[cur_batch_size] + static_image_shape[0:2] + [1],\n",
    "                params=params,\n",
    "                caller=\"ungrouped\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"ungrouped_minibatch_stddev\",\n",
    "                \"stddev_feature_map\",\n",
    "                stddev_feature_map\n",
    "            )\n",
    "\n",
    "        return stddev_feature_map\n",
    "\n",
    "    def minibatch_stddev(self, X, params, group_size=4):\n",
    "        \"\"\"Adds minibatch stddev feature map added to image.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image of shape\n",
    "                [cur_batch_size, image_size, image_size, num_channels].\n",
    "            params: dict, user passed parameters.\n",
    "            group_size: int, size of image groups.\n",
    "\n",
    "        Returns:\n",
    "            Image with minibatch standard deviation feature map added to\n",
    "                channels of shape\n",
    "                [cur_batch_size, image_size, image_size, num_channels + 1].\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"{}/minibatch_stddev\".format(self.name)):\n",
    "            # Get static shape of image.\n",
    "            # shape = (3,)\n",
    "            static_image_shape = params[\"generator_projection_dims\"]\n",
    "            print_obj(\n",
    "                \"minibatch_stddev\", \"static_image_shape\", static_image_shape\n",
    "            )\n",
    "\n",
    "            if params[\"use_tpu\"]:\n",
    "                if (params[\"batch_size\"] % group_size == 0 or\n",
    "                    params[\"batch_size\"] < group_size):\n",
    "                    stddev_feature_map = self.grouped_minibatch_stddev(\n",
    "                        X=X,\n",
    "                        cur_batch_size=params[\"batch_size\"],\n",
    "                        static_image_shape=static_image_shape,\n",
    "                        params=params,\n",
    "                        group_size=group_size\n",
    "                    )\n",
    "                else:\n",
    "                    stddev_feature_map = self.ungrouped_minibatch_stddev(\n",
    "                        X=X,\n",
    "                        cur_batch_size=params[\"batch_size\"],\n",
    "                        static_image_shape=static_image_shape,\n",
    "                        params=params\n",
    "                    )\n",
    "            else:\n",
    "                # Get dynamic shape of image.\n",
    "                # shape = (4,)\n",
    "                dynamic_image_shape = tf.shape(\n",
    "                    input=X, name=\"dynamic_image_shape\"\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"\\nminibatch_stddev\",\n",
    "                    \"dynamic_image_shape\",\n",
    "                    dynamic_image_shape\n",
    "                )\n",
    "\n",
    "                # Extract current batch size (in case this is a partial batch).\n",
    "                cur_batch_size = dynamic_image_shape[0]\n",
    "\n",
    "                # cur_batch_size must be divisible by or smaller than group_size.\n",
    "                divisbility_condition = tf.equal(\n",
    "                    x=tf.mod(x=cur_batch_size, y=group_size),\n",
    "                    y=0,\n",
    "                    name=\"divisbility_condition\"\n",
    "                )\n",
    "\n",
    "                less_than_condition = tf.less(\n",
    "                    x=cur_batch_size, y=group_size, name=\"less_than_condition\"\n",
    "                )\n",
    "\n",
    "                or_condition = tf.logical_or(\n",
    "                    x=divisbility_condition,\n",
    "                    y=less_than_condition,\n",
    "                    name=\"or_condition\"\n",
    "                )\n",
    "\n",
    "                # Get minibatch stddev feature map image from grouped or\n",
    "                # ungrouped branch.\n",
    "                stddev_feature_map = tf.cond(\n",
    "                    pred=or_condition,\n",
    "                    true_fn=lambda: self.grouped_minibatch_stddev(\n",
    "                        X=X,\n",
    "                        cur_batch_size=cur_batch_size,\n",
    "                        static_image_shape=static_image_shape,\n",
    "                        params=params,\n",
    "                        group_size=group_size\n",
    "                    ),\n",
    "                    false_fn=lambda: self.ungrouped_minibatch_stddev(\n",
    "                        X=X,\n",
    "                        cur_batch_size=cur_batch_size,\n",
    "                        static_image_shape=static_image_shape,\n",
    "                        params=params\n",
    "                    ),\n",
    "                    name=\"stddev_feature_map_cond\"\n",
    "                )\n",
    "            print_obj(\n",
    "                \"minibatch_stddev\", \"stddev_feature_map\", stddev_feature_map\n",
    "            )\n",
    "\n",
    "            # Append to image as new feature map.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     image_size,\n",
    "            #     image_size,\n",
    "            #     num_channels + 1\n",
    "            # )\n",
    "            appended_image = tf.concat(\n",
    "                values=[X, stddev_feature_map],\n",
    "                axis=-1,\n",
    "                name=\"appended_image\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"minibatch_stddev_common\",\n",
    "                \"appended_image\",\n",
    "                appended_image\n",
    "            )\n",
    "            \n",
    "        return appended_image\n",
    "\n",
    "    def use_discriminator_logits_layer(self, block_conv, params):\n",
    "        \"\"\"Uses flatten and logits layers to get logits tensor.\n",
    "\n",
    "        Args:\n",
    "            block_conv: tensor, output of last conv layer of discriminator.\n",
    "            flatten_layer: `Flatten` layer.\n",
    "            logits_layer: `Dense` layer for logits.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        print_obj(\n",
    "            \"\\nuse_discriminator_logits_layer\", \"block_conv\", block_conv\n",
    "        )\n",
    "        # Set shape to remove ambiguity for dense layer.\n",
    "        block_conv.set_shape(\n",
    "            [\n",
    "                block_conv.get_shape()[0],\n",
    "                params[\"generator_projection_dims\"][0] / 4,\n",
    "                params[\"generator_projection_dims\"][1] / 4,\n",
    "                block_conv.get_shape()[-1]]\n",
    "        )\n",
    "        print_obj(\"use_discriminator_logits_layer\", \"block_conv\", block_conv)\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Flatten final block conv tensor.\n",
    "            block_conv_flat = self.flatten_layer(inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"use_discriminator_logits_layer\",\n",
    "                \"block_conv_flat\",\n",
    "                block_conv_flat\n",
    "            )\n",
    "\n",
    "            # Final linear layer for logits.\n",
    "            logits = self.logits_layer(inputs=block_conv_flat)\n",
    "            print_obj(\"use_discriminator_logits_layer\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_base_discriminator_network(self, X, params):\n",
    "        \"\"\"Creates base discriminator network.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, input image to discriminator.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\ncreate_base_discriminator_network\", \"X\", X)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Only need the first fromRGB conv layer & block for base network.\n",
    "            from_rgb_conv_layer = self.from_rgb_conv_layers[0]\n",
    "            block_layers = self.conv_layer_blocks[0]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            from_rgb_conv = from_rgb_conv_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"create_base_discriminator_network\",\n",
    "                \"from_rgb_conv\",\n",
    "                from_rgb_conv\n",
    "            )\n",
    "\n",
    "            if params[\"use_minibatch_stddev\"]:\n",
    "                block_conv = self.minibatch_stddev(\n",
    "                    X=from_rgb_conv,\n",
    "                    params=params,\n",
    "                    group_size=params[\"minibatch_stddev_group_size\"]\n",
    "                )\n",
    "            else:\n",
    "                block_conv = from_rgb_conv\n",
    "\n",
    "            for i in range(len(block_layers)):\n",
    "                block_conv = block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_base_discriminator_network\",\n",
    "                    \"block_conv\",\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Get logits now.\n",
    "            logits = self.use_discriminator_logits_layer(\n",
    "                block_conv=block_conv,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\"create_base_discriminator_network\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_growth_transition_discriminator_network(\n",
    "            self, X, alpha_var, params, trans_idx):\n",
    "        \"\"\"Creates growth transition discriminator network.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, input image to discriminator.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "            trans_idx: int, index of current growth transition.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        print_obj(\n",
    "            \"\\nEntered create_growth_transition_discriminator_network\",\n",
    "            \"trans_idx\",\n",
    "            trans_idx\n",
    "        )\n",
    "        print_obj(\"create_growth_transition_discriminator_network\", \"X\", X)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Growing side chain.\n",
    "            growing_from_rgb_conv_layer = self.from_rgb_conv_layers[trans_idx + 1]\n",
    "            growing_block_layers = self.conv_layer_blocks[trans_idx + 1]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            growing_block_conv = growing_from_rgb_conv_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"\\ncreate_growth_transition_discriminator_network\",\n",
    "                \"growing_block_conv\",\n",
    "                growing_block_conv\n",
    "            )\n",
    "            for i in range(len(growing_block_layers)):\n",
    "                growing_block_conv = growing_block_layers[i](\n",
    "                    inputs=growing_block_conv\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_discriminator_network\",\n",
    "                    \"growing_block_conv\",\n",
    "                    growing_block_conv\n",
    "                )\n",
    "\n",
    "            # Shrinking side chain.\n",
    "            transition_downsample_layer = self.transition_downsample_layers[trans_idx]\n",
    "            shrinking_from_rgb_conv_layer = self.from_rgb_conv_layers[trans_idx]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            transition_downsample = transition_downsample_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"create_growth_transition_discriminator_network\",\n",
    "                \"transition_downsample\",\n",
    "                transition_downsample\n",
    "            )\n",
    "            shrinking_from_rgb_conv = shrinking_from_rgb_conv_layer(\n",
    "                inputs=transition_downsample\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_discriminator_network\",\n",
    "                \"shrinking_from_rgb_conv\",\n",
    "                shrinking_from_rgb_conv\n",
    "            )\n",
    "\n",
    "            # Weighted sum.\n",
    "            weighted_sum = tf.add(\n",
    "                x=growing_block_conv * alpha_var,\n",
    "                y=shrinking_from_rgb_conv * (1.0 - alpha_var),\n",
    "                name=\"{}_growth_transition_weighted_sum_{}\".format(\n",
    "                    self.name, trans_idx\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_discriminator_network\",\n",
    "                \"weighted_sum\",\n",
    "                weighted_sum\n",
    "            )\n",
    "\n",
    "            # Permanent blocks.\n",
    "            permanent_blocks = self.conv_layer_blocks[0:trans_idx + 1]\n",
    "\n",
    "            # Reverse order of blocks and flatten.\n",
    "            permanent_block_layers = [\n",
    "                item for sublist in permanent_blocks[::-1] for item in sublist\n",
    "            ]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = weighted_sum\n",
    "\n",
    "            # Find number of permanent growth conv layers.\n",
    "            num_perm_growth_conv_layers = len(permanent_block_layers)\n",
    "            num_perm_growth_conv_layers -= len(params[\"conv_num_filters\"][0])\n",
    "\n",
    "            # Loop through only the permanent growth conv layers.\n",
    "            for i in range(num_perm_growth_conv_layers):\n",
    "                block_conv = permanent_block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_discriminator_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            if params[\"use_minibatch_stddev\"]:\n",
    "                block_conv = self.minibatch_stddev(\n",
    "                    X=block_conv,\n",
    "                    params=params,\n",
    "                    group_size=params[\"minibatch_stddev_group_size\"]\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_discriminator_network\",\n",
    "                    \"minibatch_stddev_block_conv\",\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Loop through only the permanent base conv layers now.\n",
    "            for i in range(\n",
    "                    num_perm_growth_conv_layers, len(permanent_block_layers)):\n",
    "                block_conv = permanent_block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_discriminator_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Get logits now.\n",
    "            logits = self.use_discriminator_logits_layer(\n",
    "                block_conv=block_conv, params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_discriminator_network\",\n",
    "                \"logits\",\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_final_discriminator_network(self, X, params):\n",
    "        \"\"\"Creates final discriminator network.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, input image to discriminator.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of discriminator.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\ncreate_final_discriminator_network\", \"X\", X)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Only need the last fromRGB conv layer.\n",
    "            from_rgb_conv_layer = self.from_rgb_conv_layers[-1]\n",
    "\n",
    "            # Reverse order of blocks.\n",
    "            reversed_blocks = self.conv_layer_blocks[::-1]\n",
    "\n",
    "            # Flatten list of lists block layers into list.\n",
    "            block_layers = [\n",
    "                item for sublist in reversed_blocks for item in sublist\n",
    "            ]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = from_rgb_conv_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"\\ncreate_final_discriminator_network\",\n",
    "                \"block_conv\",\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "            # Find number of permanent growth conv layers.\n",
    "            num_growth_conv_layers = len(block_layers)\n",
    "            num_growth_conv_layers -= len(params[\"conv_num_filters\"][0])\n",
    "\n",
    "            # Loop through only the permanent growth conv layers.\n",
    "            for i in range(num_growth_conv_layers):\n",
    "                block_conv = block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_final_discriminator_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            if params[\"use_minibatch_stddev\"]:\n",
    "                block_conv = self.minibatch_stddev(\n",
    "                    X=block_conv,\n",
    "                    params=params,\n",
    "                    group_size=params[\"minibatch_stddev_group_size\"]\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_final_discriminator_network\",\n",
    "                    \"minibatch_stddev_block_conv\",\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Loop through only the permanent base conv layers now.\n",
    "            for i in range(num_growth_conv_layers, len(block_layers)):\n",
    "                block_conv = block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_final_discriminator_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Get logits now.\n",
    "            logits = self.use_discriminator_logits_layer(\n",
    "                block_conv=block_conv,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\"create_final_discriminator_network\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def switch_case_discriminator_logits(\n",
    "            self, X, alpha_var, params, growth_index):\n",
    "        \"\"\"Uses switch case to use the correct network to get logits.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image tensors of shape\n",
    "                [cur_batch_size, image_size, image_size, depth].\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "            growth_index: int, current growth stage.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, 1].\n",
    "        \"\"\"\n",
    "        # Switch to case based on number of steps to get logits.\n",
    "        logits = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                # 4x4\n",
    "                lambda: self.create_base_discriminator_network(\n",
    "                    X=X, params=params\n",
    "                ),\n",
    "                # 8x8\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(0, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 16x16\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(1, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 32x32\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(2, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 64x64\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(3, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 128x128\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(4, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 256x256\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(5, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 512x512\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(6, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: self.create_growth_transition_discriminator_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(7, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: self.create_final_discriminator_network(\n",
    "                    X=X, params=params\n",
    "                )\n",
    "            ],\n",
    "            name=\"{}_switch_case_logits\".format(self.name)\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def get_discriminator_logits(self, X, alpha_var, params):\n",
    "        \"\"\"Uses generator network and returns generated output for train/eval.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image tensors of shape\n",
    "                [cur_batch_size, image_size, image_size, depth].\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, 1].\n",
    "        \"\"\"\n",
    "        print_obj(\"\\nget_discriminator_logits\", \"X\", X)\n",
    "\n",
    "        # Get discriminator's logits tensor.\n",
    "        train_steps = params[\"train_steps\"] + params[\"prev_train_steps\"]\n",
    "        num_steps_until_growth = params[\"num_steps_until_growth\"]\n",
    "        num_stages = train_steps // num_steps_until_growth\n",
    "        if (num_stages <= 0 or len(params[\"conv_num_filters\"]) == 1):\n",
    "            print(\n",
    "                \"\\nget_discriminator_logits: NOT GOING TO GROW, SKIP SWITCH CASE!\"\n",
    "            )\n",
    "            # If never going to grow, no sense using the switch case.\n",
    "            # 4x4\n",
    "            logits = self.create_base_discriminator_network(\n",
    "                X=X, params=params\n",
    "            )\n",
    "        else:\n",
    "            # Find growth index based on global step and growth frequency.\n",
    "            growth_index = tf.cast(\n",
    "                x=tf.floordiv(\n",
    "                    x=tf.train.get_or_create_global_step(),\n",
    "                    y=params[\"num_steps_until_growth\"],\n",
    "                    name=\"{}_global_step_floordiv\".format(self.name)\n",
    "                ),\n",
    "                dtype=tf.int32,\n",
    "                name=\"{}_growth_index\".format(self.name)\n",
    "            )\n",
    "\n",
    "            # Switch to case based on number of steps for logits.\n",
    "            logits = self.switch_case_discriminator_logits(\n",
    "                X=X,\n",
    "                alpha_var=alpha_var,\n",
    "                params=params,\n",
    "                growth_index=growth_index\n",
    "            )\n",
    "\n",
    "        print_obj(\n",
    "            \"\\nget_discriminator_logits\", \"logits\", logits\n",
    "        )\n",
    "\n",
    "        # Wrap logits in a control dependency for the build discriminator\n",
    "        # tensors to ensure discriminator internals are built.\n",
    "        with tf.control_dependencies(\n",
    "                control_inputs=[self.build_discriminator_tensors]):\n",
    "            logits = tf.identity(\n",
    "                input=logits, name=\"{}_logits_identity\".format(self.name)\n",
    "            )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def get_gradient_penalty_loss(\n",
    "            self,\n",
    "            cur_batch_size,\n",
    "            fake_images,\n",
    "            real_images,\n",
    "            alpha_var,\n",
    "            params):\n",
    "        \"\"\"Gets discriminator gradient penalty loss.\n",
    "\n",
    "        Args:\n",
    "            cur_batch_size: tensor, in case of a partial batch instead of\n",
    "                using the user passed int.\n",
    "            fake_images: tensor, images generated by the generator from random\n",
    "                noise of shape [cur_batch_size, image_size, image_size, 3].\n",
    "            real_images: tensor, real images from input of shape\n",
    "                [cur_batch_size, image_size, image_size, 3].\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Discriminator's gradient penalty loss of shape [].\n",
    "        \"\"\"\n",
    "        func_name = \"get_gradient_penalty_loss\"\n",
    "\n",
    "        with tf.name_scope(name=\"{}/gradient_penalty\".format(self.name)):\n",
    "            # Get a random uniform number rank 4 tensor.\n",
    "            random_uniform_num = tf.random.uniform(\n",
    "                shape=[cur_batch_size, 1, 1, 1],\n",
    "                minval=0., maxval=1.,\n",
    "                dtype=tf.float32,\n",
    "                name=\"random_uniform_num\"\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\n\" + func_name, \"random_uniform_num\", random_uniform_num\n",
    "            )\n",
    "\n",
    "            # Find the element-wise difference between images.\n",
    "            image_difference = fake_images - real_images\n",
    "            print_obj(func_name, \"image_difference\", image_difference)\n",
    "\n",
    "            # Get random samples from this mixed image distribution.\n",
    "            mixed_images = random_uniform_num * image_difference\n",
    "            mixed_images += real_images\n",
    "            print_obj(func_name, \"mixed_images\", mixed_images)\n",
    "\n",
    "            # Send to the discriminator to get logits.\n",
    "            mixed_logits = self.get_discriminator_logits(\n",
    "                X=mixed_images, alpha_var=alpha_var, params=params\n",
    "            )\n",
    "            print_obj(func_name, \"mixed_logits\", mixed_logits)\n",
    "\n",
    "            # Get the mixed loss.\n",
    "            mixed_loss = tf.reduce_sum(\n",
    "                input_tensor=mixed_logits,\n",
    "                name=\"mixed_loss\"\n",
    "            )\n",
    "            print_obj(func_name, \"mixed_loss\", mixed_loss)\n",
    "\n",
    "            # Get gradient from returned list of length 1.\n",
    "            mixed_gradients = tf.gradients(\n",
    "                ys=mixed_loss,\n",
    "                xs=[mixed_images],\n",
    "                name=\"gradients\"\n",
    "            )[0]\n",
    "            print_obj(func_name, \"mixed_gradients\", mixed_gradients)\n",
    "\n",
    "            # Get gradient's L2 norm.\n",
    "            mixed_norms = tf.sqrt(\n",
    "                x=tf.reduce_sum(\n",
    "                    input_tensor=tf.square(\n",
    "                        x=mixed_gradients,\n",
    "                        name=\"squared_grads\"\n",
    "                    ),\n",
    "                    axis=[1, 2, 3]\n",
    "                ) + 1e-8\n",
    "            )\n",
    "            print_obj(func_name, \"mixed_norms\", mixed_norms)\n",
    "\n",
    "            # Get squared difference from target of 1.0.\n",
    "            squared_difference = tf.square(\n",
    "                x=mixed_norms - 1.0,\n",
    "                name=\"squared_difference\"\n",
    "            )\n",
    "            print_obj(func_name, \"squared_difference\", squared_difference)\n",
    "\n",
    "            # Get gradient penalty scalar.\n",
    "            gradient_penalty = tf.reduce_mean(\n",
    "                input_tensor=squared_difference, name=\"gradient_penalty\"\n",
    "            )\n",
    "            print_obj(func_name, \"gradient_penalty\", gradient_penalty)\n",
    "\n",
    "            # Multiply with lambda to get gradient penalty loss.\n",
    "            gradient_penalty_loss = tf.multiply(\n",
    "                x=params[\"discriminator_gradient_penalty_coefficient\"],\n",
    "                y=gradient_penalty,\n",
    "                name=\"gradient_penalty_loss\"\n",
    "            )\n",
    "\n",
    "        return gradient_penalty_loss\n",
    "\n",
    "    def get_discriminator_loss(\n",
    "            self,\n",
    "            cur_batch_size,\n",
    "            fake_images,\n",
    "            real_images,\n",
    "            fake_logits,\n",
    "            real_logits,\n",
    "            alpha_var,\n",
    "            params):\n",
    "        \"\"\"Gets discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            cur_batch_size: tensor, in case of a partial batch instead of\n",
    "                using the user passed int.\n",
    "            fake_images: tensor, images generated by the generator from random\n",
    "                noise of shape [cur_batch_size, image_size, image_size, 3].\n",
    "            real_images: tensor, real images from input of shape\n",
    "                [cur_batch_size, image_size, image_size, 3].\n",
    "            fake_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "                discriminator having processed generator's output image.\n",
    "            real_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "                discriminator having processed real image.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Discriminator's total loss tensor of shape [].\n",
    "        \"\"\"\n",
    "        # Calculate base discriminator loss.\n",
    "        discriminator_real_loss = tf.reduce_mean(\n",
    "            input_tensor=real_logits,\n",
    "            name=\"{}_real_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\nget_discriminator_loss\",\n",
    "            \"discriminator_real_loss\",\n",
    "            discriminator_real_loss\n",
    "        )\n",
    "\n",
    "        discriminator_generated_loss = tf.reduce_mean(\n",
    "            input_tensor=fake_logits,\n",
    "            name=\"{}_generated_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"discriminator_generated_loss\",\n",
    "            discriminator_generated_loss\n",
    "        )\n",
    "\n",
    "        discriminator_loss = tf.add(\n",
    "            x=discriminator_real_loss, y=-discriminator_generated_loss,\n",
    "            name=\"{}_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"discriminator_loss\",\n",
    "            discriminator_loss\n",
    "        )\n",
    "\n",
    "        # Get discriminator gradient penalty loss.\n",
    "        discriminator_gradient_penalty = self.get_gradient_penalty_loss(\n",
    "            cur_batch_size=cur_batch_size,\n",
    "            fake_images=fake_images,\n",
    "            real_images=real_images,\n",
    "            alpha_var=alpha_var,\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"discriminator_gradient_penalty\",\n",
    "            discriminator_gradient_penalty\n",
    "        )\n",
    "\n",
    "        # Get discriminator epsilon drift penalty.\n",
    "        epsilon_drift_penalty = tf.multiply(\n",
    "            x=params[\"epsilon_drift\"],\n",
    "            y=tf.reduce_mean(input_tensor=tf.square(x=real_logits)),\n",
    "            name=\"epsilon_drift_penalty\"\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"epsilon_drift_penalty\",\n",
    "            epsilon_drift_penalty\n",
    "        )\n",
    "\n",
    "        # Get discriminator Wasserstein GP loss.\n",
    "        discriminator_wasserstein_gp_loss = tf.add_n(\n",
    "            inputs=[\n",
    "                discriminator_loss,\n",
    "                discriminator_gradient_penalty,\n",
    "                epsilon_drift_penalty\n",
    "            ],\n",
    "            name=\"{}_wasserstein_gp_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"discriminator_wasserstein_gp_loss\",\n",
    "            discriminator_wasserstein_gp_loss\n",
    "        )\n",
    "\n",
    "        # Get discriminator regularization losses.\n",
    "        discriminator_reg_loss = regularization.get_regularization_loss(\n",
    "            lambda1=params[\"discriminator_l1_regularization_scale\"],\n",
    "            lambda2=params[\"discriminator_l2_regularization_scale\"],\n",
    "            scope=self.name\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"discriminator_reg_loss\",\n",
    "            discriminator_reg_loss\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        discriminator_total_loss = tf.add(\n",
    "            x=discriminator_wasserstein_gp_loss,\n",
    "            y=discriminator_reg_loss,\n",
    "            name=\"{}_total_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_discriminator_loss\",\n",
    "            \"discriminator_total_loss\",\n",
    "            discriminator_total_loss\n",
    "        )\n",
    "\n",
    "        return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/encoder.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import regularization\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Encoder(object):\n",
    "    \"\"\"Encoder that takes image input and outputs logits.\n",
    "\n",
    "    Fields:\n",
    "        name: str, name of `Encoder`.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "        from_rgb_conv_layers: list, fromRGB 1x1 `Conv2D` layers.\n",
    "        conv_layer_blocks: list, lists of `Conv2D` block layers for each\n",
    "            block.\n",
    "        transition_downsample_layers: list, `AveragePooling2D` layers for\n",
    "            downsampling shrinking transition paths.\n",
    "        flatten_layer: `Flatten` layer prior to logits layer.\n",
    "        logits_layer: `Dense` layer for logits.\n",
    "        build_encoder_tensors: list, tensors used to build layer\n",
    "            internals.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, params, name):\n",
    "        \"\"\"Instantiates and builds encoder network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            params: dict, user passed parameters.\n",
    "            name: str, name of encoder.\n",
    "        \"\"\"\n",
    "        # Set name of encoder.\n",
    "        self.name = name\n",
    "\n",
    "        # Regularizer for kernel weights.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        # Regularizer for bias weights.\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        # Instantiate encoder layers.\n",
    "        (self.from_rgb_conv_layers,\n",
    "         self.conv_layer_blocks,\n",
    "         self.transition_downsample_layers,\n",
    "         self.flatten_layer,\n",
    "         self.logits_layer) = self.instantiate_encoder_layers(\n",
    "            params\n",
    "        )\n",
    "\n",
    "        # Build encoder layer internals.\n",
    "        self.build_encoder_tensors = self.build_encoder_layers(\n",
    "            params\n",
    "        )\n",
    "\n",
    "    def instantiate_encoder_from_rgb_layers(self, params):\n",
    "        \"\"\"Instantiates encoder fromRGB layers of 1x1 convs.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of fromRGB 1x1 Conv2D layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get fromRGB layer properties.\n",
    "            from_rgb = [\n",
    "                params[\"encoder_from_rgb_layers\"][i][0][:]\n",
    "                for i in range(len(params[\"encoder_from_rgb_layers\"]))\n",
    "            ]\n",
    "\n",
    "            # Create list to hold toRGB 1x1 convs.\n",
    "            from_rgb_conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=from_rgb[i][3],\n",
    "                    kernel_size=from_rgb[i][0:2],\n",
    "                    strides=from_rgb[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_from_rgb_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        i,\n",
    "                        from_rgb[i][0],\n",
    "                        from_rgb[i][1],\n",
    "                        from_rgb[i][2],\n",
    "                        from_rgb[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(from_rgb))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_encoder_from_rgb_layers\",\n",
    "                \"from_rgb_conv_layers\",\n",
    "                from_rgb_conv_layers\n",
    "            )\n",
    "\n",
    "        return from_rgb_conv_layers\n",
    "\n",
    "    def instantiate_encoder_base_conv_layer_block(self, params):\n",
    "        \"\"\"Instantiates encoder base conv layer block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of base conv layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"encoder_base_conv_blocks\"][0]\n",
    "\n",
    "            # Create list of base conv layers.\n",
    "            base_conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[i][3],\n",
    "                    kernel_size=conv_block[i][0:2],\n",
    "                    strides=conv_block[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        i,\n",
    "                        conv_block[i][0],\n",
    "                        conv_block[i][1],\n",
    "                        conv_block[i][2],\n",
    "                        conv_block[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block) - 1)\n",
    "            ]\n",
    "\n",
    "            # Have valid padding for layer just before flatten and logits.\n",
    "            base_conv_layers.append(\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[-1][3],\n",
    "                    kernel_size=conv_block[-1][0:2],\n",
    "                    strides=conv_block[-1][4:6],\n",
    "                    padding=\"valid\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_base_layers_conv2d_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        len(conv_block) - 1,\n",
    "                        conv_block[-1][0],\n",
    "                        conv_block[-1][1],\n",
    "                        conv_block[-1][2],\n",
    "                        conv_block[-1][3]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_encoder_base_conv_layer_block\",\n",
    "                \"base_conv_layers\",\n",
    "                base_conv_layers\n",
    "            )\n",
    "\n",
    "        return base_conv_layers\n",
    "\n",
    "    def instantiate_encoder_growth_layer_block(self, params, block_idx):\n",
    "        \"\"\"Instantiates encoder growth block layers.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of growth block layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"encoder_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "            # Create new inner convolutional layers.\n",
    "            conv_layers = [\n",
    "                tf.layers.Conv2D(\n",
    "                    filters=conv_block[i][3],\n",
    "                    kernel_size=conv_block[i][0:2],\n",
    "                    strides=conv_block[i][4:6],\n",
    "                    padding=\"same\",\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=self.kernel_regularizer,\n",
    "                    bias_regularizer=self.bias_regularizer,\n",
    "                    name=\"{}_growth_layers_conv2d_{}_{}_{}x{}_{}_{}\".format(\n",
    "                        self.name,\n",
    "                        block_idx,\n",
    "                        i,\n",
    "                        conv_block[i][0],\n",
    "                        conv_block[i][1],\n",
    "                        conv_block[i][2],\n",
    "                        conv_block[i][3]\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_encoder_growth_layer_block\",\n",
    "                \"conv_layers\",\n",
    "                conv_layers\n",
    "            )\n",
    "\n",
    "            # Down sample from 2s X 2s to s X s image.\n",
    "            downsampled_image_layer = tf.layers.AveragePooling2D(\n",
    "                pool_size=(2, 2),\n",
    "                strides=(2, 2),\n",
    "                name=\"{}_growth_downsampled_image_{}\".format(\n",
    "                    self.name,\n",
    "                    block_idx\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"instantiate_encoder_growth_layer_block\",\n",
    "                \"downsampled_image_layer\",\n",
    "                downsampled_image_layer\n",
    "            )\n",
    "\n",
    "        return conv_layers + [downsampled_image_layer]\n",
    "\n",
    "    def instantiate_encoder_growth_transition_downsample_layers(\n",
    "            self, params):\n",
    "        \"\"\"Instantiates encoder growth transition downsample layers.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of growth transition downsample layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Down sample from 2s X 2s to s X s image.\n",
    "            downsample_layers = [\n",
    "                tf.layers.AveragePooling2D(\n",
    "                    pool_size=(2, 2),\n",
    "                    strides=(2, 2),\n",
    "                    name=\"{}_growth_transition_downsample_layer_{}\".format(\n",
    "                        self.name,\n",
    "                        layer_idx\n",
    "                    )\n",
    "                )\n",
    "                for layer_idx in range(\n",
    "                    1 + len(params[\"encoder_growth_conv_blocks\"])\n",
    "                )\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\ninstantiate_encoder_growth_transition_downsample_layers\",\n",
    "                \"downsample_layers\",\n",
    "                downsample_layers\n",
    "            )\n",
    "\n",
    "        return downsample_layers\n",
    "\n",
    "    def instantiate_encoder_logits_layer(self, params):\n",
    "        \"\"\"Instantiates encoder flatten and logits layers.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        Returns:\n",
    "            Flatten and logits layers of encoder.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Flatten layer to ready final block conv tensor for dense layer.\n",
    "            flatten_layer = tf.layers.Flatten(\n",
    "                name=\"{}_flatten_layer\".format(self.name)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\ncreate_encoder_logits_layer\",\n",
    "                \"flatten_layer\",\n",
    "                flatten_layer\n",
    "            )\n",
    "\n",
    "            # Final linear layer for logits with same shape as latent vector.\n",
    "            logits_layer = tf.layers.Dense(\n",
    "                units=params[\"latent_size\"],\n",
    "                activation=None,\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"{}_layers_dense_logits\".format(self.name)\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_encoder_network\",\n",
    "                \"logits_layer\",\n",
    "                logits_layer\n",
    "            )\n",
    "\n",
    "        return flatten_layer, logits_layer\n",
    "\n",
    "    def instantiate_encoder_layers(self, params):\n",
    "        \"\"\"Instantiates layers of encoder network.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            from_rgb_conv_layers: list, fromRGB 1x1 `Conv2D` layers.\n",
    "            conv_layer_blocks: list, lists of `Conv2D` block layers for each\n",
    "                block.\n",
    "            transition_downsample_layers: list, `AveragePooling2D` layers for\n",
    "                downsampling shrinking transition paths.\n",
    "            flatten_layer: `Flatten` layer prior to logits layer.\n",
    "            logits_layer: `Dense` layer for logits.\n",
    "        \"\"\"\n",
    "        # Instantiate fromRGB 1x1 `Conv2D` layers.\n",
    "        from_rgb_conv_layers = self.instantiate_encoder_from_rgb_layers(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_encoder_layers\",\n",
    "            \"from_rgb_conv_layers\",\n",
    "            from_rgb_conv_layers\n",
    "        )\n",
    "\n",
    "        # Instantiate base conv block's `Conv2D` layers, for post-growth.\n",
    "        conv_layer_blocks = [\n",
    "            self.instantiate_encoder_base_conv_layer_block(\n",
    "                params=params\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Instantiate growth `Conv2D` layer blocks.\n",
    "        conv_layer_blocks.extend(\n",
    "            [\n",
    "                self.instantiate_encoder_growth_layer_block(\n",
    "                    params=params,\n",
    "                    block_idx=block_idx\n",
    "                )\n",
    "                for block_idx in range(\n",
    "                    len(params[\"encoder_growth_conv_blocks\"])\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_encoder_layers\",\n",
    "            \"conv_layer_blocks\",\n",
    "            conv_layer_blocks\n",
    "        )\n",
    "\n",
    "        # Instantiate transition downsample `AveragePooling2D` layers.\n",
    "        transition_downsample_layers = (\n",
    "            self.instantiate_encoder_growth_transition_downsample_layers(\n",
    "                params=params\n",
    "            )\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_encoder_layers\",\n",
    "            \"transition_downsample_layers\",\n",
    "            transition_downsample_layers\n",
    "        )\n",
    "\n",
    "        # Instantiate `Flatten` and `Dense` logits layers.\n",
    "        (flatten_layer,\n",
    "         logits_layer) = self.instantiate_encoder_logits_layer(params=params)\n",
    "        print_obj(\n",
    "            \"instantiate_encoder_layers\",\n",
    "            \"flatten_layer\",\n",
    "            flatten_layer\n",
    "        )\n",
    "        print_obj(\n",
    "            \"instantiate_encoder_layers\",\n",
    "            \"logits_layer\",\n",
    "            logits_layer\n",
    "        )\n",
    "\n",
    "        return (from_rgb_conv_layers,\n",
    "                conv_layer_blocks,\n",
    "                transition_downsample_layers,\n",
    "                flatten_layer,\n",
    "                logits_layer)\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def build_encoder_from_rgb_layers(self, params):\n",
    "        \"\"\"Creates encoder fromRGB layers of 1x1 convs.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of tensors from fromRGB 1x1 `Conv2D` layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get fromRGB layer properties.\n",
    "            from_rgb = [\n",
    "                params[\"encoder_from_rgb_layers\"][i][0][:]\n",
    "                for i in range(len(params[\"encoder_from_rgb_layers\"]))\n",
    "            ]\n",
    "\n",
    "            # Create list to hold fromRGB 1x1 convs.\n",
    "            from_rgb_conv_tensors = [\n",
    "                self.from_rgb_conv_layers[i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + from_rgb[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(from_rgb))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_encoder_from_rgb_layers\",\n",
    "                \"from_rgb_conv_tensors\",\n",
    "                from_rgb_conv_tensors\n",
    "            )\n",
    "\n",
    "        return from_rgb_conv_tensors\n",
    "\n",
    "    def build_encoder_base_conv_layer_block(self, params):\n",
    "        \"\"\"Creates encoder base conv layer block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of tensors from base `Conv2D` layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"encoder_base_conv_blocks\"][0]\n",
    "\n",
    "            # The base conv block is always the 0th one.\n",
    "            base_conv_layer_block = self.conv_layer_blocks[0]\n",
    "\n",
    "            # Build base conv block layers, store in list.\n",
    "            base_conv_tensors = [\n",
    "                base_conv_layer_block[i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + conv_block[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_encoder_base_conv_layer_block\",\n",
    "                \"base_conv_tensors\",\n",
    "                base_conv_tensors\n",
    "            )\n",
    "\n",
    "        return base_conv_tensors\n",
    "\n",
    "    def build_encoder_growth_layer_block(self, params, block_idx):\n",
    "        \"\"\"Creates encoder growth block.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, the current growth block's index.\n",
    "\n",
    "        Returns:\n",
    "            List of tensors from growth block `Conv2D` layers.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Get conv block layer properties.\n",
    "            conv_block = params[\"encoder_growth_conv_blocks\"][block_idx]\n",
    "\n",
    "            # Create new inner convolutional layers.\n",
    "            conv_tensors = [\n",
    "                self.conv_layer_blocks[1 + block_idx][i](\n",
    "                    inputs=tf.zeros(\n",
    "                        shape=[1] + conv_block[i][0:3], dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                for i in range(len(conv_block))\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"\\nbuild_encoder_growth_layer_block\",\n",
    "                \"conv_tensors\",\n",
    "                conv_tensors\n",
    "            )\n",
    "\n",
    "        return conv_tensors\n",
    "\n",
    "    def build_encoder_logits_layer(self, params):\n",
    "        \"\"\"Builds flatten and logits layer internals using call.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of encoder.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            block_conv_size = params[\"encoder_base_conv_blocks\"][-1][-1][3]\n",
    "\n",
    "            # Flatten final block conv tensor.\n",
    "            block_conv_flat = self.flatten_layer(\n",
    "                inputs=tf.zeros(\n",
    "                    shape=[1, 1, 1, block_conv_size],\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\nbuild_encoder_logits_layer\",\n",
    "                \"block_conv_flat\",\n",
    "                block_conv_flat\n",
    "            )\n",
    "\n",
    "            # Final linear layer for logits.\n",
    "            logits = self.logits_layer(inputs=block_conv_flat)\n",
    "            print_obj(\"build_encoder_logits_layer\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def build_encoder_layers(self, params):\n",
    "        \"\"\"Builds encoder layer internals.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor.\n",
    "        \"\"\"\n",
    "        # Build fromRGB 1x1 `Conv2D` layers internals through call.\n",
    "        from_rgb_conv_tensors = self.build_encoder_from_rgb_layers(\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\n",
    "            \"\\nbuild_encoder_layers\",\n",
    "            \"from_rgb_conv_tensors\",\n",
    "            from_rgb_conv_tensors\n",
    "        )\n",
    "\n",
    "        with tf.control_dependencies(control_inputs=from_rgb_conv_tensors):\n",
    "            # Create base convolutional block's layer internals using call.\n",
    "            conv_block_tensors = [\n",
    "                self.build_encoder_base_conv_layer_block(\n",
    "                    params=params\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Build growth `Conv2D` layer block internals through call.\n",
    "            conv_block_tensors.extend(\n",
    "                [\n",
    "                    self.build_encoder_growth_layer_block(\n",
    "                        params=params, block_idx=block_idx\n",
    "                    )\n",
    "                    for block_idx in range(\n",
    "                       len(params[\"encoder_growth_conv_blocks\"])\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Flatten conv block tensor lists of lists into list.\n",
    "            conv_block_tensors = [\n",
    "                item for sublist in conv_block_tensors for item in sublist\n",
    "            ]\n",
    "            print_obj(\n",
    "                \"build_encoder_layers\",\n",
    "                \"conv_block_tensors\",\n",
    "                conv_block_tensors\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies(control_inputs=conv_block_tensors):\n",
    "                # Build logits layer internals using call.\n",
    "                logits_tensor = self.build_encoder_logits_layer(\n",
    "                    params=params\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"build_encoder_layers\",\n",
    "                    \"logits_tensor\",\n",
    "                    logits_tensor\n",
    "                )\n",
    "\n",
    "        return logits_tensor\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def use_encoder_logits_layer(self, block_conv, params):\n",
    "        \"\"\"Uses flatten and logits layers to get logits tensor.\n",
    "\n",
    "        Args:\n",
    "            block_conv: tensor, output of last conv layer of encoder.\n",
    "            flatten_layer: `Flatten` layer.\n",
    "            logits_layer: `Dense` layer for logits.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of encoder.\n",
    "        \"\"\"\n",
    "        print_obj(\n",
    "            \"\\nuse_encoder_logits_layer\", \"block_conv\", block_conv\n",
    "        )\n",
    "        # Set shape to remove ambiguity for dense layer.\n",
    "        block_conv.set_shape(\n",
    "            [\n",
    "                block_conv.get_shape()[0],\n",
    "                params[\"generator_projection_dims\"][0] / 4,\n",
    "                params[\"generator_projection_dims\"][1] / 4,\n",
    "                block_conv.get_shape()[-1]]\n",
    "        )\n",
    "        print_obj(\"use_encoder_logits_layer\", \"block_conv\", block_conv)\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Flatten final block conv tensor.\n",
    "            block_conv_flat = self.flatten_layer(inputs=block_conv)\n",
    "            print_obj(\n",
    "                \"use_encoder_logits_layer\",\n",
    "                \"block_conv_flat\",\n",
    "                block_conv_flat\n",
    "            )\n",
    "\n",
    "            # Final linear layer for logits.\n",
    "            logits = self.logits_layer(inputs=block_conv_flat)\n",
    "            print_obj(\"use_encoder_logits_layer\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_base_encoder_network(self, X, params):\n",
    "        \"\"\"Creates base encoder network.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, input image to encoder.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of encoder.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\ncreate_base_encoder_network\", \"X\", X)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Only need the first fromRGB conv layer & block for base network.\n",
    "            from_rgb_conv_layer = self.from_rgb_conv_layers[0]\n",
    "            block_layers = self.conv_layer_blocks[0]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            from_rgb_conv = from_rgb_conv_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"create_base_encoder_network\",\n",
    "                \"from_rgb_conv\",\n",
    "                from_rgb_conv\n",
    "            )\n",
    "\n",
    "            block_conv = from_rgb_conv\n",
    "            for i in range(len(block_layers)):\n",
    "                block_conv = block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_base_encoder_network\",\n",
    "                    \"block_conv\",\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Get logits now.\n",
    "            logits = self.use_encoder_logits_layer(\n",
    "                block_conv=block_conv,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\"create_base_encoder_network\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_growth_transition_encoder_network(\n",
    "            self, X, alpha_var, params, trans_idx):\n",
    "        \"\"\"Creates growth transition encoder network.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, input image to encoder.\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "            trans_idx: int, index of current growth transition.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of encoder.\n",
    "        \"\"\"\n",
    "        print_obj(\n",
    "            \"\\nEntered create_growth_transition_encoder_network\",\n",
    "            \"trans_idx\",\n",
    "            trans_idx\n",
    "        )\n",
    "        print_obj(\"create_growth_transition_encoder_network\", \"X\", X)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Growing side chain.\n",
    "            growing_from_rgb_conv_layer = self.from_rgb_conv_layers[trans_idx + 1]\n",
    "            growing_block_layers = self.conv_layer_blocks[trans_idx + 1]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            growing_block_conv = growing_from_rgb_conv_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"\\ncreate_growth_transition_encoder_network\",\n",
    "                \"growing_block_conv\",\n",
    "                growing_block_conv\n",
    "            )\n",
    "            for i in range(len(growing_block_layers)):\n",
    "                growing_block_conv = growing_block_layers[i](\n",
    "                    inputs=growing_block_conv\n",
    "                )\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_encoder_network\",\n",
    "                    \"growing_block_conv\",\n",
    "                    growing_block_conv\n",
    "                )\n",
    "\n",
    "            # Shrinking side chain.\n",
    "            transition_downsample_layer = self.transition_downsample_layers[trans_idx]\n",
    "            shrinking_from_rgb_conv_layer = self.from_rgb_conv_layers[trans_idx]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            transition_downsample = transition_downsample_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"create_growth_transition_encoder_network\",\n",
    "                \"transition_downsample\",\n",
    "                transition_downsample\n",
    "            )\n",
    "            shrinking_from_rgb_conv = shrinking_from_rgb_conv_layer(\n",
    "                inputs=transition_downsample\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_encoder_network\",\n",
    "                \"shrinking_from_rgb_conv\",\n",
    "                shrinking_from_rgb_conv\n",
    "            )\n",
    "\n",
    "            # Weighted sum.\n",
    "            weighted_sum = tf.add(\n",
    "                x=growing_block_conv * alpha_var,\n",
    "                y=shrinking_from_rgb_conv * (1.0 - alpha_var),\n",
    "                name=\"{}_growth_transition_weighted_sum_{}\".format(\n",
    "                    self.name, trans_idx\n",
    "                )\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_encoder_network\",\n",
    "                \"weighted_sum\",\n",
    "                weighted_sum\n",
    "            )\n",
    "\n",
    "            # Permanent blocks.\n",
    "            permanent_blocks = self.conv_layer_blocks[0:trans_idx + 1]\n",
    "\n",
    "            # Reverse order of blocks and flatten.\n",
    "            permanent_block_layers = [\n",
    "                item for sublist in permanent_blocks[::-1] for item in sublist\n",
    "            ]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = weighted_sum\n",
    "\n",
    "            # Find number of permanent growth conv layers.\n",
    "            num_perm_growth_conv_layers = len(permanent_block_layers)\n",
    "            num_perm_growth_conv_layers -= len(params[\"conv_num_filters\"][0])\n",
    "\n",
    "            # Loop through only the permanent growth conv layers.\n",
    "            for i in range(num_perm_growth_conv_layers):\n",
    "                block_conv = permanent_block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_encoder_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Loop through only the permanent base conv layers now.\n",
    "            for i in range(\n",
    "                    num_perm_growth_conv_layers, len(permanent_block_layers)):\n",
    "                block_conv = permanent_block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_growth_transition_encoder_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Get logits now.\n",
    "            logits = self.use_encoder_logits_layer(\n",
    "                block_conv=block_conv, params=params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"create_growth_transition_encoder_network\",\n",
    "                \"logits\",\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_final_encoder_network(self, X, params):\n",
    "        \"\"\"Creates final encoder network.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, input image to encoder.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Final logits tensor of encoder.\n",
    "        \"\"\"\n",
    "        print_obj(\"\\ncreate_final_encoder_network\", \"X\", X)\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Only need the last fromRGB conv layer.\n",
    "            from_rgb_conv_layer = self.from_rgb_conv_layers[-1]\n",
    "\n",
    "            # Reverse order of blocks.\n",
    "            reversed_blocks = self.conv_layer_blocks[::-1]\n",
    "\n",
    "            # Flatten list of lists block layers into list.\n",
    "            block_layers = [\n",
    "                item for sublist in reversed_blocks for item in sublist\n",
    "            ]\n",
    "\n",
    "            # Pass inputs through layer chain.\n",
    "            block_conv = from_rgb_conv_layer(inputs=X)\n",
    "            print_obj(\n",
    "                \"\\ncreate_final_encoder_network\",\n",
    "                \"block_conv\",\n",
    "                block_conv\n",
    "            )\n",
    "\n",
    "            # Find number of permanent growth conv layers.\n",
    "            num_growth_conv_layers = len(block_layers)\n",
    "            num_growth_conv_layers -= len(params[\"conv_num_filters\"][0])\n",
    "\n",
    "            # Loop through only the permanent growth conv layers.\n",
    "            for i in range(num_growth_conv_layers):\n",
    "                block_conv = block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_final_encoder_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Loop through only the permanent base conv layers now.\n",
    "            for i in range(num_growth_conv_layers, len(block_layers)):\n",
    "                block_conv = block_layers[i](inputs=block_conv)\n",
    "                print_obj(\n",
    "                    \"create_final_encoder_network\",\n",
    "                    \"block_conv_{}\".format(i),\n",
    "                    block_conv\n",
    "                )\n",
    "\n",
    "            # Get logits now.\n",
    "            logits = self.use_encoder_logits_layer(\n",
    "                block_conv=block_conv,\n",
    "                params=params\n",
    "            )\n",
    "            print_obj(\"create_final_encoder_network\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def switch_case_encoder_logits(\n",
    "            self, X, alpha_var, params, growth_index):\n",
    "        \"\"\"Uses switch case to use the correct network to get logits.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image tensors of shape\n",
    "                [cur_batch_size, image_size, image_size, depth].\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "            growth_index: int, current growth stage.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, 1].\n",
    "        \"\"\"\n",
    "        # Switch to case based on number of steps to get logits.\n",
    "        logits = tf.switch_case(\n",
    "            branch_index=growth_index,\n",
    "            branch_fns=[\n",
    "                # 4x4\n",
    "                lambda: self.create_base_encoder_network(\n",
    "                    X=X, params=params\n",
    "                ),\n",
    "                # 8x8\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(0, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 16x16\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(1, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 32x32\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(2, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 64x64\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(3, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 128x128\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(4, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 256x256\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(5, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 512x512\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(6, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: self.create_growth_transition_encoder_network(\n",
    "                    X=X,\n",
    "                    alpha_var=alpha_var,\n",
    "                    params=params,\n",
    "                    trans_idx=min(7, len(params[\"conv_num_filters\"]) - 2)\n",
    "                ),\n",
    "                # 1024x1024\n",
    "                lambda: self.create_final_encoder_network(\n",
    "                    X=X, params=params\n",
    "                )\n",
    "            ],\n",
    "            name=\"{}_switch_case_logits\".format(self.name)\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def get_train_eval_encoder_logits(self, X, alpha_var, params):\n",
    "        \"\"\"Uses encoder network and returns encoded logits for train/eval.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image tensors of shape\n",
    "                [cur_batch_size, image_size, image_size, depth].\n",
    "            alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, latent_size].\n",
    "        \"\"\"\n",
    "        print_obj(\"\\nget_train_eval_encoder_logits\", \"X\", X)\n",
    "\n",
    "        # Get encoder's logits tensor.\n",
    "        train_steps = params[\"train_steps\"] + params[\"prev_train_steps\"]\n",
    "        num_steps_until_growth = params[\"num_steps_until_growth\"]\n",
    "        num_stages = train_steps // num_steps_until_growth\n",
    "        if (num_stages <= 0 or len(params[\"conv_num_filters\"]) == 1):\n",
    "            print(\n",
    "                \"\\nget_train_eval_encoder_logits: NOT GOING TO GROW, SKIP SWITCH CASE!\"\n",
    "            )\n",
    "            # If never going to grow, no sense using the switch case.\n",
    "            # 4x4\n",
    "            logits = self.create_base_encoder_network(\n",
    "                X=X, params=params\n",
    "            )\n",
    "        else:\n",
    "            # Find growth index based on global step and growth frequency.\n",
    "            growth_index = tf.cast(\n",
    "                x=tf.floordiv(\n",
    "                    x=tf.train.get_or_create_global_step(),\n",
    "                    y=params[\"num_steps_until_growth\"],\n",
    "                    name=\"{}_global_step_floordiv\".format(self.name)\n",
    "                ),\n",
    "                dtype=tf.int32,\n",
    "                name=\"{}_growth_index\".format(self.name)\n",
    "            )\n",
    "\n",
    "            # Switch to case based on number of steps for logits.\n",
    "            logits = self.switch_case_encoder_logits(\n",
    "                X=X,\n",
    "                alpha_var=alpha_var,\n",
    "                params=params,\n",
    "                growth_index=growth_index\n",
    "            )\n",
    "\n",
    "        print_obj(\n",
    "            \"\\nget_train_eval_encoder_logits\", \"logits\", logits\n",
    "        )\n",
    "\n",
    "        # Wrap logits in a control dependency for the build encoder\n",
    "        # tensors to ensure encoder internals are built.\n",
    "        with tf.control_dependencies(\n",
    "                control_inputs=[self.build_encoder_tensors]):\n",
    "            logits = tf.identity(\n",
    "                input=logits, name=\"{}_logits_identity\".format(self.name)\n",
    "            )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_predict_encoder_logits(self, X, params, block_idx):\n",
    "        \"\"\"Uses encoder network and returns encoded logits for predict.\n",
    "\n",
    "        Args:\n",
    "            X: tensor, image tensors of shape\n",
    "                [cur_batch_size, image_size, image_size, depth].\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, current conv layer block's index.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, latent_size].\n",
    "        \"\"\"\n",
    "        print_obj(\"\\nget_predict_encoder_logits\", \"X\", X)\n",
    "\n",
    "        # Get encoder's logits tensor.\n",
    "        if block_idx == 0:\n",
    "            # 4x4\n",
    "            logits = self.create_base_encoder_network(X=X, params=params)\n",
    "        elif block_idx < len(params[\"conv_num_filters\"]) - 1:\n",
    "            # 8x8 through 512x512\n",
    "            logits = self.create_growth_transition_encoder_network(\n",
    "                X=X,\n",
    "                alpha_var=tf.ones(shape=[], dtype=tf.float32),\n",
    "                params=params,\n",
    "                trans_idx=block_idx - 1\n",
    "            )\n",
    "        else:\n",
    "            # 1024x1024\n",
    "            logits = self.create_final_encoder_network(X=X, params=params)\n",
    "\n",
    "        print_obj(\"\\nget_predict_encoder_logits\", \"logits\", logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "    def get_encoder_loss(\n",
    "            self,\n",
    "            fake_images,\n",
    "            encoded_images,\n",
    "            params):\n",
    "        \"\"\"Gets encoder loss.\n",
    "\n",
    "        Args:\n",
    "            fake_images: tensor, images generated by the generator from random\n",
    "                noise of shape [cur_batch_size, image_size, image_size, 3].\n",
    "            encoded_images: tensor, images generated by the generator from\n",
    "                encoder's vector output of shape\n",
    "                [cur_batch_size, image_size, image_size, 3].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Encoder's total loss tensor of shape [].\n",
    "        \"\"\"\n",
    "        # Get difference between fake images and encoder images.\n",
    "        generator_encoder_image_diff = tf.subtract(\n",
    "            x=fake_images,\n",
    "            y=encoded_images,\n",
    "            name=\"generator_encoder_image_diff\"\n",
    "        )\n",
    "\n",
    "        # Get L1 norm of image difference.\n",
    "        image_diff_l1_norm = tf.reduce_sum(\n",
    "            input_tensor=tf.abs(x=generator_encoder_image_diff),\n",
    "            axis=[1, 2, 3]\n",
    "        )\n",
    "\n",
    "        # Calculate base encoder loss.\n",
    "        encoder_loss = tf.reduce_mean(\n",
    "            input_tensor=image_diff_l1_norm,\n",
    "            name=\"{}_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_encoder_loss\",\n",
    "            \"encoder_loss\",\n",
    "            encoder_loss\n",
    "        )\n",
    "\n",
    "        # Get encoder regularization losses.\n",
    "        encoder_reg_loss = regularization.get_regularization_loss(\n",
    "            lambda1=params[\"encoder_l1_regularization_scale\"],\n",
    "            lambda2=params[\"encoder_l2_regularization_scale\"],\n",
    "            scope=self.name\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_encoder_loss\",\n",
    "            \"encoder_reg_loss\",\n",
    "            encoder_reg_loss\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        encoder_total_loss = tf.add(\n",
    "            x=encoder_loss,\n",
    "            y=encoder_reg_loss,\n",
    "            name=\"{}_total_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"get_encoder_loss\",\n",
    "            \"encoder_total_loss\",\n",
    "            encoder_total_loss\n",
    "        )\n",
    "\n",
    "        return encoder_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/regularization.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_regularization_loss(lambda1=0., lambda2=0., scope=None):\n",
    "    \"\"\"Gets regularization losses from variables attached to a regularizer.\n",
    "\n",
    "    Args:\n",
    "        lambda1: float, L1 regularization scale parameter.\n",
    "        lambda2: float, L2 regularization scale parameter.\n",
    "        scope: str, the name of the variable scope.\n",
    "\n",
    "    Returns:\n",
    "        Scalar regularization loss tensor.\n",
    "    \"\"\"\n",
    "    def sum_nd_tensor_list_to_scalar_tensor(t_list):\n",
    "        \"\"\"Sums different shape tensors into a scalar tensor.\n",
    "\n",
    "        Args:\n",
    "            t_list: list, tensors of varying shapes.\n",
    "\n",
    "        Returns:\n",
    "            Scalar tensor.\n",
    "        \"\"\"\n",
    "        # Sum list of tensors into a list of scalars.\n",
    "        t_reduce_sum_list = [\n",
    "            tf.reduce_sum(\n",
    "                # Remove the :0 from the end of the name.\n",
    "                input_tensor=t, name=\"{}_reduce_sum\".format(t.name[:-2])\n",
    "            )\n",
    "            for t in t_list\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"\\nsum_nd_tensor_list_to_scalar_tensor\",\n",
    "            \"t_reduce_sum_list\",\n",
    "            t_reduce_sum_list\n",
    "        )\n",
    "\n",
    "        # Add all scalars together into one scalar.\n",
    "        t_scalar_sum_tensor = tf.add_n(\n",
    "            inputs=t_reduce_sum_list,\n",
    "            name=\"{}_t_scalar_sum_tensor\".format(scope)\n",
    "        )\n",
    "        print_obj(\n",
    "            \"sum_nd_tensor_list_to_scalar_tensor\",\n",
    "            \"t_scalar_sum_tensor\",\n",
    "            t_scalar_sum_tensor\n",
    "        )\n",
    "\n",
    "        return t_scalar_sum_tensor\n",
    "\n",
    "    print_obj(\"\\nget_regularization_loss\", \"scope\", scope)\n",
    "    if lambda1 <= 0. and lambda2 <= 0.:\n",
    "        # No regularization so return zero.\n",
    "        return tf.zeros(shape=[], dtype=tf.float32)\n",
    "\n",
    "    # Get list of trainable variables with a regularizer attached in scope.\n",
    "    trainable_reg_vars_list = tf.get_collection(\n",
    "        tf.GraphKeys.REGULARIZATION_LOSSES, scope=scope)\n",
    "    print_obj(\n",
    "        \"get_regularization_loss\",\n",
    "        \"trainable_reg_vars_list\",\n",
    "        trainable_reg_vars_list\n",
    "    )\n",
    "\n",
    "    for var in trainable_reg_vars_list:\n",
    "        print_obj(\n",
    "            \"get_regularization_loss_{}\".format(scope),\n",
    "            \"{}\".format(var.name),\n",
    "            var.graph\n",
    "        )\n",
    "\n",
    "    l1_loss = 0.\n",
    "    if lambda1 > 0.:\n",
    "        # For L1 regularization, take the absolute value element-wise of each.\n",
    "        trainable_reg_vars_abs_list = [\n",
    "            tf.abs(\n",
    "                x=var,\n",
    "                # Clean up regularizer scopes in variable names.\n",
    "                name=\"{}_abs\".format((\"/\").join(var.name.split(\"/\")[0:3]))\n",
    "            )\n",
    "            for var in trainable_reg_vars_list\n",
    "        ]\n",
    "\n",
    "        # Get L1 loss\n",
    "        l1_loss = tf.multiply(\n",
    "            x=lambda1,\n",
    "            y=sum_nd_tensor_list_to_scalar_tensor(\n",
    "                t_list=trainable_reg_vars_abs_list\n",
    "            ),\n",
    "            name=\"{}_l1_loss\".format(scope)\n",
    "        )\n",
    "\n",
    "    l2_loss = 0.\n",
    "    if lambda2 > 0.:\n",
    "        # For L2 regularization, square all variables element-wise.\n",
    "        trainable_reg_vars_squared_list = [\n",
    "            tf.square(\n",
    "                x=var,\n",
    "                # Clean up regularizer scopes in variable names.\n",
    "                name=\"{}_squared\".format((\"/\").join(var.name.split(\"/\")[0:3]))\n",
    "            )\n",
    "            for var in trainable_reg_vars_list\n",
    "        ]\n",
    "        print_obj(\n",
    "            \"get_regularization_loss\",\n",
    "            \"trainable_reg_vars_squared_list\",\n",
    "            trainable_reg_vars_squared_list\n",
    "        )\n",
    "\n",
    "        # Get L2 loss\n",
    "        l2_loss = tf.multiply(\n",
    "            x=lambda2,\n",
    "            y=sum_nd_tensor_list_to_scalar_tensor(\n",
    "                t_list=trainable_reg_vars_squared_list\n",
    "            ),\n",
    "            name=\"{}_l2_loss\".format(scope)\n",
    "        )\n",
    "\n",
    "    l1_l2_loss = tf.add(\n",
    "        x=l1_loss, y=l2_loss, name=\"{}_l1_l2_loss\".format(scope)\n",
    "    )\n",
    "\n",
    "    return l1_l2_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/train_and_eval.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_logits_and_losses(\n",
    "        features, generator, discriminator, encoder, alpha_var, params):\n",
    "    \"\"\"Gets logits and losses for both train and eval modes.\n",
    "\n",
    "    Args:\n",
    "        features: dict, feature tensors from serving input function.\n",
    "        generator: instance of generator.`Generator`.\n",
    "        discriminator: instance of discriminator.`Discriminator`.\n",
    "        encoder: instance of encoder.`Encoder`.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Real and fake logits and generator, discriminator, and encoder losses.\n",
    "    \"\"\"\n",
    "    # Extract image from features dictionary.\n",
    "    X = features[\"image\"]\n",
    "    print_obj(\"\\nget_logits_and_losses\", \"X\", X)\n",
    "\n",
    "    if params[\"use_tpu\"]:\n",
    "        # Get dynamic batch size in case of partial batch.\n",
    "        cur_batch_size = tf.shape(\n",
    "            input=X,\n",
    "            out_type=tf.int32,\n",
    "            name=\"get_logits_and_losses_cur_batch_size\"\n",
    "        )[0]\n",
    "\n",
    "        # Create random noise latent vector for each batch example.\n",
    "        Z = tf.random.normal(\n",
    "            shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    else:\n",
    "        cur_batch_size = X.shape[0]\n",
    "        # Create random noise latent vector for each batch example.\n",
    "        Z = tf.random.normal(\n",
    "            shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    print_obj(\"get_logits_and_losses\", \"Z\", Z)\n",
    "\n",
    "    # Get generated image from generator network from gaussian noise.\n",
    "    print(\"\\nCall generator with Z = {}.\".format(Z))\n",
    "    generator_outputs = generator.get_train_eval_generator_outputs(\n",
    "        Z=Z, alpha_var=alpha_var, params=params\n",
    "    )\n",
    "\n",
    "    # Get fake logits from discriminator using generator's output image.\n",
    "    print(\n",
    "        \"\\nCall discriminator with generator_outputs = {}.\".format(\n",
    "            generator_outputs\n",
    "        )\n",
    "    )\n",
    "    fake_logits = discriminator.get_discriminator_logits(\n",
    "        X=generator_outputs, alpha_var=alpha_var, params=params\n",
    "    )\n",
    "\n",
    "    # Resize real images based on the current size of the GAN.\n",
    "    real_images = image_utils.resize_real_images(image=X, params=params)\n",
    "\n",
    "    # Get real logits from discriminator using real image.\n",
    "    print(\n",
    "        \"\\nCall discriminator with real_images = {}.\".format(real_images)\n",
    "    )\n",
    "    real_logits = discriminator.get_discriminator_logits(\n",
    "        X=real_images, alpha_var=alpha_var, params=params\n",
    "    )\n",
    "\n",
    "    # Get encoder logits using generated images from generator.\n",
    "    print(\n",
    "        \"\\nCall encoder with generator_outputs = {}.\".format(\n",
    "            generator_outputs\n",
    "        )\n",
    "    )\n",
    "    encoder_logits = encoder.get_train_eval_encoder_logits(\n",
    "        X=generator_outputs, alpha_var=alpha_var, params=params\n",
    "    )\n",
    "\n",
    "    # Get encoded images from generator using encoder's logits.\n",
    "    print(\n",
    "        \"\\nCall generator with encoder_logits = {}.\".format(\n",
    "            encoder_logits\n",
    "        )\n",
    "    )\n",
    "    encoded_images = generator.get_train_eval_generator_outputs(\n",
    "        Z=encoder_logits, alpha_var=alpha_var, params=params\n",
    "    )\n",
    "\n",
    "    # Get generator total loss.\n",
    "    generator_total_loss = generator.get_generator_loss(\n",
    "        fake_logits=fake_logits, params=params\n",
    "    )\n",
    "\n",
    "    # Get discriminator total loss.\n",
    "    discriminator_total_loss = discriminator.get_discriminator_loss(\n",
    "        cur_batch_size=cur_batch_size,\n",
    "        fake_images=generator_outputs,\n",
    "        real_images=real_images,\n",
    "        fake_logits=fake_logits,\n",
    "        real_logits=real_logits,\n",
    "        alpha_var=alpha_var,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Get encoder total loss.\n",
    "    encoder_total_loss = encoder.get_encoder_loss(\n",
    "        fake_images=generator_outputs,\n",
    "        encoded_images=encoded_images,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    return (real_logits,\n",
    "            fake_logits,\n",
    "            generator_total_loss,\n",
    "            discriminator_total_loss,\n",
    "            encoder_total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/train.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_gradients(loss, global_step, params, scope):\n",
    "    \"\"\"Returns the gradients and variables of the current training step.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the network's name to find its variables to train.\n",
    "\n",
    "    Returns:\n",
    "        Gradient tensors.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nget_gradients_and_variables\", \"loss\", loss)\n",
    "    print_obj(\"get_gradients_and_variables\", \"global_step\", global_step)\n",
    "    print_obj(\"get_gradients_and_variables\", \"scope\", scope)\n",
    "\n",
    "    # Get trainable variables.\n",
    "    variables = tf.trainable_variables(scope=scope)\n",
    "    print_obj(\"\\nget_gradients_and_variables\", \"variables\", variables)\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=variables,\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "    print_obj(\"\\nget_gradients_and_variables\", \"gradients\", gradients)\n",
    "\n",
    "    # Clip gradients.\n",
    "    if params[\"{}_clip_gradients\".format(scope)]:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=params[\"{}_clip_gradients\".format(scope)],\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "        print_obj(\"\\nget_gradients_and_variables\", \"gradients\", gradients)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def get_optimizer(params, scope):\n",
    "    \"\"\"Returns instance of chosen `Optimizer` class.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the current network's scope.\n",
    "\n",
    "    Returns:\n",
    "        Instance of chosen `Optimizer` class.\n",
    "    \"\"\"\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adam\": tf.train.AdamOptimizer,\n",
    "        \"Adadelta\": tf.train.AdadeltaOptimizer,\n",
    "        \"AdagradDA\": tf.train.AdagradDAOptimizer,\n",
    "        \"Adagrad\": tf.train.AdagradOptimizer,\n",
    "        \"Ftrl\": tf.train.FtrlOptimizer,\n",
    "        \"GradientDescent\": tf.train.GradientDescentOptimizer,\n",
    "        \"Momentum\": tf.train.MomentumOptimizer,\n",
    "        \"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer,\n",
    "        \"ProximalGradientDescent\": tf.train.ProximalGradientDescentOptimizer,\n",
    "        \"RMSProp\": tf.train.RMSPropOptimizer\n",
    "    }\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    optimizer_name = params[\"{}_optimizer\".format(scope)]\n",
    "    learning_rate = params[\"{}_learning_rate\".format(scope)]\n",
    "\n",
    "    optimizer = optimizers[optimizer_name](learning_rate=learning_rate)\n",
    "    print_obj(\"\\nget_optimizer\", \"optimizer\", optimizer)\n",
    "\n",
    "    # If using TPU, wrap optimizer to use an allreduce to aggregate gradients\n",
    "    # and broadcast the result to each shard.\n",
    "    if params[\"use_tpu\"]:\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(opt=optimizer)\n",
    "        print_obj(\"get_optimizer\", \"optimizer\", optimizer)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def jointly_train_generator_encoder(\n",
    "        generator_loss,\n",
    "        encoder_loss,\n",
    "        global_step,\n",
    "        params,\n",
    "        generator_scope,\n",
    "        encoder_scope,\n",
    "        discriminator_scope):\n",
    "    \"\"\"Returns generator's/encoder's combined objects needed for training.\n",
    "\n",
    "    Args:\n",
    "        generator_loss: tensor, generator's loss with shape [].\n",
    "        encoder_loss: tensor, encoder's loss with shape [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        generator_scope: str, the generator's name to find its variables.\n",
    "        encoder_scope: str, the encoder's name to find its variables.\n",
    "        discriminator_scope: str, the discriminator's name to find its\n",
    "            variables.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor anddict of gradient tensors.\n",
    "    \"\"\"\n",
    "    # Add generator and encoder losses together.\n",
    "    loss = tf.add(\n",
    "        x=generator_loss,\n",
    "        y=encoder_loss,\n",
    "        name=\"jointly_train_generator_encoder_add_loss\"\n",
    "    )\n",
    "    print_obj(\"\\njointly_train_generator_encoder\", \"loss\", loss)\n",
    "\n",
    "    # Get generator gradients.\n",
    "    generator_gradients = get_gradients(\n",
    "        loss=generator_loss,\n",
    "        global_step=global_step,\n",
    "        params=params,\n",
    "        scope=generator_scope\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\njointly_train_generator_encoder\",\n",
    "        \"generator_gradients\",\n",
    "        generator_gradients\n",
    "    )\n",
    "\n",
    "    # Get encoder gradients.\n",
    "    encoder_gradients = get_gradients(\n",
    "        loss=encoder_loss,\n",
    "        global_step=global_step,\n",
    "        params=params,\n",
    "        scope=encoder_scope\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\njointly_train_generator_encoder\",\n",
    "        \"encoder_gradients\",\n",
    "        encoder_gradients\n",
    "    )\n",
    "\n",
    "    # Get discriminator variables and set gradients to zero.\n",
    "    discriminator_variables = tf.trainable_variables(scope=\"discriminator\")\n",
    "    discriminator_gradients = [\n",
    "        tf.zeros_like(tensor=v, dtype=tf.float32)\n",
    "        for v in discriminator_variables\n",
    "    ]\n",
    "\n",
    "    # Combine gradients into a dictionary.\n",
    "    gradients = {\n",
    "        generator_scope: generator_gradients,\n",
    "        encoder_scope: encoder_gradients,\n",
    "        discriminator_scope: discriminator_gradients\n",
    "    }\n",
    "    print_obj(\"\\njointly_train_generator_encoder\", \"gradients\", gradients)\n",
    "\n",
    "    return loss, gradients\n",
    "\n",
    "\n",
    "def train_discriminator(\n",
    "        discriminator_loss,\n",
    "        global_step,\n",
    "        params,\n",
    "        generator_scope,\n",
    "        encoder_scope,\n",
    "        discriminator_scope):\n",
    "    \"\"\"Returns discriminator's objects needed for training.\n",
    "\n",
    "    Args:\n",
    "        discriminator_loss: tensor, discriminator's loss with shape [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        generator_scope: str, the generator's name to find its variables.\n",
    "        encoder_scope: str, the encoder's name to find its variables.\n",
    "        discriminator_scope: str, the discriminator's name to find its\n",
    "            variables.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor and dict of gradient tensors.\n",
    "    \"\"\"\n",
    "    # The loss is just the discriminator loss.\n",
    "    loss = discriminator_loss\n",
    "\n",
    "    # Get generator variables and set gradients to zero.\n",
    "    generator_variables = tf.trainable_variables(scope=generator_scope)\n",
    "    generator_gradients = [\n",
    "        tf.zeros_like(tensor=v, dtype=tf.float32)\n",
    "        for v in generator_variables\n",
    "    ]\n",
    "\n",
    "    # Get encoder variables and set gradients to zero.\n",
    "    encoder_variables = tf.trainable_variables(scope=encoder_scope)\n",
    "    encoder_gradients = [0. for _ in encoder_variables]\n",
    "    encoder_gradients = [\n",
    "        tf.zeros_like(tensor=v, dtype=tf.float32)\n",
    "        for v in encoder_variables\n",
    "    ]\n",
    "\n",
    "    # Get discriminator gradients.\n",
    "    discriminator_gradients = get_gradients(\n",
    "        loss=discriminator_loss,\n",
    "        global_step=global_step,\n",
    "        params=params,\n",
    "        scope=discriminator_scope\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\ntrain_discriminator\",\n",
    "        \"discriminator_gradients\",\n",
    "        discriminator_gradients\n",
    "    )\n",
    "\n",
    "    # Combine gradients into a dictionary.\n",
    "    gradients = {\n",
    "        generator_scope: generator_gradients,\n",
    "        encoder_scope: encoder_gradients,\n",
    "        discriminator_scope: discriminator_gradients\n",
    "    }\n",
    "    print_obj(\"\\ntrain_discriminator\", \"gradients\", gradients)\n",
    "\n",
    "    return loss, gradients\n",
    "\n",
    "\n",
    "def get_train_op(gradients, global_step, params, scope):\n",
    "    \"\"\"Returns train op of applying gradients with optimizer to variavles.\n",
    "\n",
    "    Args:\n",
    "        gradients: list, gradient tensors for in scope trainable variables.\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the network's name to find its variables to train.\n",
    "\n",
    "    Returns:\n",
    "        Training op.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nget_train_op\", \"gradients\", gradients)\n",
    "    print_obj(\"\\nget_train_op\", \"global_step\", global_step)\n",
    "    print_obj(\"get_train_op\", \"scope\", scope)\n",
    "\n",
    "    # Get trainables variables from scope.\n",
    "    variables = tf.trainable_variables(scope=scope)\n",
    "\n",
    "    # Zip together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, variables)\n",
    "    print_obj(\"\\nget_train_op\", \"grads_and_vars\", grads_and_vars)\n",
    "\n",
    "    # Get optimizers.\n",
    "    optimizer = get_optimizer(params=params, scope=scope)\n",
    "    print_obj(\"get_train_op\", \"optimizer\", optimizer)\n",
    "\n",
    "    # Create train op by applying gradients to variables and incrementing\n",
    "    # global step.\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        global_step=global_step,\n",
    "        name=\"{}_apply_gradients\".format(scope)\n",
    "    )\n",
    "    print_obj(\"get_train_op\", \"train_op\", train_op)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def get_train_ops(gradients, global_step, params):\n",
    "    \"\"\"Returns train op of applying gradients with optimizer to variavles.\n",
    "\n",
    "    Args:\n",
    "        gradients: list, gradient tensors for in scope trainable variables.\n",
    "        optimizer: instance of `Optimizer` class.\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Training op.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nget_train_ops\", \"gradients\", gradients)\n",
    "    print_obj(\"\\nget_train_ops\", \"global_step\", global_step)\n",
    "\n",
    "    # Create list of train ops.\n",
    "    train_ops = [\n",
    "        get_train_op(\n",
    "            gradients=g, global_step=global_step, params=params, scope=s\n",
    "        )\n",
    "        for s, g in gradients.items()\n",
    "    ]\n",
    "    print_obj(\"\\nget_train_op\", \"train_ops\", train_ops)\n",
    "\n",
    "    # Group together train ops.\n",
    "    train_op = tf.group(\n",
    "        train_ops,\n",
    "        name=\"jointly_train_generator_encoder_group_train_op\"\n",
    "    )\n",
    "    print_obj(\"\\nget_train_ops\", \"train_op\", train_op)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def update_alpha(global_step, alpha_var, params):\n",
    "    \"\"\"Returns update op for alpha variable.\n",
    "\n",
    "    Args:\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Alpha variable update operation.\n",
    "    \"\"\"\n",
    "    # If never grow, then no need to update alpha since it is not used.\n",
    "    if len(params[\"conv_num_filters\"]) > 1:\n",
    "        # Update alpha var to linearly scale from 0 to 1 based on steps.\n",
    "        alpha_var_update_op = tf.assign(\n",
    "            ref=alpha_var,\n",
    "            value=tf.divide(\n",
    "                x=tf.cast(\n",
    "                    x=tf.mod(\n",
    "                        x=global_step, y=params[\"num_steps_until_growth\"]\n",
    "                    ),\n",
    "                    dtype=tf.float32\n",
    "                ),\n",
    "                y=params[\"num_steps_until_growth\"]\n",
    "            ),\n",
    "            name=\"alpha_var_update_op_assign\"\n",
    "        )\n",
    "    else:\n",
    "        alpha_var_update_op = tf.no_op(name=\"alpha_var_update_op_no_op\")\n",
    "    print_obj(\n",
    "        \"update_alpha\", \"alpha_var_update_op\", alpha_var_update_op\n",
    "    )\n",
    "\n",
    "    return alpha_var_update_op\n",
    "\n",
    "\n",
    "def get_loss_and_train_op(\n",
    "        generator_total_loss,\n",
    "        encoder_total_loss,\n",
    "        discriminator_total_loss,\n",
    "        alpha_var,\n",
    "        params):\n",
    "    \"\"\"Returns loss and train op for train mode.\n",
    "\n",
    "    Args:\n",
    "        generator_total_loss: tensor, scalar total loss of generator.\n",
    "        encoder_total_loss: tensor, scalar total loss of encoder.\n",
    "        discriminator_total_loss: tensor, scalar total loss of discriminator.\n",
    "        alpha_var: variable, alpha for weighted sum of fade-in of layers.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Loss scalar tensor and train_op to be used by the EstimatorSpec.\n",
    "    \"\"\"\n",
    "    # Get global step.\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Determine if it is time to train generator or discriminator.\n",
    "    cycle_step = tf.mod(\n",
    "        x=global_step,\n",
    "        y=tf.cast(\n",
    "            x=tf.add(\n",
    "                x=params[\"generator_train_steps\"],\n",
    "                y=params[\"discriminator_train_steps\"]\n",
    "            ),\n",
    "            dtype=tf.int64\n",
    "        ),\n",
    "        name=\"get_loss_and_train_op_cycle_step\"\n",
    "    )\n",
    "\n",
    "    # Create choose generator condition.\n",
    "    condition = tf.less(\n",
    "        x=cycle_step,\n",
    "        y=params[\"generator_train_steps\"],\n",
    "        name=\"get_loss_and_train_op_condition\"\n",
    "    )\n",
    "\n",
    "    # Needed for batch normalization, but has no effect otherwise.\n",
    "    update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    with tf.control_dependencies(control_inputs=update_ops):\n",
    "        # Conditionally choose to train generator/encoder or discriminator.\n",
    "        loss, gradients = tf.cond(\n",
    "            pred=condition,\n",
    "            true_fn=lambda: jointly_train_generator_encoder(\n",
    "                generator_loss=generator_total_loss,\n",
    "                encoder_loss=encoder_total_loss,\n",
    "                global_step=global_step,\n",
    "                params=params,\n",
    "                generator_scope=\"generator\",\n",
    "                encoder_scope=\"encoder\",\n",
    "                discriminator_scope=\"discriminator\"\n",
    "            ),\n",
    "            false_fn=lambda: train_discriminator(\n",
    "                discriminator_loss=discriminator_total_loss,\n",
    "                global_step=global_step,\n",
    "                params=params,\n",
    "                generator_scope=\"generator\",\n",
    "                encoder_scope=\"encoder\",\n",
    "                discriminator_scope=\"discriminator\"\n",
    "            ),\n",
    "            name=\"get_loss_and_train_op_cond\"\n",
    "        )\n",
    "\n",
    "        print_obj(\"\\nget_loss_and_train_op\", \"gradients\", gradients)\n",
    "\n",
    "        # Crete train_op with whatever was returned from conditional branch.\n",
    "        train_op = get_train_ops(gradients, global_step, params)\n",
    "\n",
    "        # Get update op for the alpha variable.\n",
    "        alpha_var_update_op = update_alpha(global_step, alpha_var, params)\n",
    "\n",
    "        # Ensure alpha variable gets updated.\n",
    "        with tf.control_dependencies(control_inputs=[alpha_var_update_op]):\n",
    "            loss = tf.identity(\n",
    "                input=loss,\n",
    "                name=\"get_loss_and_train_op_loss_identity\"\n",
    "            )\n",
    "\n",
    "    return loss, train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval_metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/eval_metrics.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_eval_metric_ops(fake_logits, real_logits):\n",
    "    \"\"\"Gets eval metric ops.\n",
    "\n",
    "    Args:\n",
    "        fake_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "            discriminator having processed generator's output image.\n",
    "        real_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "            discriminator having processed real image.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of eval metric ops.\n",
    "    \"\"\"\n",
    "    # Concatenate discriminator logits and labels.\n",
    "    discriminator_logits = tf.concat(\n",
    "        values=[real_logits, fake_logits],\n",
    "        axis=0,\n",
    "        name=\"discriminator_concat_logits\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\get_eval_metric_ops\", \"discriminator_logits\", discriminator_logits\n",
    "    )\n",
    "\n",
    "    discriminator_labels = tf.concat(\n",
    "        values=[\n",
    "            tf.ones_like(tensor=real_logits),\n",
    "            tf.zeros_like(tensor=fake_logits)\n",
    "        ],\n",
    "        axis=0,\n",
    "        name=\"discriminator_concat_labels\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_eval_metric_ops\", \"discriminator_labels\", discriminator_labels\n",
    "    )\n",
    "\n",
    "    # Calculate discriminator probabilities.\n",
    "    discriminator_probabilities = tf.nn.sigmoid(\n",
    "        x=discriminator_logits, name=\"discriminator_probabilities\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_eval_metric_ops\",\n",
    "        \"discriminator_probabilities\",\n",
    "        discriminator_probabilities\n",
    "    )\n",
    "\n",
    "    # Create eval metric ops dictionary.\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_accuracy\"\n",
    "        ),\n",
    "        \"precision\": tf.metrics.precision(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_precision\"\n",
    "        ),\n",
    "        \"recall\": tf.metrics.recall(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_recall\"\n",
    "        ),\n",
    "        \"auc_roc\": tf.metrics.auc(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            num_thresholds=200,\n",
    "            curve=\"ROC\",\n",
    "            name=\"discriminator_auc_roc\"\n",
    "        ),\n",
    "        \"auc_pr\": tf.metrics.auc(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            num_thresholds=200,\n",
    "            curve=\"PR\",\n",
    "            name=\"discriminator_auc_pr\"\n",
    "        )\n",
    "    }\n",
    "    print_obj(\"get_eval_metric_ops\", \"eval_metric_ops\", eval_metric_ops)\n",
    "\n",
    "    return eval_metric_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## anomaly.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/anomaly.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def minmax_normalization(X):\n",
    "    min_x = tf.reduce_min(input_tensor=X, name=\"minmax_normalization_min\")\n",
    "\n",
    "    max_x = tf.reduce_min(input_tensor=X, name=\"minmax_normalization_max\")\n",
    "\n",
    "    X_normalized = tf.math.divide_no_nan(\n",
    "        x=X - min_x, y=max_x, name=\"minmax_normalization_normalized\"\n",
    "    )\n",
    "\n",
    "    return X_normalized\n",
    "\n",
    "\n",
    "def get_residual_loss(query_images, encoded_images, params):\n",
    "    \"\"\"Gets residual loss between query and encoded images.\n",
    "\n",
    "    Args:\n",
    "        query_images: tensor, query image input for predictions.\n",
    "        encoded_images: tensor, image from generator from encoder's logits.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Residual loss scalar tensor.\n",
    "    \"\"\"\n",
    "    # Minmax normalize query images.\n",
    "    query_images_normalized = minmax_normalization(X=query_images)\n",
    "    print_obj(\n",
    "        \"\\nquery_images_normalized\",\n",
    "        \"query_images_normalized\",\n",
    "        query_images_normalized\n",
    "    )\n",
    "\n",
    "    # Minmax normalize encoded images.\n",
    "    encoded_images_normalized = minmax_normalization(X=encoded_images)\n",
    "\n",
    "    # Find pixel difference between the normalized query and encoded images.\n",
    "    image_difference = tf.subtract(\n",
    "        x=query_images_normalized,\n",
    "        y=encoded_images_normalized,\n",
    "        name=\"image_difference\"\n",
    "    )\n",
    "    print_obj(\"get_residual_loss\", \"image_difference\", image_difference)\n",
    "\n",
    "    # Take L2 norm of difference.\n",
    "    image_difference_l2_norm = tf.reduce_sum(\n",
    "        input_tensor=tf.square(x=image_difference),\n",
    "        axis=[1, 2, 3],\n",
    "        name=\"image_difference_l2_norm\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_residual_loss\",\n",
    "        \"image_difference_l2_norm\",\n",
    "        image_difference_l2_norm\n",
    "    )\n",
    "\n",
    "    # Scale by image dimension sizes to get residual loss.\n",
    "    residual_loss = tf.divide(\n",
    "        x=image_difference_l2_norm,\n",
    "        y=tf.cast(\n",
    "            x=params[\"height\"] * params[\"width\"] * params[\"depth\"],\n",
    "            dtype=tf.float32\n",
    "        ),\n",
    "        name=\"residual_loss\"\n",
    "    )\n",
    "    print_obj(\"get_residual_loss\", \"residual_loss\", residual_loss)\n",
    "\n",
    "    return residual_loss\n",
    "\n",
    "\n",
    "def get_origin_distance_loss(encoder_logits, params):\n",
    "    \"\"\"Gets origin distance loss measuring distance of z-hat from origin.\n",
    "\n",
    "    Args:\n",
    "        encoder_logits: tensor, encoder's logits encoded from query images.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Origin distance loss scalar tensor.\n",
    "    \"\"\"\n",
    "    # Take L2 norm of difference.\n",
    "    z_hat_l2_norm = tf.sqrt(\n",
    "        x=tf.reduce_sum(\n",
    "            input_tensor=tf.square(x=encoder_logits),\n",
    "            axis=-1\n",
    "        ),\n",
    "        name=\"z_hat_l2_norm\"\n",
    "    )\n",
    "    print_obj(\"\\nget_origin_distance_loss\", \"z_hat_l2_norm\", z_hat_l2_norm)\n",
    "\n",
    "    # Scale by latent size to get origin distance loss.\n",
    "    origin_distance_loss = tf.divide(\n",
    "        x=-z_hat_l2_norm,\n",
    "        y=tf.sqrt(x=tf.cast(x=params[\"latent_size\"], dtype=tf.float32)),\n",
    "        name=\"origin_distance_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_origin_distance_loss\",\n",
    "        \"origin_distance_loss\",\n",
    "        origin_distance_loss\n",
    "    )\n",
    "\n",
    "    return origin_distance_loss\n",
    "\n",
    "\n",
    "def get_anomaly_scores(query_images, encoder_logits, encoded_images, params):\n",
    "    \"\"\"Gets anomaly scores from query and encoded images.\n",
    "\n",
    "    Args:\n",
    "        query_images: tensor, query image input for predictions.\n",
    "        encoder_logits: tensor, encoder's logits encoded from query images.\n",
    "        encoded_images: tensor, image from generator from encoder's logits.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Predictions dictionary and export outputs dictionary.\n",
    "    \"\"\"\n",
    "    # Get residual loss.\n",
    "    residual_loss = get_residual_loss(query_images, encoded_images, params)\n",
    "    print_obj(\"\\nget_anomaly_scores\", \"residual_loss\", residual_loss)\n",
    "\n",
    "    # Get origin distance loss.\n",
    "    origin_dist_loss = get_origin_distance_loss(encoder_logits, params)\n",
    "    print_obj(\"get_anomaly_scores\", \"origin_dist_loss\", origin_dist_loss)\n",
    "\n",
    "    # Get anomaly scores.\n",
    "    residual_scl = params[\"anom_convex_combo_factor\"] * residual_loss\n",
    "    origin_scl = (1. - params[\"anom_convex_combo_factor\"]) * origin_dist_loss\n",
    "    anomaly_scores = tf.add(\n",
    "        x=residual_scl, y=origin_scl, name=\"anomaly_scores\"\n",
    "    )\n",
    "    print_obj(\"get_anomaly_scores\", \"anomaly_scores\", anomaly_scores)\n",
    "\n",
    "    return anomaly_scores\n",
    "\n",
    "\n",
    "def anomaly_detection(query_images, encoder_logits, encoded_images, params):\n",
    "    \"\"\"Gets anomaly scores from query and encoded images.\n",
    "\n",
    "    Args:\n",
    "        query_images: tensor, query image input for predictions.\n",
    "        encoder_logits: tensor, encoder's logits encoded from query images.\n",
    "        encoded_images: tensor, image from generator from encoder's logits.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Predictions dictionary and export outputs dictionary.\n",
    "    \"\"\"\n",
    "    # Get anomaly scores.\n",
    "    anomaly_scores = get_anomaly_scores(\n",
    "        query_images, encoder_logits, encoded_images, params\n",
    "    )\n",
    "    print_obj(\"\\nanomaly_detection\", \"anomaly_scores\", anomaly_scores)\n",
    "\n",
    "    # Get anomaly flags.\n",
    "    anomaly_flags = tf.cast(\n",
    "        x=tf.greater(x=anomaly_scores, y=params[\"anomaly_threshold\"]),\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "    print_obj(\"anomaly_detection\", \"anomaly_flags\", anomaly_flags)\n",
    "\n",
    "    return anomaly_scores, anomaly_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/predict.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import anomaly\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_predictions(query_images, generator, encoder, params, block_idx):\n",
    "    \"\"\"Gets predictions from query image.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, tf.float32 query image of shape\n",
    "            [None, height, width, depth].\n",
    "        generator: instance of generator.`Generator`.\n",
    "        encoder: instance of encoder.`Encoder`.\n",
    "        params: dict, user passed parameters.\n",
    "        block_idx: int, current conv layer block's index.\n",
    "\n",
    "    Returns:\n",
    "        Predictions dictionary of encoded images from generator, anomaly\n",
    "            scores, and anomaly flags.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nget_predictions\", \"query_images\", query_images)\n",
    "    # Resize query image.\n",
    "    resized_query_images = image_utils.resize_real_image(\n",
    "        image=query_images, params=params, block_idx=block_idx\n",
    "    )\n",
    "    print_obj(\"get_predictions\", \"resized_query_images\", resized_query_images)\n",
    "\n",
    "    # Get encoder logits using query images.\n",
    "    print(\n",
    "        \"\\nCall encoder with resized_query_images = {}.\".format(\n",
    "            resized_query_images\n",
    "        )\n",
    "    )\n",
    "    encoder_logits = encoder.get_predict_encoder_logits(\n",
    "        X=resized_query_images, params=params, block_idx=block_idx\n",
    "    )\n",
    "    print_obj(\"get_predictions\", \"encoder_logits\", encoder_logits)\n",
    "\n",
    "    # Get encoded images from generator using encoder's logits.\n",
    "    print(\n",
    "        \"\\nCall generator with encoder_logits = {}.\".format(\n",
    "            encoder_logits\n",
    "        )\n",
    "    )\n",
    "    encoded_images = generator.get_predict_generator_outputs(\n",
    "        Z=encoder_logits, params=params, block_idx=block_idx\n",
    "    )\n",
    "    print_obj(\"get_predictions\", \"encoded_images\", encoded_images)\n",
    "\n",
    "    # Perform anomaly detection.\n",
    "    anomaly_scores, anomaly_flags = anomaly.anomaly_detection(\n",
    "        resized_query_images, encoder_logits, encoded_images, params\n",
    "    )\n",
    "    print_obj(\"get_predictions\", \"anomaly_scores\", anomaly_scores)\n",
    "    print_obj(\"get_predictions\", \"anomaly_flags\", anomaly_flags)\n",
    "\n",
    "    # Calculate image size for returned dict keys.\n",
    "    image_dim = 4 * 2 ** block_idx\n",
    "    image_size = \"{}x{}\".format(image_dim, image_dim)\n",
    "\n",
    "    return {\n",
    "        \"encoded_images_{}\".format(image_size): encoded_images,\n",
    "        \"anomaly_scores_{}\".format(image_size): anomaly_scores,\n",
    "        \"anomaly_flags_{}\".format(image_size): anomaly_flags\n",
    "    }\n",
    "\n",
    "\n",
    "def get_predictions_and_export_outputs(features, generator, encoder, params):\n",
    "    \"\"\"Gets predictions and serving export outputs.\n",
    "\n",
    "    Args:\n",
    "        features: dict, feature tensors from serving input function.\n",
    "        generator: instance of generator.`Generator`.\n",
    "        encoder: instance of encoder.`Encoder`.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Predictions dictionary and export outputs dictionary.\n",
    "    \"\"\"\n",
    "    # Extract given query image from features dictionary.\n",
    "    query_images = features[\"query_image\"]\n",
    "    print_obj(\n",
    "        \"\\nget_predictions_and_export_outputs\", \"query_images\", query_images\n",
    "    )\n",
    "\n",
    "    loop_end = len(params[\"conv_num_filters\"])\n",
    "    loop_start = 0 if params[\"predict_all_resolutions\"] else loop_end - 1\n",
    "    print_obj(\"get_predictions_and_export_outputs\", \"loop_start\", loop_start)\n",
    "    print_obj(\"get_predictions_and_export_outputs\", \"loop_end\", loop_end)\n",
    "\n",
    "    # Create predictions dictionary.\n",
    "    predictions_dict = {}\n",
    "    for i in range(loop_start, loop_end):\n",
    "        predictions = get_predictions(\n",
    "            query_images=query_images,\n",
    "            generator=generator,\n",
    "            encoder=encoder,\n",
    "            params=params,\n",
    "            block_idx=i\n",
    "        )\n",
    "        predictions_dict.update(predictions)\n",
    "    print_obj(\n",
    "        \"get_predictions_and_export_outputs\",\n",
    "        \"predictions_dict\",\n",
    "        predictions_dict\n",
    "    )\n",
    "\n",
    "    # Create export outputs.\n",
    "    export_outputs = {\n",
    "        \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "            outputs=predictions_dict)\n",
    "    }\n",
    "    print_obj(\n",
    "        \"get_predictions_and_export_outputs\", \"export_outputs\", export_outputs\n",
    "    )\n",
    "\n",
    "    return predictions_dict, export_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pg_anogan_sim_enc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/pg_anogan_sim_enc.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import discriminator\n",
    "from . import encoder\n",
    "from . import eval_metrics\n",
    "from . import generator\n",
    "from . import image_utils\n",
    "from . import predict\n",
    "from . import train\n",
    "from . import train_and_eval\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def pg_anogan_sim_enc_model(features, labels, mode, params):\n",
    "    \"\"\"PGAnoGAN with simultaneous encoder custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `tf.estimator.EstimatorSpec` class.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\npg_anogan_sim_enc_model\", \"features\", features)\n",
    "    print_obj(\"pg_anogan_sim_enc_model\", \"labels\", labels)\n",
    "    print_obj(\"pg_anogan_sim_enc_model\", \"mode\", mode)\n",
    "    print_obj(\"pg_anogan_sim_enc_model\", \"params\", params)\n",
    "\n",
    "    # Loss function, training/eval ops, etc.\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "\n",
    "    # Instantiate generator.\n",
    "    pgan_generator = generator.Generator(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"generator_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"generator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        params=params,\n",
    "        name=\"generator\"\n",
    "    )\n",
    "\n",
    "    # Instantiate discriminator.\n",
    "    pgan_discriminator = discriminator.Discriminator(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"discriminator_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"discriminator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        params=params,\n",
    "        name=\"discriminator\"\n",
    "    )\n",
    "\n",
    "    # Instantiate encoder.\n",
    "    pgan_encoder = encoder.Encoder(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"encoder_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"encoder_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        params=params,\n",
    "        name=\"encoder\"\n",
    "    )\n",
    "\n",
    "    # Create alpha variable to use for weighted sum for smooth fade-in.\n",
    "    alpha_var = tf.get_variable(\n",
    "        name=\"alpha_var\",\n",
    "        dtype=tf.float32,\n",
    "        # When the initializer is a function, tensorflow can place it\n",
    "        # \"outside of the control flow context\" to make sure it always runs.\n",
    "        initializer=lambda: tf.zeros(shape=[], dtype=tf.float32),\n",
    "        trainable=False\n",
    "    )\n",
    "    print_obj(\"pg_anogan_sim_enc_model\", \"alpha_var\", alpha_var)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Get predictions and export outputs for EstimatorSpec.\n",
    "        (predictions_dict,\n",
    "         export_outputs) = predict.get_predictions_and_export_outputs(\n",
    "            features=features,\n",
    "            generator=pgan_generator,\n",
    "            encoder=pgan_encoder,\n",
    "            params=params\n",
    "        )\n",
    "    else:\n",
    "        # Get logits and losses from networks for train and eval modes.\n",
    "        (real_logits,\n",
    "         fake_logits,\n",
    "         generator_total_loss,\n",
    "         discriminator_total_loss,\n",
    "         encoder_total_loss) = train_and_eval.get_logits_and_losses(\n",
    "            features=features,\n",
    "            generator=pgan_generator,\n",
    "            discriminator=pgan_discriminator,\n",
    "            encoder=pgan_encoder,\n",
    "            alpha_var=alpha_var,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Get loss and train op for EstimatorSpec.\n",
    "            loss, train_op = train.get_loss_and_train_op(\n",
    "                generator_total_loss=generator_total_loss,\n",
    "                encoder_total_loss=encoder_total_loss,\n",
    "                discriminator_total_loss=discriminator_total_loss,\n",
    "                alpha_var=alpha_var,\n",
    "                params=params\n",
    "            )\n",
    "        else:\n",
    "            loss = discriminator_total_loss\n",
    "\n",
    "            if params[\"use_tpu\"]:\n",
    "                eval_metric_ops = (\n",
    "                    eval_metrics.get_eval_metric_ops,\n",
    "                    {\"real_logits\": real_logits, \"fake_logits\": fake_logits}\n",
    "                )\n",
    "            else:\n",
    "                eval_metric_ops = eval_metrics.get_eval_metric_ops(\n",
    "                    real_logits, fake_logits\n",
    "                )\n",
    "\n",
    "    if params[\"eval_on_tpu\"]:\n",
    "        # Return TPUEstimatorSpec\n",
    "        return tf.estimator.tpu.TPUEstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions_dict,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metrics=eval_metric_ops,\n",
    "            export_outputs=export_outputs\n",
    "        )\n",
    "    else:\n",
    "        # Return EstimatorSpec\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions_dict,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metric_ops,\n",
    "            export_outputs=export_outputs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/serving.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def serving_input_fn(params):\n",
    "    \"\"\"Serving input function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        ServingInputReceiver object containing features and receiver tensors.\n",
    "    \"\"\"\n",
    "    # Create placeholders to accept data sent to the model at serving time.\n",
    "    # shape = (batch_size,)\n",
    "    feature_placeholders = {\n",
    "        \"query_image\": tf.placeholder(\n",
    "            dtype=tf.uint8,\n",
    "            shape=[None, params[\"height\"], params[\"width\"], params[\"depth\"]],\n",
    "            name=\"serving_input_placeholder_query_image\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"\\nserving_input_fn\",\n",
    "        \"feature_placeholders\",\n",
    "        feature_placeholders\n",
    "    )\n",
    "\n",
    "    # Create clones of the feature placeholder tensors so that the SavedModel\n",
    "    # SignatureDef will point to the placeholder.\n",
    "    features = {\n",
    "        key: tf.identity(\n",
    "            input=value,\n",
    "            name=\"serving_input_fn_identity_placeholder_{}\".format(key)\n",
    "        )\n",
    "        for key, value in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    # Apply same processing to image as train and eval.\n",
    "    features[\"query_image\"] = image_utils.preprocess_image(\n",
    "        image=features[\"query_image\"], params=params\n",
    "    )\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"features\",\n",
    "        features\n",
    "    )\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=feature_placeholders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/model.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import input\n",
    "from . import serving\n",
    "from . import pg_anogan_sim_enc\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates custom Estimator model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        `Estimator` object.\n",
    "    \"\"\"\n",
    "    print_obj(\"train_and_evaluate\", \"args\", args)\n",
    "    # Set logging to be level of INFO.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Create TPU config.\n",
    "    if args[\"use_tpu\"]:\n",
    "        STEPS_PER_EVAL = args[\"num_steps_until_growth\"]\n",
    "        tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
    "        # This is the TPU initialization code that has to be at the beginning.\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "#         print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "        config = tf.contrib.tpu.RunConfig(\n",
    "            cluster=tpu_cluster_resolver,\n",
    "#             master=,\n",
    "            model_dir=args[\"output_dir\"],\n",
    "            save_checkpoints_steps=STEPS_PER_EVAL,\n",
    "            tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "                iterations_per_loop=STEPS_PER_EVAL,\n",
    "                per_host_input_for_training=True\n",
    "            )\n",
    "        )\n",
    "#   is_per_host = contrib_tpu.InputPipelineConfig.PER_HOST_V2\n",
    "#   run_config = contrib_tpu.RunConfig(\n",
    "#       cluster=tpu_cluster_resolver,\n",
    "#       master=FLAGS.master,\n",
    "#       model_dir=FLAGS.output_dir,\n",
    "#       save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "#       tpu_config=contrib_tpu.TPUConfig(\n",
    "#           iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "#           num_shards=FLAGS.num_tpu_cores,\n",
    "#           per_host_input_for_training=is_per_host))\n",
    "    else:\n",
    "        config = tf.contrib.tpu.RunConfig()\n",
    "\n",
    "    # Create our custom estimator using our model function.\n",
    "    estimator = tf.estimator.tpu.TPUEstimator(\n",
    "        model_fn=pg_anogan_sim_enc.pg_anogan_sim_enc_model,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        config=config,\n",
    "        params=args,\n",
    "        use_tpu=args[\"use_tpu\"],\n",
    "        train_batch_size=args[\"train_batch_size\"],\n",
    "        eval_batch_size=args[\"eval_batch_size\"],\n",
    "        eval_on_tpu=args[\"eval_on_tpu\"],\n",
    "        export_to_tpu=args[\"export_to_tpu\"],\n",
    "        export_to_cpu=args[\"export_to_cpu\"]\n",
    "    )\n",
    "\n",
    "    if args[\"use_tpu\"]:\n",
    "        # Train estimator.\n",
    "        estimator.train(\n",
    "            input_fn=input.read_dataset(\n",
    "                filename=args[\"train_file_pattern\"],\n",
    "                mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                batch_size=args[\"train_batch_size\"],\n",
    "                params=args\n",
    "            ),\n",
    "            max_steps=args[\"train_steps\"]\n",
    "        )\n",
    "\n",
    "        # Export SavedModel.\n",
    "        tf.logging.info(\"Starting to export model.\")\n",
    "        estimator.export_savedmodel(\n",
    "            export_dir_base=os.path.join(\n",
    "                args[\"output_dir\"], \"export/exporter\"\n",
    "            ),\n",
    "            serving_input_receiver_fn=lambda: serving.serving_input_fn(args)\n",
    "        )\n",
    "    else:\n",
    "        # Create train spec to read in our training data.\n",
    "        train_spec = tf.estimator.TrainSpec(\n",
    "            input_fn=input.read_dataset(\n",
    "                filename=args[\"train_file_pattern\"],\n",
    "                mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                batch_size=args[\"train_batch_size\"],\n",
    "                params=args\n",
    "            ),\n",
    "            max_steps=args[\"train_steps\"]\n",
    "        )\n",
    "\n",
    "        # Create exporter to save out the complete model to disk.\n",
    "        exporter = tf.estimator.LatestExporter(\n",
    "            name=\"exporter\",\n",
    "            serving_input_receiver_fn=lambda: serving.serving_input_fn(args),\n",
    "            exports_to_keep=args[\"exports_to_keep\"]\n",
    "        )\n",
    "\n",
    "        # Create eval spec to read in our validation data and export our model.\n",
    "        eval_spec = tf.estimator.EvalSpec(\n",
    "            input_fn=input.read_dataset(\n",
    "                filename=args[\"eval_file_pattern\"],\n",
    "                mode=tf.estimator.ModeKeys.EVAL,\n",
    "                batch_size=args[\"eval_batch_size\"],\n",
    "                params=args\n",
    "            ),\n",
    "            steps=args[\"eval_steps\"],\n",
    "            start_delay_secs=args[\"start_delay_secs\"],\n",
    "            throttle_secs=args[\"throttle_secs\"],\n",
    "            exporters=exporter\n",
    "        )\n",
    "\n",
    "        # Create train and evaluate loop to train and evaluate our estimator.\n",
    "        tf.estimator.train_and_evaluate(\n",
    "            estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pg_anogan_sim_enc_module/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "def calc_generator_discriminator_conv_layer_properties(\n",
    "        conv_num_filters, conv_kernel_sizes, conv_strides, depth):\n",
    "    \"\"\"Calculates generator and discriminator conv layer properties.\n",
    "\n",
    "    Args:\n",
    "        num_filters: list, nested list of ints of the number of filters\n",
    "            for each conv layer.\n",
    "        kernel_sizes: list, nested list of ints of the kernel sizes for\n",
    "            each conv layer.\n",
    "        strides: list, nested list of ints of the strides for each conv\n",
    "            layer.\n",
    "        depth: int, depth dimension of images.\n",
    "\n",
    "    Returns:\n",
    "        Nested lists of conv layer properties for both generator and\n",
    "            discriminator.\n",
    "    \"\"\"\n",
    "    def make_generator(num_filters, kernel_sizes, strides, depth):\n",
    "        \"\"\"Calculates generator conv layer properties.\n",
    "\n",
    "        Args:\n",
    "            num_filters: list, nested list of ints of the number of filters\n",
    "                for each conv layer.\n",
    "            kernel_sizes: list, nested list of ints of the kernel sizes for\n",
    "                each conv layer.\n",
    "            strides: list, nested list of ints of the strides for each conv\n",
    "                layer.\n",
    "            depth: int, depth dimension of images.\n",
    "\n",
    "        Returns:\n",
    "            Nested list of conv layer properties for generator.\n",
    "        \"\"\"\n",
    "        # Get the number of growths.\n",
    "        num_growths = len(num_filters) - 1\n",
    "\n",
    "        # Make base block.\n",
    "        in_out = num_filters[0]\n",
    "        base = [\n",
    "            [kernel_sizes[0][i]] * 2 + in_out + [strides[0][i]] * 2\n",
    "            for i in range(len(num_filters[0]))\n",
    "        ]\n",
    "        blocks = [base]\n",
    "\n",
    "        # Add growth blocks.\n",
    "        for i in range(1, num_growths + 1):\n",
    "            in_out = [[blocks[i - 1][-1][-3], num_filters[i][0]]]\n",
    "            block = [[kernel_sizes[i][0]] * 2 + in_out[0] + [strides[i][0]] * 2]\n",
    "            for j in range(1, len(num_filters[i])):\n",
    "                in_out.append([block[-1][-3], num_filters[i][j]])\n",
    "                block.append(\n",
    "                    [kernel_sizes[i][j]] * 2 + in_out[j] + [strides[i][j]] * 2\n",
    "                )\n",
    "            blocks.append(block)\n",
    "\n",
    "        # Add toRGB conv.\n",
    "        blocks[-1].append([1, 1, blocks[-1][-1][-3], depth] + [1] * 2)\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def make_discriminator(generator):\n",
    "        \"\"\"Calculates discriminator conv layer properties.\n",
    "\n",
    "        Args:\n",
    "            generator: list, nested list of conv layer properties for\n",
    "                generator.\n",
    "\n",
    "        Returns:\n",
    "            Nested list of conv layer properties for discriminator.\n",
    "        \"\"\"\n",
    "        # Reverse generator.\n",
    "        discriminator = generator[::-1]\n",
    "\n",
    "        # Reverse input and output shapes.\n",
    "        discriminator = [\n",
    "            [\n",
    "                conv[0:2] + conv[2:4][::-1] + conv[-2:]\n",
    "                for conv in block[::-1]\n",
    "            ]\n",
    "            for block in discriminator\n",
    "        ]\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    # Calculate conv layer properties for generator using args.\n",
    "    generator = make_generator(\n",
    "        conv_num_filters, conv_kernel_sizes, conv_strides, depth\n",
    "    )\n",
    "\n",
    "    # Calculate conv layer properties for discriminator using generator\n",
    "    # properties.\n",
    "    discriminator = make_discriminator(generator)\n",
    "\n",
    "    return generator, discriminator\n",
    "\n",
    "\n",
    "def split_up_generator_conv_layer_properties(\n",
    "        generator, num_filters, strides, depth):\n",
    "    \"\"\"Splits up generator conv layer properties into lists.\n",
    "\n",
    "    Args:\n",
    "        generator: list, nested list of conv layer properties for\n",
    "            generator.\n",
    "        num_filters: list, nested list of ints of the number of filters\n",
    "            for each conv layer.\n",
    "        strides: list, nested list of ints of the strides for each conv\n",
    "            layer.\n",
    "        depth: int, depth dimension of images.\n",
    "\n",
    "    Returns:\n",
    "        Nested lists of conv layer properties for generator.\n",
    "    \"\"\"\n",
    "    generator_base_conv_blocks = [generator[0][0:len(num_filters[0])]]\n",
    "\n",
    "    generator_growth_conv_blocks = []\n",
    "    if len(num_filters) > 1:\n",
    "        generator_growth_conv_blocks = generator[1:-1] + [generator[-1][:-1]]\n",
    "\n",
    "    generator_to_rgb_layers = [\n",
    "        [[1] * 2 + [num_filters[i][0]] + [depth] + [strides[i][0]] * 2]\n",
    "        for i in range(len(num_filters))\n",
    "    ]\n",
    "\n",
    "    return (generator_base_conv_blocks,\n",
    "            generator_growth_conv_blocks,\n",
    "            generator_to_rgb_layers)\n",
    "\n",
    "\n",
    "def split_up_discriminator_conv_layer_properties(\n",
    "        discriminator, num_filters, strides, depth):\n",
    "    \"\"\"Splits up discriminator conv layer properties into lists.\n",
    "\n",
    "    Args:\n",
    "        discriminator: list, nested list of conv layer properties for\n",
    "            discriminator.\n",
    "        num_filters: list, nested list of ints of the number of filters\n",
    "            for each conv layer.\n",
    "        strides: list, nested list of ints of the strides for each conv\n",
    "            layer.\n",
    "        depth: int, depth dimension of images.\n",
    "\n",
    "    Returns:\n",
    "        Nested lists of conv layer properties for discriminator.\n",
    "    \"\"\"\n",
    "    discriminator_from_rgb_layers = [\n",
    "        [[1] * 2 + [depth] + [num_filters[i][0]] + [strides[i][0]] * 2]\n",
    "        for i in range(len(num_filters))\n",
    "    ]\n",
    "\n",
    "    if len(num_filters) > 1:\n",
    "        discriminator_base_conv_blocks = [discriminator[-1]]\n",
    "    else:\n",
    "        discriminator_base_conv_blocks = [discriminator[-1][1:]]\n",
    "\n",
    "    discriminator_growth_conv_blocks = []\n",
    "    if len(num_filters) > 1:\n",
    "        discriminator_growth_conv_blocks = [discriminator[0][1:]] + discriminator[1:-1]\n",
    "        discriminator_growth_conv_blocks = discriminator_growth_conv_blocks[::-1]\n",
    "\n",
    "    return (discriminator_from_rgb_layers,\n",
    "            discriminator_base_conv_blocks,\n",
    "            discriminator_growth_conv_blocks)\n",
    "\n",
    "\n",
    "def convert_string_to_bool(string):\n",
    "    \"\"\"Converts string to bool.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        Boolean conversion of string.\n",
    "    \"\"\"\n",
    "    return False if string.lower() == \"false\" else True\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_float(string):\n",
    "    \"\"\"Converts string to None or float.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or float conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else float(string)\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_int(string):\n",
    "    \"\"\"Converts string to None or int.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or int conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else int(string)\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_ints(string, sep):\n",
    "    \"\"\"Converts string to list of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of ints conversion of string.\n",
    "    \"\"\"\n",
    "    return [int(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_lists_of_ints(string, outer_sep, inner_sep):\n",
    "    \"\"\"Converts string to list of lists of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        outer_sep: str, separator for outer list string.\n",
    "        inner_sep: str, separator for inner list string.\n",
    "\n",
    "    Returns:\n",
    "        List of lists of ints conversion of string.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        convert_string_to_list_of_ints(x, inner_sep)\n",
    "        for x in string.split(outer_sep)\n",
    "    ]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # File arguments.\n",
    "    parser.add_argument(\n",
    "        \"--train_file_pattern\",\n",
    "        help=\"GCS location to read training data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_file_pattern\",\n",
    "        help=\"GCS location to read evaluation data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud.\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "\n",
    "    # Training parameters.\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Number of examples in training batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Number of steps to train for.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prev_train_steps\",\n",
    "        help=\"Number of steps already been trained in previous runs.\",\n",
    "        type=int,\n",
    "        default=0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_tpu\",\n",
    "        help=\"Whether want to use TPU or not.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "    # Eval parameters.\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        help=\"Number of examples in evaluation batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Number of steps to evaluate for.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--start_delay_secs\",\n",
    "        help=\"Number of seconds to wait before first evaluation.\",\n",
    "        type=int,\n",
    "        default=60\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--throttle_secs\",\n",
    "        help=\"Number of seconds to wait between evaluations.\",\n",
    "        type=int,\n",
    "        default=120\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_on_tpu\",\n",
    "        help=\"Whether want to evaluate on TPU or not.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "    # Serving parameters.\n",
    "    parser.add_argument(\n",
    "        \"--exports_to_keep\",\n",
    "        help=\"Number of exports to keep before overwriting oldest.\",\n",
    "        type=int,\n",
    "        default=5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--export_to_tpu\",\n",
    "        help=\"Whether want to export to TPU or not.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--export_to_cpu\",\n",
    "        help=\"Whether want to export to CPU or not.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--predict_all_resolutions\",\n",
    "        help=\"If want all resolutions predicted or just largest one.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--anomaly_threshold\",\n",
    "        help=\"Anomaly threshold to test anomaly scores against.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--anom_convex_combo_factor\",\n",
    "        help=\"Weight for sum of residual and origin distance loss for anomaly scores.\",\n",
    "        type=float,\n",
    "        default=0.05\n",
    "    )\n",
    "\n",
    "    # Image parameters.\n",
    "    parser.add_argument(\n",
    "        \"--height\",\n",
    "        help=\"Height of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--width\",\n",
    "        help=\"Width of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--depth\",\n",
    "        help=\"Depth of image.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "\n",
    "    # Shared parameters.\n",
    "    parser.add_argument(\n",
    "        \"--num_steps_until_growth\",\n",
    "        help=\"Number of steps until layer added to generator & discriminator.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--conv_num_filters\",\n",
    "        help=\"Number of filters for growth conv layers.\",\n",
    "        type=str,\n",
    "        default=\"512,512;512,512\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--conv_kernel_sizes\",\n",
    "        help=\"Kernel sizes for growth conv layers.\",\n",
    "        type=str,\n",
    "        default=\"3,3;3,3\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--conv_strides\",\n",
    "        help=\"Strides for growth conv layers.\",\n",
    "        type=str,\n",
    "        default=\"1,1;1,1\"\n",
    "    )\n",
    "\n",
    "    # Generator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--latent_size\",\n",
    "        help=\"The latent size of the noise vector.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_pixel_norm\",\n",
    "        help=\"If want to use pixel norm op after each convolution.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pixel_norm_epsilon\",\n",
    "        help=\"Small value to add to denominator for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalize_latent\",\n",
    "        help=\"If want to normalize latent vector before projection.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_projection_dims\",\n",
    "        help=\"The 3D dimensions to project latent noise vector into.\",\n",
    "        type=str,\n",
    "        default=\"8,8,256\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_optimizer\",\n",
    "        help=\"Name of optimizer to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for generator.\",\n",
    "        type=float,\n",
    "        default=0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_train_steps\",\n",
    "        help=\"Number of steps to train generator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Discriminator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--use_minibatch_stddev\",\n",
    "        help=\"If want to use minibatch stddev op before first base conv layer.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--minibatch_stddev_group_size\",\n",
    "        help=\"The size of groups to split minibatch examples into.\",\n",
    "        type=int,\n",
    "        default=4\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--minibatch_stddev_averaging\",\n",
    "        help=\"If want to average across feature maps and pixels for minibatch stddev.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_optimizer\",\n",
    "        help=\"Name of optimizer to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_gradient_penalty_coefficient\",\n",
    "        help=\"Coefficient of gradient penalty for discriminator.\",\n",
    "        type=float,\n",
    "        default=10.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon_drift\",\n",
    "        help=\"Coefficient of epsilon drift penalty for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_train_steps\",\n",
    "        help=\"Number of steps to train discriminator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Encoder parameters.\n",
    "    parser.add_argument(\n",
    "        \"--encoder_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for encoder.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoder_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for encoder.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoder_optimizer\",\n",
    "        help=\"Name of optimizer to use for encoder.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoder_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for encoder.\",\n",
    "        type=float,\n",
    "        default=0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoder_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for encoder.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "\n",
    "    # Parse all arguments.\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service.\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Fix use_tpu.\n",
    "    arguments[\"use_tpu\"] = convert_string_to_bool(arguments[\"use_tpu\"])\n",
    "\n",
    "    # Fix eval steps.\n",
    "    arguments[\"eval_steps\"] = convert_string_to_none_or_int(\n",
    "        arguments[\"eval_steps\"])\n",
    "\n",
    "    # Fix eval_on_tpu.\n",
    "    arguments[\"eval_on_tpu\"] = convert_string_to_bool(\n",
    "        arguments[\"eval_on_tpu\"]\n",
    "    )\n",
    "\n",
    "    # Fix export_to_tpu.\n",
    "    arguments[\"export_to_tpu\"] = convert_string_to_bool(\n",
    "        arguments[\"export_to_tpu\"]\n",
    "    )\n",
    "    # Fix export_to_cpu.\n",
    "    arguments[\"export_to_cpu\"] = convert_string_to_bool(\n",
    "        arguments[\"export_to_cpu\"]\n",
    "    )\n",
    "\n",
    "    # Fix predict_all_resolutions.\n",
    "    arguments[\"predict_all_resolutions\"] = convert_string_to_bool(\n",
    "        arguments[\"predict_all_resolutions\"]\n",
    "    )\n",
    "\n",
    "    # Fix conv layer property parameters.\n",
    "    arguments[\"conv_num_filters\"] = convert_string_to_list_of_lists_of_ints(\n",
    "        string=arguments[\"conv_num_filters\"], outer_sep=\";\", inner_sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"conv_kernel_sizes\"] = convert_string_to_list_of_lists_of_ints(\n",
    "        string=arguments[\"conv_kernel_sizes\"], outer_sep=\";\", inner_sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"conv_strides\"] = convert_string_to_list_of_lists_of_ints(\n",
    "        string=arguments[\"conv_strides\"], outer_sep=\";\", inner_sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Make some assertions.\n",
    "    assert len(arguments[\"conv_num_filters\"]) > 0\n",
    "    assert len(arguments[\"conv_num_filters\"]) == len(arguments[\"conv_kernel_sizes\"])\n",
    "    assert len(arguments[\"conv_num_filters\"]) == len(arguments[\"conv_strides\"])\n",
    "\n",
    "    # Truncate lists if over the 1024x1024 current limit.\n",
    "    if len(arguments[\"conv_num_filters\"]) > 9:\n",
    "        arguments[\"conv_num_filters\"] = arguments[\"conv_num_filters\"][0:10]\n",
    "        arguments[\"conv_kernel_sizes\"] = arguments[\"conv_kernel_sizes\"][0:10]\n",
    "        arguments[\"conv_strides\"] = arguments[\"conv_strides\"][0:10]\n",
    "\n",
    "    # Get conv layer properties for generator and discriminator.\n",
    "    (generator,\n",
    "     discriminator) = calc_generator_discriminator_conv_layer_properties(\n",
    "        arguments[\"conv_num_filters\"],\n",
    "        arguments[\"conv_kernel_sizes\"],\n",
    "        arguments[\"conv_strides\"],\n",
    "        arguments[\"depth\"]\n",
    "    )\n",
    "\n",
    "    # Split up generator properties into separate lists.\n",
    "    (generator_base_conv_blocks,\n",
    "     generator_growth_conv_blocks,\n",
    "     generator_to_rgb_layers) = split_up_generator_conv_layer_properties(\n",
    "        generator,\n",
    "        arguments[\"conv_num_filters\"],\n",
    "        arguments[\"conv_strides\"],\n",
    "        arguments[\"depth\"]\n",
    "    )\n",
    "    arguments[\"generator_base_conv_blocks\"] = generator_base_conv_blocks\n",
    "    arguments[\"generator_growth_conv_blocks\"] = generator_growth_conv_blocks\n",
    "    arguments[\"generator_to_rgb_layers\"] = generator_to_rgb_layers\n",
    "\n",
    "    # Split up discriminator properties into separate lists.\n",
    "    (discriminator_from_rgb_layers,\n",
    "     discriminator_base_conv_blocks,\n",
    "     discriminator_growth_conv_blocks) = split_up_discriminator_conv_layer_properties(\n",
    "        discriminator,\n",
    "        arguments[\"conv_num_filters\"],\n",
    "        arguments[\"conv_strides\"],\n",
    "        arguments[\"depth\"]\n",
    "    )\n",
    "    arguments[\"discriminator_from_rgb_layers\"] = discriminator_from_rgb_layers\n",
    "    arguments[\"discriminator_base_conv_blocks\"] = discriminator_base_conv_blocks\n",
    "    arguments[\"discriminator_growth_conv_blocks\"] = discriminator_growth_conv_blocks\n",
    "\n",
    "    # For now just have encoder match architecture of discriminator.\n",
    "    arguments[\"encoder_from_rgb_layers\"] = discriminator_from_rgb_layers\n",
    "    arguments[\"encoder_base_conv_blocks\"] = discriminator_base_conv_blocks\n",
    "    arguments[\"encoder_growth_conv_blocks\"] = discriminator_growth_conv_blocks\n",
    "\n",
    "    # Fix normalize_latent.\n",
    "    arguments[\"normalize_latent\"] = convert_string_to_bool(\n",
    "        arguments[\"normalize_latent\"]\n",
    "    )\n",
    "\n",
    "    # Fix use_pixel_norm.\n",
    "    arguments[\"use_pixel_norm\"] = convert_string_to_bool(\n",
    "        arguments[\"use_pixel_norm\"]\n",
    "    )\n",
    "\n",
    "    # Fix generator_projection_dims.\n",
    "    arguments[\"generator_projection_dims\"] = convert_string_to_list_of_ints(\n",
    "        arguments[\"generator_projection_dims\"], \",\"\n",
    "    )\n",
    "\n",
    "    # Fix use_minibatch_stddev.\n",
    "    arguments[\"use_minibatch_stddev\"] = convert_string_to_bool(\n",
    "        arguments[\"use_minibatch_stddev\"]\n",
    "    )\n",
    "\n",
    "    # Fix clip_gradients.\n",
    "    arguments[\"generator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        arguments[\"generator_clip_gradients\"])\n",
    "\n",
    "    arguments[\"discriminator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        arguments[\"discriminator_clip_gradients\"])\n",
    "\n",
    "    arguments[\"encoder_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        arguments[\"encoder_clip_gradients\"])\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning.\n",
    "    # This code can be removed if you are not using hyperparameter tuning.\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\n",
    "                \"TF_CONFIG\", \"{}\"\n",
    "            )\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\"))\n",
    "\n",
    "    # Run the training job.\n",
    "    model.train_and_evaluate(arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
