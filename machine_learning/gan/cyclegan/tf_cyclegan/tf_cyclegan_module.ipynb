{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile README.md\n",
    "Implementation of [Unpaired Image-to-Image Translation\n",
    "using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/print_object.py\n",
    "def print_obj(function_name, object_name, object_value):\n",
    "    \"\"\"Prints enclosing function, object name, and object value.\n",
    "\n",
    "    Args:\n",
    "        function_name: str, name of function.\n",
    "        object_name: str, name of object.\n",
    "        object_value: object, value of passed object.\n",
    "    \"\"\"\n",
    "#     pass\n",
    "    print(\"{}: {} = {}\".format(function_name, object_name, object_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/image_utils.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def decode_image(image_bytes, params):\n",
    "    \"\"\"Decodes image bytes tensor.\n",
    "\n",
    "    Args:\n",
    "        image_bytes: tensor, image bytes with shape [?,].\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Decoded image tensor of shape [height, width, depth].\n",
    "    \"\"\"\n",
    "    func_name = \"decode_image\"\n",
    "    print_obj(\"\\n\" + func_name, \"image_bytes\", image_bytes)\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.decode_raw(\n",
    "        input_bytes=image_bytes,\n",
    "        out_type=tf.uint8,\n",
    "        name=\"image_decoded\"\n",
    "    )\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]],\n",
    "        name=\"image_reshaped\"\n",
    "    )\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, mode, params):\n",
    "    \"\"\"Preprocess image tensor.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, input image with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image tensor with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "    \"\"\"\n",
    "    func_name = \"preprocess_image\"\n",
    "    print_obj(\"\\n\" + func_name, \"image\", image)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # Add some random jitter.\n",
    "        if params[\"preprocess_image_resize_jitter_size\"]:\n",
    "            image = tf.image.resize(\n",
    "                images=image,\n",
    "                size=params[\"preprocess_image_resize_jitter_size\"],\n",
    "                method=\"bilinear\",\n",
    "                name=\"{}_jitter_resize\".format(func_name)\n",
    "            )\n",
    "            print_obj(func_name, \"image\", image)\n",
    "\n",
    "            image = tf.image.random_crop(\n",
    "                value=image,\n",
    "                size=[params[\"height\"], params[\"width\"], params[\"depth\"]],\n",
    "                name=\"{}_jitter_crop\".format(func_name)\n",
    "            )\n",
    "            print_obj(func_name, \"image\", image)\n",
    "\n",
    "        # Random mirroring.\n",
    "        if params[\"preprocess_image_use_random_mirroring\"]:\n",
    "            image = tf.image.random_flip_left_right(image=image)\n",
    "            print_obj(func_name, \"image\", image)\n",
    "\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.subtract(\n",
    "        x=tf.cast(x=image, dtype=tf.float32) * (2. / 255),\n",
    "        y=1.0,\n",
    "        name=\"{}_scaled\".format(func_name)\n",
    "    )\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def handle_input_image(image_bytes, mode, params):\n",
    "    \"\"\"Handles image tensor transformations.\n",
    "\n",
    "    Args:\n",
    "        image_bytes: tensor, image bytes with shape [?,].\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN or EVAL.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image tensor with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "    \"\"\"\n",
    "    func_name = \"handle_input_image\"\n",
    "    print_obj(\"\\n\" + func_name, \"image_bytes\", image_bytes)\n",
    "\n",
    "    # Decode image.\n",
    "    image = decode_image(image_bytes=image_bytes, params=params)\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    # Preprocess image.\n",
    "    image = preprocess_image(image=image, mode=mode, params=params)\n",
    "    print_obj(func_name, \"image\", image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_fake_images(fake_images, params):\n",
    "    \"\"\"Resizes fake images to match real image sizes.\n",
    "\n",
    "    Args:\n",
    "        fake_images: tensor, fake images from generator.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Resized image tensor.\n",
    "    \"\"\"\n",
    "    func_name = \"resize_fake_images\"\n",
    "    print_obj(\"\\n\" + func_name, \"fake_images\", fake_images)\n",
    "\n",
    "    # Resize fake images to match real image sizes.\n",
    "    resized_fake_images = tf.image.resize(\n",
    "        images=fake_images,\n",
    "        size=[params[\"height\"], params[\"width\"]],\n",
    "        method=\"nearest\",\n",
    "        name=\"resized_fake_images\"\n",
    "    )\n",
    "    print_obj(func_name, \"resized_fake_images\", resized_fake_images)\n",
    "\n",
    "    return resized_fake_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/input.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def decode_example(protos, mode, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN or EVAL.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image tensors from domain a and 2.\n",
    "    \"\"\"\n",
    "    func_name = \"decode_example\"\n",
    "\n",
    "    # Create feature schema map for protos.\n",
    "    feature_schema = {\n",
    "        \"domain_a_image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"domain_b_image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.parse_single_example(\n",
    "        serialized=protos, features=feature_schema\n",
    "    )\n",
    "    print_obj(\"\\n\" + func_name, \"parsed_features\", parsed_features)\n",
    "\n",
    "    # Decode source image.\n",
    "    domain_a_image = image_utils.handle_input_image(\n",
    "        image_bytes=parsed_features[\"domain_a_image_raw\"],\n",
    "        mode=mode,\n",
    "        params=params\n",
    "    )\n",
    "    print_obj(func_name, \"domain_a_image\", domain_a_image)\n",
    "\n",
    "    # Decode target image.\n",
    "    domain_b_image = image_utils.handle_input_image(\n",
    "        image_bytes=parsed_features[\"domain_b_image_raw\"],\n",
    "        mode=mode,\n",
    "        params=params\n",
    "    )\n",
    "    print_obj(func_name, \"domain_b_image\", domain_b_image)\n",
    "\n",
    "    return {\"domain_a_image\": domain_a_image, \"domain_b_image\": domain_b_image}\n",
    "\n",
    "\n",
    "def set_static_shape(features, batch_size, params):\n",
    "    \"\"\"Sets static shape of batched input tensors in dataset.\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params:\n",
    "    Returns:\n",
    "        Features tensor dictionary and labels tensor.\n",
    "    \"\"\"\n",
    "    features[\"domain_a_image\"].set_shape(\n",
    "        features[\"domain_a_image\"].get_shape().merge_with(\n",
    "            tf.TensorShape(dims=[batch_size, None, None, None])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    features[\"domain_b_image\"].set_shape(\n",
    "        features[\"domain_b_image\"].get_shape().merge_with(\n",
    "            tf.TensorShape(dims=[batch_size, None, None, None])\n",
    "        )\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    \"\"\"Read data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read dataset\n",
    "    using Dataset API, apply necessary preprocessing, and return an input\n",
    "    function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern to read into our tf.data dataset.\n",
    "        mode: The estimator ModeKeys. Can be TRAIN or EVAL.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create list of files that match pattern.\n",
    "        file_list = tf.gfile.Glob(filename=filename)\n",
    "\n",
    "        # Create dataset from file list.\n",
    "        if params[\"input_fn_autotune\"]:\n",
    "            dataset = tf.data.TFRecordDataset(\n",
    "                filenames=file_list,\n",
    "                num_parallel_reads=tf.contrib.data.AUTOTUNE\n",
    "            )\n",
    "        else:\n",
    "            dataset = tf.data.TFRecordDataset(filenames=file_list)\n",
    "\n",
    "        # Shuffle and repeat if training with fused op.\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(\n",
    "                    buffer_size=50 * batch_size,\n",
    "                    count=None  # indefinitely\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decode file into a features dictionary of tensors, then batch.\n",
    "        if params[\"input_fn_autotune\"]:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.map_and_batch(\n",
    "                    map_func=lambda x: decode_example(\n",
    "                        protos=x,\n",
    "                        mode=mode,\n",
    "                        params=params\n",
    "                    ),\n",
    "                    batch_size=batch_size,\n",
    "                    num_parallel_calls=tf.contrib.data.AUTOTUNE\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.map_and_batch(\n",
    "                    map_func=lambda x: decode_example(\n",
    "                        protos=x,\n",
    "                        mode=mode,\n",
    "                        params=params\n",
    "                    ),\n",
    "                    batch_size=batch_size,\n",
    "                    drop_remainder=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Assign static shape, namely make the batch size axis static.\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x: set_static_shape(\n",
    "                features=x, batch_size=batch_size, params=params\n",
    "            )\n",
    "        )\n",
    "        # Prefetch data to improve latency.\n",
    "        if params[\"input_fn_autotune\"]:\n",
    "            dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\n",
    "        else:\n",
    "            dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "        # Create a iterator, then get batch of features from example queue.\n",
    "        batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return batched_dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/padding.py\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class CustomPadding2D(tf.layers.Layer):\n",
    "    \"\"\"Custom layer for 2D padding.\n",
    "\n",
    "    Fields:\n",
    "        paddings: tensor, rank 2 tensor that stores paddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_inputs, kernel_size, strides, padding, **kwargs):\n",
    "        \"\"\"Instantiates custom 2D padding layer.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, inputs to previous conv layer.\n",
    "            kernel_size: int, list, or tuple, if int then the kernel size for\n",
    "                both height and width.\n",
    "            strides: int, list, or tuple, if int then the stride for both\n",
    "                height and width.\n",
    "            padding: str, either same or valid padding.\n",
    "        \"\"\"\n",
    "        # Get kernel sizes.\n",
    "        if isinstance(kernel_size, (list, tuple)):\n",
    "            kernel_size = kernel_size\n",
    "        elif isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "        # Get strides.\n",
    "        if isinstance(strides, (list, tuple)):\n",
    "            strides = strides\n",
    "        elif isinstance(strides, int):\n",
    "            strides = (strides, strides)\n",
    "\n",
    "        # Get padding type.\n",
    "        padding = padding.lower()\n",
    "        assert padding in {\"valid\", \"same\"}\n",
    "\n",
    "        # Create paddings.\n",
    "        self.paddings = self._create_paddings(\n",
    "            conv_inputs, kernel_size, strides, padding\n",
    "        )\n",
    "\n",
    "        # Pass anything else to base `Layer` class.\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _create_paddings(self, conv_inputs, kernel_size, strides, padding):\n",
    "        \"\"\"Creates rank 2 paddings tensor.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, inputs to previous conv layer.\n",
    "            kernel_size: int, list, or tuple, if int then the kernel size for\n",
    "                both height and width.\n",
    "            strides: int, list, or tuple, if int then the stride for both\n",
    "                height and width.\n",
    "            padding: str, either same or valid padding.\n",
    "        \"\"\"\n",
    "        # Get input shape.\n",
    "        input_shape = [x.value for x in conv_inputs.shape]\n",
    "        print_obj(\"calculate_padding\", \"input_shape\", input_shape)\n",
    "        input_h, input_w = input_shape[1:3]\n",
    "\n",
    "        # Expand kernel size into height and width.\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "\n",
    "        # Expand strides into height and width.\n",
    "        stride_h, stride_w = strides\n",
    "\n",
    "        # Only pad if padding is same, otherwise don't if padding is valid.\n",
    "        if padding == \"same\":\n",
    "            # Calculate output shape.\n",
    "            output_h = int(math.ceil(float(input_h) / float(stride_h)))\n",
    "            output_w = int(math.ceil(float(input_w) / float(stride_w)))\n",
    "\n",
    "            # Find out how much is missing in each dimension.\n",
    "            pad_along_height = max(\n",
    "                (output_h - 1) * stride_h + kernel_h - input_h, 0\n",
    "            )\n",
    "            pad_along_width = max(\n",
    "                (output_w - 1) * stride_w + kernel_w - input_w, 0\n",
    "            )\n",
    "\n",
    "            # Preference to add any extra to bottom and right.\n",
    "            pad_top = pad_along_height // 2\n",
    "            pad_bottom = pad_along_height - pad_top\n",
    "            pad_left = pad_along_width // 2\n",
    "            pad_right = pad_along_width - pad_left\n",
    "        elif padding == \"valid\":\n",
    "            pad_top = 0\n",
    "            pad_bottom = 0\n",
    "            pad_left = 0\n",
    "            pad_right = 0\n",
    "\n",
    "        # Create paddings.\n",
    "        return [\n",
    "            [0, 0],\n",
    "            [pad_top, pad_bottom],\n",
    "            [pad_left, pad_right],\n",
    "            [0, 0]\n",
    "        ]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Returns padded output shape given input shape.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, 4-tuple that cotains dimension sizes for rank\n",
    "                4 input shape.\n",
    "        Returns:\n",
    "            4-tuple of computed padded output shape.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            input_shape[0],\n",
    "            self.paddings[1][0] + input_shape[1] + self.paddings[1][1],\n",
    "            self.paddings[2][0] + input_shape[2] + self.paddings[2][1],\n",
    "            input_shape[3]\n",
    "        )\n",
    "\n",
    "\n",
    "class ConstantPadding2D(CustomPadding2D):\n",
    "    \"\"\"Custom layer for 2D constant padding.\n",
    "\n",
    "    Fields:\n",
    "        constant: float, constant value to pad tensor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_inputs,\n",
    "        kernel_size,\n",
    "        strides,\n",
    "        padding,\n",
    "        constant=0.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Instantiates replication 2D padding layer.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, inputs to previous conv layer.\n",
    "            kernel_size: int, list, or tuple, if int then the kernel size for\n",
    "                both height and width.\n",
    "            strides: int, list, or tuple, if int then the stride for both\n",
    "                height and width.\n",
    "            padding: str, either same or valid padding.\n",
    "            constant: float, constant value to pad tensor.\n",
    "        \"\"\"\n",
    "        self.constant = constant\n",
    "        super().__init__(conv_inputs, kernel_size, strides, padding, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Returns constant padded input tensor.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, outputs of previous conv layer with no\n",
    "                padding.\n",
    "        Returns:\n",
    "            Constant padded input tensor.\n",
    "        \"\"\"\n",
    "        return tf.pad(\n",
    "            tensor=inputs,\n",
    "            paddings=self.paddings,\n",
    "            mode=\"CONSTANT\",\n",
    "            constant_values=self.constant\n",
    "        )\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(CustomPadding2D):\n",
    "    \"\"\"Custom layer for 2D reflection padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_inputs, kernel_size, strides, padding, **kwargs):\n",
    "        \"\"\"Instantiates replication 2D padding layer.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, inputs to previous conv layer.\n",
    "            kernel_size: int, list, or tuple, if int then the kernel size for\n",
    "                both height and width.\n",
    "            strides: int, list, or tuple, if int then the stride for both\n",
    "                height and width.\n",
    "            padding: str, either same or valid padding.\n",
    "        \"\"\"\n",
    "        super().__init__(conv_inputs, kernel_size, strides, padding, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Returns reflection padded input tensor.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, outputs of previous conv layer with no\n",
    "                padding.\n",
    "        Returns:\n",
    "            Reflection padded input tensor.\n",
    "        \"\"\"\n",
    "        return tf.pad(tensor=inputs, paddings=self.paddings, mode=\"REFLECT\")\n",
    "\n",
    "\n",
    "class ReplicationPadding2D(CustomPadding2D):\n",
    "    \"\"\"Custom layer for 2D replication padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_inputs, kernel_size, strides, padding, **kwargs):\n",
    "        \"\"\"Instantiates replication 2D padding layer.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, inputs to previous conv layer.\n",
    "            kernel_size: int, list, or tuple, if int then the kernel size for\n",
    "                both height and width.\n",
    "            strides: int, list, or tuple, if int then the stride for both\n",
    "                height and width.\n",
    "            padding: str, either same or valid padding.\n",
    "        \"\"\"\n",
    "        super().__init__(conv_inputs, kernel_size, strides, padding, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Returns replication padded input tensor.\n",
    "\n",
    "        Args:\n",
    "            conv_inputs: tensor, outputs of previous conv layer with no\n",
    "                padding.\n",
    "        Returns:\n",
    "            Replication padded input tensor.\n",
    "        \"\"\"\n",
    "        return tf.pad(tensor=inputs, paddings=self.paddings, mode=\"SYMMETRIC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance_normalization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/instance_normalization.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class GroupNormalization(tf.layers.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        groups=2,\n",
    "        axis=-1,\n",
    "        epsilon=1e-3,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        beta_initializer=\"zeros\",\n",
    "        gamma_initializer=\"ones\",\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(\n",
    "            inputs, input_shape, tensor_input_shape\n",
    "        )\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(\n",
    "            reshaped_inputs, input_shape\n",
    "        )\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"groups\": self.groups,\n",
    "            \"axis\": self.axis,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"center\": self.center,\n",
    "            \"scale\": self.scale,\n",
    "            \"beta_initializer\": tf.keras.initializers.serialize(\n",
    "                self.beta_initializer\n",
    "            ),\n",
    "            \"gamma_initializer\": tf.keras.initializers.serialize(\n",
    "                self.gamma_initializer\n",
    "            ),\n",
    "            \"beta_regularizer\": tf.keras.regularizers.serialize(\n",
    "                self.beta_regularizer\n",
    "            ),\n",
    "            \"gamma_regularizer\": tf.keras.regularizers.serialize(\n",
    "                self.gamma_regularizer\n",
    "            ),\n",
    "            \"beta_constraint\": tf.keras.constraints.serialize(\n",
    "                self.beta_constraint\n",
    "            ),\n",
    "            \"gamma_constraint\": tf.keras.constraints.serialize(\n",
    "                self.gamma_constraint\n",
    "            ),\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(\n",
    "            reshaped_inputs, group_reduction_axes, keepdims=True\n",
    "        )\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon,\n",
    "        )\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError(\n",
    "                \"Axis \" + str(self.axis) + \" of \"\n",
    "                \"input tensor should have a defined dimension \"\n",
    "                \"but the layer received an input with shape \" + str(input_shape) + \".\"\n",
    "            )\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n",
    "                \"more than the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") must be a \"\n",
    "                \"multiple of the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\"\n",
    "            )\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(\n",
    "            ndim=len(input_shape), axes={self.axis: dim}\n",
    "        )\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"gamma\",\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"beta\",\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape\n",
    "\n",
    "\n",
    "class InstanceNormalization(GroupNormalization):\n",
    "    \"\"\"Instance normalization layer.\n",
    "    Instance Normalization is an specific case of ```GroupNormalization```since\n",
    "    it normalizes all features of one channel. The Groupsize is equal to the\n",
    "    channel size. Empirically, its accuracy is more stable than batch norm in a\n",
    "    wide range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Arguments\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization]\n",
    "        (https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        if \"groups\" in kwargs:\n",
    "            logging.warning(\"The given value for groups will be overwritten.\")\n",
    "\n",
    "        kwargs[\"groups\"] = -1\n",
    "        super().__init__(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## networks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/networks.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import instance_normalization\n",
    "from . import padding\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Networks(object):\n",
    "    \"\"\"Network base class for generators and discriminators.\n",
    "\n",
    "    Fields:\n",
    "        name: str, name of network.\n",
    "        kernel_regularizer: `l1_l2_regularizer` object, regularizar for kernel\n",
    "            variables.\n",
    "        bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "            variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, name):\n",
    "        \"\"\"Instantiates network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of network.\n",
    "        \"\"\"\n",
    "        # Set name of network.\n",
    "        self.name = name\n",
    "\n",
    "        # Regularizer for kernel weights.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        # Regularizer for bias weights.\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "    def pre_activation_network(\n",
    "            self, network, params, scope, downscale, block_idx, layer_idx):\n",
    "        \"\"\"Creates pre-activation network graph.\n",
    "\n",
    "        Args:\n",
    "            network: tensor, rank 4 image tensor from previous block.\n",
    "            params: dict, user passed parameters.\n",
    "            scope: str, scope name of network.\n",
    "            downscale: bool, whether using Conv2D or Conv2DTranspose layers.\n",
    "            block_idx: int, current layer block index.\n",
    "            layer_idx: int, current layer index within layer block.\n",
    "\n",
    "        Returns:\n",
    "            Final rank 4 image tensor of current pre-activation block.\n",
    "        \"\"\"\n",
    "        func_name = \"pre_activation_network_{}_{}\".format(\n",
    "            block_idx, layer_idx\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"network\", network)\n",
    "\n",
    "        scope_split = scope.split(\"/\")\n",
    "\n",
    "        if scope_split[0] == \"generator\":\n",
    "            network_name = scope_split[0] + \"_\" + \"_\".join(scope_split[2:])\n",
    "        else:\n",
    "            network_name = scope_split[0] + \"_\".join(scope_split[2:])\n",
    "\n",
    "        # Create kernel weight initializer.\n",
    "        kernel_initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.02\n",
    "        )\n",
    "\n",
    "        filters = params[\"{}_num_filters\".format(network_name)][layer_idx]\n",
    "        kernel_size = params[\"{}_kernel_sizes\".format(network_name)][layer_idx]\n",
    "        strides = params[\"{}_strides\".format(network_name)][layer_idx]\n",
    "\n",
    "        if params[\"{}_downsample\".format(network_name)][layer_idx]:\n",
    "            # Create Conv2D layer with no padding.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     kernel_sizes[i - 1] / strides[i],\n",
    "            #     kernel_sizes[i - 1] / strides[i],\n",
    "            #     num_filters[i]\n",
    "            # )\n",
    "            conv_outputs = tf.layers.conv2d(\n",
    "                inputs=network,\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=\"valid\",\n",
    "                activation=None,\n",
    "                kernel_initializer=kernel_initializer,\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"layers_conv2d_{}_{}\".format(block_idx, layer_idx)\n",
    "            )\n",
    "\n",
    "            # Now add padding.\n",
    "            padding_type = params[\"{}_pad_type\".format(network_name)].lower()\n",
    "            if padding_type == \"constant\":\n",
    "                network = padding.ConstantPadding2D(\n",
    "                    conv_inputs=network,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=strides,\n",
    "                    padding=\"same\",\n",
    "                    constant=params[\"{}_pad_constant\".format(network_name)]\n",
    "                )(inputs=conv_outputs)\n",
    "            elif padding_type == \"reflection\":\n",
    "                network = padding.ReflectionPadding2D(\n",
    "                    conv_inputs=network,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=strides,\n",
    "                    padding=\"same\"\n",
    "                )(inputs=conv_outputs)\n",
    "            elif padding_type == \"replication\":\n",
    "                network = padding.ReplicationPadding2D(\n",
    "                    conv_inputs=network,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=strides,\n",
    "                    padding=\"same\"\n",
    "                )(inputs=conv_outputs)\n",
    "            else:\n",
    "                network = padding.ConstantPadding2D(\n",
    "                    conv_inputs=network,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=strides,\n",
    "                    padding=\"same\",\n",
    "                    constant=0.0\n",
    "                )(inputs=conv_outputs)\n",
    "            print_obj(func_name, \"network\", network)\n",
    "        else:\n",
    "            # Create Conv2DTranspose layer with no padding.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     kernel_sizes[i - 1] * strides[i],\n",
    "            #     kernel_sizes[i - 1] * strides[i],\n",
    "            #     num_filters[i]\n",
    "            # )\n",
    "            network = tf.layers.conv2d_transpose(\n",
    "                inputs=network,\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_initializer=kernel_initializer,\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                name=\"layers_conv2d_transpose_{}_{}\".format(\n",
    "                    block_idx, layer_idx\n",
    "                )\n",
    "            )\n",
    "        print_obj(func_name, \"network\", network)\n",
    "\n",
    "        dropout_rates = params[\"{}_dropout_rates\".format(network_name)]\n",
    "        dropout_rate = dropout_rates[layer_idx]\n",
    "        if dropout_rate:\n",
    "            # Maybe add some dropout.\n",
    "            if params[\"{}_dropout_before_act\".format(network_name)]:\n",
    "                if params[\"{}_dropout_before_norm\".format(network_name)]:\n",
    "                    network = tf.layers.dropout(\n",
    "                        inputs=network,\n",
    "                        rate=dropout_rate,\n",
    "                        training=True,\n",
    "                        name=\"layers_dropout_{}_{}\".format(\n",
    "                            block_idx, layer_idx\n",
    "                        )\n",
    "                    )\n",
    "                    print_obj(func_name, \"network\", network)\n",
    "\n",
    "        if params[\"{}_layer_norm_before_act\".format(network_name)]:\n",
    "            layer_norms = params[\"{}_layer_norm_type\".format(network_name)]\n",
    "            layer_norm = layer_norms[layer_idx]\n",
    "            # Add layer normalization to keep inputs from blowing up.\n",
    "            if layer_norm == \"batch\":\n",
    "                network = tf.layers.batch_normalization(\n",
    "                    inputs=network,\n",
    "                    training=True,\n",
    "                    name=\"layers_batch_norm_{}_{}\".format(\n",
    "                        block_idx, layer_idx\n",
    "                    )\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "            elif layer_norm == \"instance\":\n",
    "                network = instance_normalization.InstanceNormalization(\n",
    "                    axis=-1,\n",
    "                    center=False,\n",
    "                    scale=False,\n",
    "                    name=\"layers_instance_norm_{}_{}\".format(\n",
    "                        block_idx, layer_idx\n",
    "                    )\n",
    "                )(inputs=network)\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        if dropout_rate:\n",
    "            # Maybe add some dropout.\n",
    "            if params[\"{}_dropout_before_act\".format(network_name)]:\n",
    "                if not params[\"{}_dropout_before_norm\".format(network_name)]:\n",
    "                    network = tf.layers.dropout(\n",
    "                        inputs=network,\n",
    "                        rate=dropout_rate,\n",
    "                        training=True,\n",
    "                        name=\"layers_dropout_{}_{}\".format(\n",
    "                            block_idx, layer_idx\n",
    "                        )\n",
    "                    )\n",
    "                    print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def apply_activation(\n",
    "            self,\n",
    "            input_tensor,\n",
    "            activation_name,\n",
    "            activation_set,\n",
    "            params,\n",
    "            scope,\n",
    "            block_idx,\n",
    "            layer_idx):\n",
    "        \"\"\"Applies activation to input tensor.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: tensor, input to activation function.\n",
    "            activation_name: str, name of activation function to apply.\n",
    "            activation_set: set, allowable set of activation functions.\n",
    "            params: dict, user passed parameters.\n",
    "            scope: str, current scope of network.\n",
    "            block_idx: int, current layer block index.\n",
    "            layer_idx: int, current layer index within layer block.\n",
    "\n",
    "        Returns:\n",
    "            Activation tensor of same shape as input_tensor.\n",
    "        \"\"\"\n",
    "        func_name = \"apply_activation_{}_{}\".format(block_idx, layer_idx)\n",
    "        print_obj(\"\\n\" + func_name, \"input_tensor\", input_tensor)\n",
    "\n",
    "        scope_split = scope.split(\"/\")\n",
    "\n",
    "        if scope_split[0] == \"generator\":\n",
    "            network_name = scope_split[0] + \"_\" + \"_\".join(scope_split[2:])\n",
    "        else:\n",
    "            network_name = scope_split[0] + \"_\".join(scope_split[2:])\n",
    "\n",
    "        # Lowercase activation name.\n",
    "        activation_name = activation_name.lower()\n",
    "\n",
    "        assert activation_name in activation_set\n",
    "        if activation_name == \"relu\":\n",
    "            activation = tf.nn.relu(\n",
    "                features=input_tensor,\n",
    "                name=\"relu_{}_{}\".format(block_idx, layer_idx)\n",
    "            )\n",
    "        elif activation_name == \"leaky_relu\":\n",
    "            activation = tf.nn.leaky_relu(\n",
    "                features=input_tensor,\n",
    "                alpha=params[\"{}_leaky_relu_alpha\".format(network_name)],\n",
    "                name=\"leaky_relu_{}_{}\".format(block_idx, layer_idx)\n",
    "            )\n",
    "        elif activation_name == \"tanh\":\n",
    "            activation = tf.math.tanh(\n",
    "                x=input_tensor,\n",
    "                name=\"tanh_{}_{}\".format(block_idx, layer_idx)\n",
    "            )\n",
    "        else:\n",
    "            activation = input_tensor\n",
    "        print_obj(func_name, \"activation\", activation)\n",
    "\n",
    "        return activation\n",
    "\n",
    "    def post_activation_network(\n",
    "            self, network, params, scope, block_idx, layer_idx):\n",
    "        \"\"\"Creates post-activation network graph.\n",
    "\n",
    "        Args:\n",
    "            network: tensor, rank 4 image tensor from previous block.\n",
    "            params: dict, user passed parameters.\n",
    "            scope: str, scope name of network.\n",
    "            block_idx: int, current layer block index.\n",
    "            layer_idx: int, current layer index within layer block.\n",
    "\n",
    "        Returns:\n",
    "            Final rank 4 image tensor of current post-activation block.\n",
    "        \"\"\"\n",
    "        func_name = \"post_activation_network_{}_{}\".format(\n",
    "            block_idx, layer_idx\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"network\", network)\n",
    "\n",
    "        scope_split = scope.split(\"/\")\n",
    "\n",
    "        if scope_split[0] == \"generator\":\n",
    "            network_name = scope_split[0] + \"_\" + \"_\".join(scope_split[2:])\n",
    "        else:\n",
    "            network_name = scope_split[0] + \"_\".join(scope_split[2:])\n",
    "\n",
    "        dropout_rates = params[\"{}_dropout_rates\".format(network_name)]\n",
    "        dropout_rate = dropout_rates[layer_idx]\n",
    "        if dropout_rate:\n",
    "            # Maybe add some dropout.\n",
    "            if not params[\"{}_dropout_before_act\".format(network_name)]:\n",
    "                if params[\"{}_dropout_before_norm\".format(network_name)]:\n",
    "                    network = tf.layers.dropout(\n",
    "                        inputs=network,\n",
    "                        rate=dropout_rate,\n",
    "                        training=True,\n",
    "                        name=\"layers_dropout_{}_{}\".format(\n",
    "                            block_idx, layer_idx\n",
    "                        )\n",
    "                    )\n",
    "                    print_obj(func_name, \"network\", network)\n",
    "\n",
    "        if not params[\"{}_layer_norm_before_act\".format(network_name)]:\n",
    "            layer_norms = params[\"{}_layer_norm_type\".format(network_name)]\n",
    "            layer_norm = layer_norms[layer_idx]\n",
    "            # Add layer normalization to keep inputs from blowing up.\n",
    "            if layer_norm == \"batch\":\n",
    "                network = tf.layers.batch_normalization(\n",
    "                    inputs=network,\n",
    "                    training=True,\n",
    "                    name=\"layers_batch_norm_{}_{}\".format(\n",
    "                        block_idx, layer_idx\n",
    "                    )\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "            elif layer_norm == \"instance\":\n",
    "                network = InstanceNormalization(\n",
    "                    axis=-1,\n",
    "                    center=False,\n",
    "                    scale=False,\n",
    "                    name=\"layers_instance_norm_{}_{}\".format(\n",
    "                        block_idx, layer_idx\n",
    "                    )\n",
    "                )(inputs=network)\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        if dropout_rate:\n",
    "            # Maybe add some dropout.\n",
    "            if not params[\"{}_dropout_before_act\".format(network_name)]:\n",
    "                if not params[\"{}_dropout_before_norm\".format(network_name)]:\n",
    "                    network = tf.layers.dropout(\n",
    "                        inputs=network,\n",
    "                        rate=dropout_rate,\n",
    "                        training=True,\n",
    "                        name=\"layers_dropout_{}\".format(\n",
    "                            block_idx, layer_idx\n",
    "                        )\n",
    "                    )\n",
    "                    print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/generator.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from . import networks\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Generator(networks.Networks):\n",
    "    \"\"\"Generator that takes source image input and outputs target image.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, name):\n",
    "        \"\"\"Instantiates generator network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of generator.\n",
    "        \"\"\"\n",
    "        # Initialize base network class.\n",
    "        super().__init__(kernel_regularizer, bias_regularizer, name)\n",
    "\n",
    "    def unet_encoder(self, source_images, params):\n",
    "        \"\"\"Creates generator's U-net encoder network.\n",
    "\n",
    "        Args:\n",
    "            source_images: tensor, source images of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Bottlenecked image tensor of shape\n",
    "                [cur_batch_size, 1, 1, generator_encoder_num_filters[-1]] and\n",
    "                reversed list of encoder activation tensors for use in U-net\n",
    "                decoder.\n",
    "        \"\"\"\n",
    "        func_name = \"unet_encoder\"\n",
    "        print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "        # Create list for encoder activations if using optional U-net decoder.\n",
    "        encoder_activations = []\n",
    "\n",
    "        # The set of allowed activations.\n",
    "        activation_set = {\"relu\", \"leaky_relu\", \"tanh\", \"none\"}\n",
    "\n",
    "        # Define scope.\n",
    "        scope = \"{}/unet/encoder\".format(self.name)\n",
    "\n",
    "        # Create input layer to encoder.\n",
    "        network = source_images\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            # Iteratively build downsampling layers.\n",
    "            for i in range(len(params[\"generator_unet_encoder_num_filters\"])):\n",
    "                # Create pre-activation network graph.\n",
    "                network = self.pre_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    downscale=True,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Apply activation.\n",
    "                activation = params[\"generator_unet_encoder_activation\"][i]\n",
    "                network = self.apply_activation(\n",
    "                    input_tensor=network,\n",
    "                    activation_name=activation,\n",
    "                    activation_set=activation_set,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Create post-activation network graph.\n",
    "                network = self.post_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Add encoder activations to list if using U-net decoder.\n",
    "                encoder_activations.append(network)\n",
    "\n",
    "            # If encoder activation list is not empty.\n",
    "            if encoder_activations:\n",
    "                # Drop final layer since it is the bottleneck.\n",
    "                encoder_activations = encoder_activations[:-1]\n",
    "\n",
    "                # Reverse order to match decoder image sizes.\n",
    "                encoder_activations = encoder_activations[::-1]\n",
    "\n",
    "                # Add None to end of list so concatenation in decoder doesn't\n",
    "                # occur for last layer.\n",
    "                encoder_activations += [None]\n",
    "\n",
    "        return network, encoder_activations\n",
    "\n",
    "    def unet_decoder(self, bottleneck, encoder_activations, params):\n",
    "        \"\"\"Creates generator's U-net decoder network.\n",
    "\n",
    "        Args:\n",
    "            bottleneck: tensor, bottleneck of shape\n",
    "                [cur_batch_size, 1, 1, generator_encoder_num_filters[-1]].\n",
    "            encoder_activations: list, reversed list of encoder activation\n",
    "                tensors for use U-net decoder.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated target images tensor of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"unet_decoder\"\n",
    "        print_obj(\"\\n\" + func_name, \"bottleneck\", bottleneck)\n",
    "        print_obj(func_name, \"encoder_activations\", encoder_activations)\n",
    "\n",
    "        # The set of allowed activations.\n",
    "        activation_set = {\"relu\", \"leaky_relu\", \"tanh\", \"none\"}\n",
    "\n",
    "        # Define scope.\n",
    "        scope = \"{}/unet/decoder\".format(self.name)\n",
    "\n",
    "        # Create input layer to decoder.\n",
    "        network = bottleneck\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            # Iteratively build upsampling layers.\n",
    "            for i in range(len(params[\"generator_unet_decoder_num_filters\"])):\n",
    "                # Create pre-activation network graph.\n",
    "                network = self.pre_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    downscale=False,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Concatenate network layer with ith encoder activations along\n",
    "                # channels.\n",
    "                if tf.is_tensor(x=encoder_activations[i]):\n",
    "                    network = tf.concat(\n",
    "                        values=[network, encoder_activations[i]],\n",
    "                        axis=-1,\n",
    "                        name=\"unet_concat_{}\".format(i)\n",
    "                    )\n",
    "                    print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Apply activation.\n",
    "                activation = params[\"generator_unet_decoder_activation\"][i]\n",
    "                network = self.apply_activation(\n",
    "                    input_tensor=network,\n",
    "                    activation_name=activation,\n",
    "                    activation_set=activation_set,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Create post-activation network graph.\n",
    "                network = self.post_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def unet_generator(self, source_images, params):\n",
    "        \"\"\"Creates U-net generator network and returns generated images.\n",
    "\n",
    "        Args:\n",
    "            source_images: tensor, source images of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated target images tensor of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"unet_generator\"\n",
    "        print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "        # Encode image into bottleneck.\n",
    "        bottleneck, encoder_activations = self.unet_encoder(\n",
    "            source_images=source_images, params=params\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"bottleneck\", bottleneck)\n",
    "        print_obj(func_name, \"encoder_activations\", encoder_activations)\n",
    "\n",
    "        # Decode bottleneck back into image.\n",
    "        fake_target_images = self.unet_decoder(\n",
    "            bottleneck=bottleneck,\n",
    "            encoder_activations=encoder_activations,\n",
    "            params=params\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "        return fake_target_images\n",
    "\n",
    "    def resnet_encoder(self, source_images, params):\n",
    "        \"\"\"Creates generator's resnet encoder network.\n",
    "\n",
    "        Args:\n",
    "            source_images: tensor, source images of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Bottlenecked image tensor of shape\n",
    "                [cur_batch_size, 1, 1, generator_encoder_num_filters[-1]].\n",
    "        \"\"\"\n",
    "        func_name = \"resnet_encoder\"\n",
    "        print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "        # The set of allowed activations.\n",
    "        activation_set = {\"relu\", \"leaky_relu\", \"tanh\", \"none\"}\n",
    "\n",
    "        # Define scope.\n",
    "        scope = \"{}/resnet/enc\".format(self.name)\n",
    "\n",
    "        # Create input layer to encoder.\n",
    "        network = source_images\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            # Iteratively build downsampling layers.\n",
    "            for i in range(len(params[\"generator_resnet_enc_num_filters\"])):\n",
    "                # Create pre-activation network graph.\n",
    "                network = self.pre_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    downscale=True,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Apply activation.\n",
    "                activation = params[\"generator_resnet_enc_activation\"][i]\n",
    "                network = self.apply_activation(\n",
    "                    input_tensor=network,\n",
    "                    activation_name=activation,\n",
    "                    activation_set=activation_set,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Create post-activation network graph.\n",
    "                network = self.post_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def resnet_res_block(self, inputs, params, block_idx):\n",
    "        \"\"\"Creates resnet residual block for resnet generator network.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor, input tensor to resnet residual block.\n",
    "            params: dict, user passed parameters.\n",
    "            block_idx: int, index of the current residual block.\n",
    "\n",
    "        Returns:\n",
    "            Output image tensor of resnet block.\n",
    "        \"\"\"\n",
    "        func_name = \"resnet_res_block_{}\".format(block_idx)\n",
    "        print_obj(\"\\n\" + func_name, \"inputs\", inputs)\n",
    "\n",
    "        # The set of allowed activations.\n",
    "        activation_set = {\"relu\", \"leaky_relu\", \"tanh\", \"none\"}\n",
    "\n",
    "        # Define scope.\n",
    "        scope = \"{}/resnet/res\".format(self.name)\n",
    "\n",
    "        # Create input layer to residual block.\n",
    "        network = inputs\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            # Iteratively build upsampling layers.\n",
    "            for i in range(len(params[\"generator_resnet_res_num_filters\"])):\n",
    "                # Create pre-activation network graph.\n",
    "                network = self.pre_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    downscale=True,\n",
    "                    block_idx=block_idx,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Apply activation.\n",
    "                activation = params[\"generator_resnet_res_activation\"][i]\n",
    "                network = self.apply_activation(\n",
    "                    input_tensor=network,\n",
    "                    activation_name=activation,\n",
    "                    activation_set=activation_set,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=block_idx,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Create post-activation network graph.\n",
    "                network = self.post_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=block_idx,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "            # Concatenate inputs with last Conv2D layer output along channels.\n",
    "            network = tf.concat(values=[inputs, network], axis=-1)\n",
    "            print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def resnet_decoder(self, bottleneck, params):\n",
    "        \"\"\"Creates generator's resnet decoder network.\n",
    "\n",
    "        Args:\n",
    "            bottleneck: tensor, bottleneck of shape\n",
    "                [cur_batch_size, 1, 1, generator_encoder_num_filters[-1]].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated target images tensor of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"resnet_decoder\"\n",
    "        print_obj(\"\\n\" + func_name, \"bottleneck\", bottleneck)\n",
    "\n",
    "        # The set of allowed activations.\n",
    "        activation_set = {\"relu\", \"leaky_relu\", \"tanh\", \"none\"}\n",
    "\n",
    "        # Define scope.\n",
    "        scope = \"{}/resnet/dec\".format(self.name)\n",
    "\n",
    "        # Create input layer to decoder.\n",
    "        network = bottleneck\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=scope, reuse=tf.AUTO_REUSE):\n",
    "            # Iteratively build upsampling layers.\n",
    "            for i in range(len(params[\"generator_resnet_dec_num_filters\"])):\n",
    "                # Create pre-activation network graph.\n",
    "                network = self.pre_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    downscale=False,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Apply activation.\n",
    "                activation = params[\"generator_resnet_dec_activation\"][i]\n",
    "                network = self.apply_activation(\n",
    "                    input_tensor=network,\n",
    "                    activation_name=activation,\n",
    "                    activation_set=activation_set,\n",
    "                    params=params,\n",
    "                    scope=\"resnet_decoder\",\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Create post-activation network graph.\n",
    "                network = self.post_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=scope,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def resnet_generator(self, source_images, params):\n",
    "        \"\"\"Creates resnet generator network and returns generated images.\n",
    "\n",
    "        Args:\n",
    "            source_images: tensor, source images of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated target images tensor of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"resnet_generator\"\n",
    "        print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "        # Encode image into bottleneck.\n",
    "        bottleneck = self.resnet_encoder(\n",
    "            source_images=source_images, params=params\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"bottleneck\", bottleneck)\n",
    "\n",
    "        # Pass bottleneck through residual blocks.\n",
    "        for i in range(params[\"generator_num_resnet_blocks\"]):\n",
    "            bottleneck = self.resnet_res_block(\n",
    "                inputs=bottleneck, params=params, block_idx=i\n",
    "            )\n",
    "            print_obj(\"\\n\" + func_name, \"bottleneck\", bottleneck)\n",
    "\n",
    "        # Decode bottleneck back into image.\n",
    "        fake_target_images = self.resnet_decoder(\n",
    "            bottleneck=bottleneck, params=params\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "        return fake_target_images\n",
    "\n",
    "    def get_fake_images(self, source_images, params):\n",
    "        \"\"\"Creates generator network and returns generated images.\n",
    "\n",
    "        Args:\n",
    "            source_images: tensor, source images of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Generated target images tensor of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"get_fake_images\"\n",
    "        print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "        if params[\"generator_use_unet\"]:\n",
    "            fake_target_images = self.unet_generator(\n",
    "                source_images=source_images, params=params\n",
    "            )\n",
    "        else:\n",
    "            fake_target_images = self.resnet_generator(\n",
    "                source_images=source_images, params=params\n",
    "            )\n",
    "\n",
    "        # Resize fake target images to match real target image sizes.\n",
    "        fake_target_images = image_utils.resize_fake_images(\n",
    "            fake_images=fake_target_images, params=params\n",
    "        )\n",
    "        print_obj(\"\\n\" + func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "        return fake_target_images\n",
    "\n",
    "    def get_generator_loss(self, fake_logits, params):\n",
    "        \"\"\"Gets generator loss.\n",
    "\n",
    "        Args:\n",
    "            fake_logits: tensor, shape of\n",
    "                [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of generator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        func_name = \"get_generator_loss\"\n",
    "\n",
    "        # Calculate base generator loss.\n",
    "        if params[\"use_least_squares_loss\"]:\n",
    "            generator_loss = tf.losses.mean_squared_error(\n",
    "                labels=tf.ones_like(tensor=fake_logits),\n",
    "                predictions=fake_logits\n",
    "            )\n",
    "        else:\n",
    "            generator_loss = tf.reduce_mean(\n",
    "                input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=fake_logits,\n",
    "                    labels=tf.ones_like(tensor=fake_logits)\n",
    "                ),\n",
    "                name=\"{}_loss\".format(self.name)\n",
    "            )\n",
    "        print_obj(\n",
    "            \"\\n\" + func_name, \"{}_loss\".format(self.name), generator_loss\n",
    "        )\n",
    "\n",
    "        # Get regularization losses.\n",
    "        generator_reg_loss = tf.losses.get_regularization_loss(\n",
    "            scope=self.name,\n",
    "            name=\"{}_regularization_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"{}_reg_loss\".format(self.name), generator_reg_loss\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        generator_total_loss = tf.math.add(\n",
    "            x=generator_loss,\n",
    "            y=generator_reg_loss,\n",
    "            name=\"{}_total_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"{}_total_loss\".format(self.name), generator_total_loss\n",
    "        )\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_loss\".format(self.name),\n",
    "            tensor=generator_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_reg_loss\".format(self.name),\n",
    "            tensor=generator_reg_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_total_loss\".format(self.name),\n",
    "            tensor=generator_total_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "\n",
    "        return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/discriminator.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import networks\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class Discriminator(networks.Networks):\n",
    "    \"\"\"Discriminator that takes image input and outputs logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_regularizer, bias_regularizer, name):\n",
    "        \"\"\"Instantiates discriminator network.\n",
    "\n",
    "        Args:\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of discriminator.\n",
    "        \"\"\"\n",
    "        # Initialize base network class.\n",
    "        super().__init__(kernel_regularizer, bias_regularizer, name)\n",
    "\n",
    "    def get_discriminator_logits(self, target_image, params):\n",
    "        \"\"\"Creates discriminator network and returns logits.\n",
    "\n",
    "        Args:\n",
    "            target_image: tensor, target image tensor of shape\n",
    "                [cur_batch_size, height, width, depth].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Logits tensor of shape [cur_batch_size, 16, 16, 1].\n",
    "        \"\"\"\n",
    "        func_name = \"get_discriminator_logits\"\n",
    "        print_obj(\"\\n\" + func_name, \"target_image\", target_image)\n",
    "\n",
    "        # The set of allowed activations.\n",
    "        activation_set = {\"relu\", \"leaky_relu\", \"tanh\", \"none\"}\n",
    "\n",
    "        # Create input layer to discriminator network.\n",
    "        network = target_image\n",
    "\n",
    "        with tf.variable_scope(name_or_scope=self.name, reuse=tf.AUTO_REUSE):\n",
    "            # Iteratively build downsampling layers.\n",
    "            for i in range(len(params[\"discriminator_num_filters\"])):\n",
    "                # Create pre-activation network graph.\n",
    "                network = self.pre_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=self.name,\n",
    "                    downscale=True,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Apply activation.\n",
    "                network = self.apply_activation(\n",
    "                    input_tensor=network,\n",
    "                    activation_name=params[\"discriminator_activation\"][i],\n",
    "                    activation_set=activation_set,\n",
    "                    params=params,\n",
    "                    scope=self.name,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "                # Create post-activation network graph.\n",
    "                network = self.post_activation_network(\n",
    "                    network=network,\n",
    "                    params=params,\n",
    "                    scope=self.name,\n",
    "                    block_idx=0,\n",
    "                    layer_idx=i\n",
    "                )\n",
    "                print_obj(func_name, \"network\", network)\n",
    "\n",
    "        return network\n",
    "\n",
    "    def get_discriminator_loss(self, fake_logits, real_logits, params):\n",
    "        \"\"\"Gets discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            fake_logits: tensor, shape of [cur_batch_size, 1].\n",
    "            real_logits: tensor, shape of [cur_batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of discriminator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        func_name = \"get_{}_loss\".format(self.name)\n",
    "        # Calculate base discriminator loss.\n",
    "        if params[\"use_least_squares_loss\"]:\n",
    "            discriminator_real_loss = tf.losses.mean_squared_error(\n",
    "                labels=tf.ones_like(tensor=real_logits),\n",
    "                predictions=real_logits\n",
    "            )\n",
    "        else:\n",
    "            discriminator_real_loss = tf.reduce_mean(\n",
    "                input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=real_logits,\n",
    "                    labels=tf.ones_like(tensor=real_logits)\n",
    "                ),\n",
    "                name=\"{}_real_loss\".format(self.name)\n",
    "            )\n",
    "        print_obj(\n",
    "            \"\\n\" + func_name,\n",
    "            \"{}_real_loss\".format(self.name),\n",
    "            discriminator_real_loss\n",
    "        )\n",
    "\n",
    "        if params[\"use_least_squares_loss\"]:\n",
    "            discriminator_fake_loss = tf.losses.mean_squared_error(\n",
    "                labels=tf.zeros_like(tensor=fake_logits),\n",
    "                predictions=fake_logits\n",
    "            )\n",
    "        else:\n",
    "            discriminator_fake_loss = tf.reduce_mean(\n",
    "                input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=fake_logits,\n",
    "                    labels=tf.zeros_like(tensor=fake_logits)\n",
    "                ),\n",
    "                name=\"{}_fake_loss\".format(self.name)\n",
    "            )\n",
    "        print_obj(\n",
    "            func_name,\n",
    "            \"{}_fake_loss\".format(self.name),\n",
    "            discriminator_fake_loss\n",
    "        )\n",
    "\n",
    "        discriminator_loss = tf.add(\n",
    "            x=discriminator_real_loss,\n",
    "            y=discriminator_fake_loss,\n",
    "            name=\"{}_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"{}_loss\".format(self.name), discriminator_loss\n",
    "        )\n",
    "\n",
    "        # Divide discriminator loss by 2 so that it trains slower.\n",
    "        discriminator_loss *= 0.5\n",
    "\n",
    "        # Get regularization losses.\n",
    "        discriminator_reg_loss = tf.losses.get_regularization_loss(\n",
    "            scope=self.name,\n",
    "            name=\"{}_reg_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"{}_reg_loss\".format(self.name), discriminator_reg_loss\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        discriminator_total_loss = tf.math.add(\n",
    "            x=discriminator_loss,\n",
    "            y=discriminator_reg_loss,\n",
    "            name=\"{}_total_loss\".format(self.name)\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"{}_total_loss\".format(self.name), discriminator_total_loss\n",
    "        )\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_real_loss\".format(self.name),\n",
    "            tensor=discriminator_real_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_fake_loss\".format(self.name),\n",
    "            tensor=discriminator_fake_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_loss\".format(self.name),\n",
    "            tensor=discriminator_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_reg_loss\".format(self.name),\n",
    "            tensor=discriminator_reg_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"{}_total_loss\".format(self.name),\n",
    "            tensor=discriminator_total_loss,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "\n",
    "        return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_pool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/image_pool.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "class ImagePool():\n",
    "    \"\"\"An image buffer that stores previously generated images.\n",
    "\n",
    "    This buffer enables us to update discriminators using a history of\n",
    "    generated images rather than the ones produced by the latest generators.\n",
    "\n",
    "    Fields:\n",
    "        pool_domain: str, name of the domain of images the pool contains.\n",
    "        pool_capacity: int, the max size of the image pool.\n",
    "        num_images: int, the current number of images in the pool.\n",
    "        images: list, previously saved generated images.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_domain, pool_capacity, params):\n",
    "        \"\"\"Initialize an `ImagePool`.\n",
    "\n",
    "        Args:\n",
    "            pool_domain: str, name of the domain of images the pool contains.\n",
    "            pool_capacity: int, size of image buffer, if pool_capacity = 0, no\n",
    "                buffer will be created.\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        self.pool_domain = pool_domain\n",
    "        self.pool_capacity = pool_capacity\n",
    "        # Create an empty pool\n",
    "        if self.pool_capacity > 0:\n",
    "            self._create_image_pool_variables(params=params)\n",
    "\n",
    "    def _create_image_pool_variables(self, params):\n",
    "        \"\"\"Creates image pool variables.\n",
    "\n",
    "        Args:\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        func_name = \"_create_image_pool_variables_{}\".format(self.pool_domain)\n",
    "\n",
    "        with tf.variable_scope(\n",
    "            name_or_scope=\"image_pool_{}\".format(self.pool_domain),\n",
    "            reuse=tf.AUTO_REUSE\n",
    "        ):\n",
    "            # Keeps tally of how many images are currently in the image pool.\n",
    "            self.num_images_var = tf.get_variable(\n",
    "                name=\"num_images\",\n",
    "                initializer=tf.zeros(shape=[], dtype=tf.int32),\n",
    "                trainable=False\n",
    "            )\n",
    "            print_obj(func_name, \"num_images_var\", self.num_images_var)\n",
    "\n",
    "            # Contains previous generated images from generator.\n",
    "            self.images_var = tf.get_variable(\n",
    "                name=\"images\",\n",
    "                initializer=tf.zeros(\n",
    "                    shape=[\n",
    "                        self.pool_capacity,\n",
    "                        params[\"height\"],\n",
    "                        params[\"width\"],\n",
    "                        params[\"depth\"]\n",
    "                    ],\n",
    "                    dtype=tf.float32\n",
    "                ),\n",
    "                trainable=False\n",
    "            )\n",
    "            print_obj(func_name, \"images_var\", self.images_var)\n",
    "\n",
    "    def return_pool_variable_values(self):\n",
    "        \"\"\"Returns image pool variable values.\n",
    "\n",
    "        Returns:\n",
    "            Returns number of images currently in pool of shape [] and current\n",
    "                images in pool of shape [pool_capacity, height, width, depth].\n",
    "        \"\"\"\n",
    "        return self.num_images_var.value(), self.images_var.value()\n",
    "\n",
    "    def _add_images_to_not_full_pool(self, images):\n",
    "        \"\"\"Adds images to pool iff pool isn't already full.\n",
    "\n",
    "        Args:\n",
    "            images: tensor, generated images of shape\n",
    "                [batch_size, height, width, depth].\n",
    "        Returns:\n",
    "            A tensor of the new images added to the pool of shape\n",
    "                [num_new_images_to_add, height, width, depth], a tensor of the\n",
    "                number of new images added to the pool of shape [], and a\n",
    "                tensor of the number of new images still remaining to add of\n",
    "                shape [].\n",
    "        \"\"\"\n",
    "        func_name = \"_add_images_to_not_full_pool\"\n",
    "        print_obj(\"\\n\" + func_name, \"images\", images)\n",
    "\n",
    "        # Get batch size.\n",
    "        batch_size = images.shape[0].value\n",
    "\n",
    "        # Calculate remaining capacity.\n",
    "        remaining_capacity = self.pool_capacity - self.num_images_var\n",
    "\n",
    "        # Find how many images we can add to not full pool.\n",
    "        num_new_images_to_add = tf.minimum(\n",
    "            x=batch_size, y=remaining_capacity\n",
    "        )\n",
    "\n",
    "        # Check if there will be any remaining new images to add later.\n",
    "        num_new_images_remaining = batch_size - num_new_images_to_add\n",
    "\n",
    "        # Slice full new images tensor for just the ones to add to not\n",
    "        # overflow capacity.\n",
    "        new_images_add = images[0:num_new_images_to_add, :, :, :]\n",
    "        print_obj(func_name, \"new_images_add\", new_images_add)\n",
    "\n",
    "        # Assign new images to pool.\n",
    "        images_var_assign_op = self.images_var.assign(\n",
    "            value=tf.concat(\n",
    "                values=[\n",
    "                    self.images_var[0:self.num_images_var, :, :, :],\n",
    "                    new_images_add,\n",
    "                    self.images_var[self.num_images_var + num_new_images_to_add:, :, :, :],\n",
    "                ],\n",
    "                axis=0\n",
    "            ),\n",
    "            use_locking=True,\n",
    "            read_value=False\n",
    "        )\n",
    "        print_obj(func_name, \"images_var_assign_op\", images_var_assign_op)\n",
    "\n",
    "        with tf.control_dependencies(control_inputs=[images_var_assign_op]):\n",
    "            # Assign new image count.\n",
    "            num_images_var_assign_op = self.num_images_var.assign_add(\n",
    "                delta=num_new_images_to_add,\n",
    "                use_locking=True,\n",
    "                read_value=False\n",
    "            )\n",
    "            print_obj(\n",
    "                func_name, \"num_images_var_assign_op\", num_images_var_assign_op\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies(\n",
    "                control_inputs=[\n",
    "                    num_images_var_assign_op\n",
    "                ]\n",
    "            ):\n",
    "                num_new_images_remaining = tf.identity(\n",
    "                    input=num_new_images_remaining\n",
    "                )\n",
    "                print_obj(\n",
    "                    func_name,\n",
    "                    \"num_new_images_remaining\",\n",
    "                    num_new_images_remaining\n",
    "                )\n",
    "\n",
    "        return (new_images_add,\n",
    "                num_new_images_to_add,\n",
    "                num_new_images_remaining\n",
    "                )\n",
    "\n",
    "    def _select_previous_images(self, num_new_images_remaining):\n",
    "        \"\"\"Selects previous images from pool.\n",
    "\n",
    "        Args:\n",
    "            num_new_images_remaining: tensor, number of new images still\n",
    "                remaining to add of shape [].\n",
    "        Returns:\n",
    "            Tensor of previous images from the pool of shape\n",
    "                [num_new_images_remaining, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"_select_previous_images\"\n",
    "        print_obj(\n",
    "            \"\\n\" + func_name,\n",
    "            \"num_new_images_remaining\",\n",
    "            num_new_images_remaining\n",
    "        )\n",
    "\n",
    "        # Shuffle indices to get a random, unique ordering.\n",
    "        shuffled_indices = tf.random.shuffle(\n",
    "            value=tf.range(\n",
    "                start=0, limit=self.num_images_var.value(), dtype=tf.int32\n",
    "            ),\n",
    "            name=\"{}_random_shuffle\".format(self.pool_domain)\n",
    "        )\n",
    "        print_obj(func_name, \"shuffled_indices\", shuffled_indices)\n",
    "\n",
    "        # Since it is shuffled, take just as many stored images as needed.\n",
    "        sliced_indices = shuffled_indices[0:num_new_images_remaining]\n",
    "        print_obj(func_name, \"sliced_indices\", sliced_indices)\n",
    "\n",
    "        # Gather images using indices.\n",
    "        prev_images = tf.gather(\n",
    "            params=self.images_var,\n",
    "            indices=sliced_indices,\n",
    "            axis=0,\n",
    "            batch_dims=1,\n",
    "            name=\"{}_gathered_prev_images\".format(self.pool_domain)\n",
    "        )\n",
    "        print_obj(func_name, \"prev_images\", prev_images)\n",
    "\n",
    "        return prev_images\n",
    "\n",
    "    def _get_mix_of_new_and_prev_images(\n",
    "            self, images, num_new_images_added, num_new_images_remaining):\n",
    "        \"\"\"Gets mix of new and previous images from the pool.\n",
    "\n",
    "        Args:\n",
    "            images: tensor, generated images of shape\n",
    "                [batch_size, height, width, depth].\n",
    "            num_new_images_added: tensor, number of new images already added\n",
    "                to the pool of shape [].\n",
    "            num_new_images_remaining: tensor, number of new images still\n",
    "                remaining to add of shape [].\n",
    "        Returns:\n",
    "            Tensor of new and previous images from the pool mixed of shape\n",
    "                [num_new_images_remaining, height, width, depth].\n",
    "        \"\"\"\n",
    "        func_name = \"_get_mix_of_new_and_prev_images\"\n",
    "        print_obj(\"\\n\" + func_name, \"images\", images)\n",
    "\n",
    "        # Randomly choose previous images.\n",
    "        prev_images = self._select_previous_images(\n",
    "            num_new_images_remaining\n",
    "        )\n",
    "        print_obj(func_name, \"prev_images\", prev_images)\n",
    "\n",
    "        # Determine what branch to do with 50%/50% probability.\n",
    "        random_probs = tf.random.uniform(\n",
    "            shape=[num_new_images_remaining],\n",
    "            minval=0.,\n",
    "            maxval=1.,\n",
    "            dtype=tf.float32,\n",
    "            name=\"{}_random_probabilities\".format(self.pool_domain)\n",
    "        )\n",
    "        print_obj(func_name, \"random_probs\", random_probs)\n",
    "\n",
    "        # With 50% probability, choose whether to use previous or new image\n",
    "        # for each remaining image slot.\n",
    "        mixed_images = tf.where(\n",
    "            condition=tf.greater(x=random_probs, y=0.5),\n",
    "            x=prev_images,\n",
    "            y=images[num_new_images_added:, :, :, :],\n",
    "        )\n",
    "        print_obj(func_name, \"mixed_images\", mixed_images)\n",
    "\n",
    "        # Gather previous images to keep and not overwrite.\n",
    "        prev_images_to_keep = tf.gather(\n",
    "            params=self.images_var,\n",
    "            indices=tf.range(\n",
    "                start=num_new_images_remaining,\n",
    "                limit=self.pool_capacity,\n",
    "                dtype=tf.int32\n",
    "            ),\n",
    "            axis=0,\n",
    "            batch_dims=1\n",
    "        )\n",
    "        print_obj(func_name, \"prev_images_to_keep\", prev_images_to_keep)\n",
    "\n",
    "        # Combine previous images to keep with mixed images for mixed pool.\n",
    "        mixed_pool = tf.concat(\n",
    "            values=[prev_images_to_keep, mixed_images], axis=0\n",
    "        )\n",
    "        print_obj(func_name, \"mixed_pool\", mixed_pool)\n",
    "\n",
    "        # Assign to image pool variable.\n",
    "        images_var_assign_op = self.images_var.assign(\n",
    "            value=mixed_pool,\n",
    "            use_locking=True,\n",
    "            read_value=False\n",
    "        )\n",
    "        print_obj(func_name, \"images_var_assign_op\", images_var_assign_op)\n",
    "\n",
    "        # Make sure pool image variable assignment is complete.\n",
    "        with tf.control_dependencies(control_inputs=[images_var_assign_op]):\n",
    "            # With 50% probability, choose whether to return previous or new\n",
    "            # images for each image.\n",
    "            return_images = tf.where(\n",
    "                condition=tf.greater(x=random_probs, y=0.5),\n",
    "                x=prev_images,\n",
    "                y=images[num_new_images_added:, :, :, :]\n",
    "            )\n",
    "            print_obj(func_name, \"return_images\", return_images)\n",
    "\n",
    "        return return_images\n",
    "\n",
    "    def query(self, images, params):\n",
    "        \"\"\"Returns an image from the pool.\n",
    "\n",
    "        50% chance buffer will return input images or 50% chance buffer will\n",
    "        return previous images previously and put current images into buffer.\n",
    "\n",
    "        Args:\n",
    "            images: tensor, the latest generated images from the generator of\n",
    "                shape [batch_size, height, width, depth].\n",
    "        Returns:\n",
    "            Mix of new images and images from the buffer.\n",
    "        \"\"\"\n",
    "        func_name = \"query\"\n",
    "        print_obj(\"\\n\" + func_name, \"num_new_images_remaining\", images)\n",
    "\n",
    "        if self.pool_capacity == 0:\n",
    "            # If the buffer capacity is zero, then just return input images.\n",
    "            return images\n",
    "\n",
    "        # If still room in buffer, put current images into it.\n",
    "        # Add images to pool if not full.\n",
    "        (new_return_images,\n",
    "         num_new_images_added,\n",
    "         num_new_images_remaining\n",
    "         ) = self._add_images_to_not_full_pool(images)\n",
    "        print_obj(func_name, \"new_return_images\", new_return_images)\n",
    "\n",
    "        # If there are still some new images remaining.\n",
    "        mix_return_images = self._get_mix_of_new_and_prev_images(\n",
    "            images, num_new_images_added, num_new_images_remaining\n",
    "        )\n",
    "        print_obj(func_name, \"mix_return_images\", mix_return_images)\n",
    "\n",
    "        # Concatenate both sets of return images.\n",
    "        return_images = tf.concat(\n",
    "            values=[new_return_images, mix_return_images],\n",
    "            axis=0,\n",
    "            name=\"{}_updated_return_list\".format(self.pool_domain)\n",
    "        )\n",
    "        print_obj(func_name, \"return_images\", return_images)\n",
    "\n",
    "        return return_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/train_and_eval.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_pool\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_logits_and_losses(\n",
    "        source_images,\n",
    "        real_target_images,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        image_pool,\n",
    "        mode,\n",
    "        params,\n",
    "        domain_index):\n",
    "    \"\"\"Gets logits and losses for both train and eval modes.\n",
    "\n",
    "    Args:\n",
    "        source_images: tensor, images of source domain.\n",
    "        real_target_images: tensor, real images of target domain.\n",
    "        generator: instance of generator.`Generator`.\n",
    "        discriminator: instance of discriminator.`Discriminator`.\n",
    "        image_pool: instance of `ImagePool` buffer to store generated images\n",
    "            for disriminator.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN or EVAL.\n",
    "        params: dict, user passed parameters.\n",
    "        domain_index: int, index of current domain.\n",
    "\n",
    "    Returns:\n",
    "        Real and fake logits and generator and discriminator losses.\n",
    "    \"\"\"\n",
    "    func_name = \"get_logits_and_losses\"\n",
    "    print_obj(func_name, \"source_images\", source_images)\n",
    "    print_obj(func_name, \"real_target_images\", real_target_images)\n",
    "\n",
    "    # Get generated target image from generator network from source image.\n",
    "    print(\"\\nCall generator with source_images = {}.\".format(source_images))\n",
    "    fake_target_images = generator.get_fake_images(\n",
    "        source_images=source_images, params=params\n",
    "    )\n",
    "    print_obj(func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "    # Add summaries for TensorBoard.\n",
    "    tf.summary.image(\n",
    "        name=\"fake_target_images_domain{}\".format(domain_index + 1),\n",
    "        tensor=tf.reshape(\n",
    "            tensor=fake_target_images,\n",
    "            shape=[-1, params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "        ),\n",
    "        max_outputs=5,\n",
    "    )\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # Add generated images to `ImagePool` and retrieve previous ones.\n",
    "        fake_target_images = image_pool.query(\n",
    "            images=fake_target_images, params=params\n",
    "        )\n",
    "        print_obj(func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "    # Get fake logits from discriminator with generator's output target image.\n",
    "    print(\n",
    "        \"\\nCall discriminator with fake_target_images = {}.\".format(\n",
    "            fake_target_images\n",
    "        )\n",
    "    )\n",
    "    fake_logits = discriminator.get_discriminator_logits(\n",
    "        target_image=fake_target_images,\n",
    "        params=params\n",
    "    )\n",
    "    print_obj(func_name, \"fake_logits\", fake_logits)\n",
    "\n",
    "    # Get real logits from discriminator using real target image.\n",
    "    print(\n",
    "        \"\\nCall discriminator with real_target_images = {}.\".format(\n",
    "            real_target_images\n",
    "        )\n",
    "    )\n",
    "    real_logits = discriminator.get_discriminator_logits(\n",
    "        target_image=real_target_images,\n",
    "        params=params\n",
    "    )\n",
    "    print_obj(func_name, \"fake_logits\", fake_logits)\n",
    "\n",
    "    # Get generator total loss.\n",
    "    generator_total_loss = generator.get_generator_loss(\n",
    "        fake_logits=fake_logits, params=params\n",
    "    )\n",
    "\n",
    "    # Get discriminator total loss.\n",
    "    discriminator_total_loss = discriminator.get_discriminator_loss(\n",
    "        fake_logits=fake_logits, real_logits=real_logits, params=params\n",
    "    )\n",
    "\n",
    "    return (real_logits,\n",
    "            fake_logits,\n",
    "            generator_total_loss,\n",
    "            discriminator_total_loss\n",
    "            )\n",
    "\n",
    "\n",
    "def get_cycle_consistency_loss(\n",
    "        source_images, generator1, generator2, params, domain_index):\n",
    "    \"\"\"Gets cycle consistency L1 loss between source and fake source images.\n",
    "\n",
    "    Args:\n",
    "        source_images: tensor, source images of domain 2.\n",
    "        generator1: instance of `Generator` of domain 1.\n",
    "        generator2: instance of `Generator` of domain 2.\n",
    "        params: dict, user passed parameters.\n",
    "        domain_index: int, index of current domain.\n",
    "\n",
    "    Returns:\n",
    "        Cycle L1 loss between source and fake source images.\n",
    "    \"\"\"\n",
    "    func_name = \"get_cycle_consistency_loss\"\n",
    "    print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "    # Get generated target image from generator network from source image.\n",
    "    print(\n",
    "        \"\\nCall generator{} with source_images = {}.\".format(\n",
    "            domain_index, source_images\n",
    "        )\n",
    "    )\n",
    "    fake_target_images = generator1.get_fake_images(\n",
    "        source_images=source_images, params=params\n",
    "    )\n",
    "    print_obj(func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "    # Get generated target image from generator network from source image.\n",
    "    print(\n",
    "        \"\\nCall generator{} with fake_target_images = {}.\".format(\n",
    "            1 - domain_index, fake_target_images\n",
    "        )\n",
    "    )\n",
    "    fake_source_images = generator2.get_fake_images(\n",
    "        source_images=fake_target_images, params=params\n",
    "    )\n",
    "    print_obj(func_name, \"fake_source_images\", fake_source_images)\n",
    "\n",
    "    # Get L1 loss from difference between source and fake source images.\n",
    "    cycle_l1_loss = tf.reduce_mean(\n",
    "        input_tensor=tf.abs(\n",
    "            x=tf.subtract(\n",
    "                x=source_images, y=fake_source_images\n",
    "            )\n",
    "        ),\n",
    "        name=\"cycle_l1_loss_{}\".format(domain_index)\n",
    "    ) + 1e-8\n",
    "    print_obj(func_name, \"cycle_l1_loss\", cycle_l1_loss)\n",
    "\n",
    "    return cycle_l1_loss\n",
    "\n",
    "\n",
    "def get_identity_loss(real_target_images, generator, params, domain_index):\n",
    "    \"\"\"Gets identity loss between target and fake target images.\n",
    "\n",
    "    Args:\n",
    "        real_target_images: tensor, real images of target domain.\n",
    "        generator: instance of `Generator`.\n",
    "        params: dict, user passed parameters.\n",
    "        domain_index: int, index of current domain.\n",
    "\n",
    "    Returns:\n",
    "        Identity loss between target and fake target images.\n",
    "    \"\"\"\n",
    "    func_name = \"get_identity_loss\"\n",
    "    print_obj(\"\\n\" + func_name, \"real_target_images\", real_target_images)\n",
    "\n",
    "    # Get generated target image from generator network from target image.\n",
    "    print(\n",
    "        \"\\nCall generator{} with real_target_images = {}.\".format(\n",
    "            domain_index, real_target_images\n",
    "        )\n",
    "    )\n",
    "    fake_target_images = generator.get_fake_images(\n",
    "        source_images=real_target_images, params=params\n",
    "    )\n",
    "    print_obj(func_name, \"fake_target_images\", fake_target_images)\n",
    "\n",
    "    # Get L1 loss from difference between real and fake target images.\n",
    "    identity_l1_loss = tf.reduce_mean(\n",
    "        input_tensor=tf.abs(\n",
    "            x=tf.subtract(\n",
    "                x=real_target_images, y=fake_target_images\n",
    "            )\n",
    "        ),\n",
    "        name=\"identity_l1_loss_{}\".format(domain_index)\n",
    "    ) + 1e-8\n",
    "    print_obj(func_name, \"identity_l1_loss\", identity_l1_loss)\n",
    "\n",
    "    return identity_l1_loss\n",
    "\n",
    "\n",
    "def get_logits_and_losses_combined_domains(\n",
    "        features,\n",
    "        generator_domain_a2b,\n",
    "        discriminator_domain_b,\n",
    "        generator_domain_b2a,\n",
    "        discriminator_domain_a,\n",
    "        mode,\n",
    "        params):\n",
    "    \"\"\"Gets logits and losses for both train and eval modes for both domains.\n",
    "\n",
    "    Args:\n",
    "        features: dict, feature tensors from serving input function.\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of discriminator.`Discriminator`.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN or EVAL.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Real and fake logits and generator and discriminator losses.\n",
    "    \"\"\"\n",
    "    func_name = \"get_logits_and_losses_combined_domains\"\n",
    "\n",
    "    # Extract images from features dictionary.\n",
    "    domain_a_images = features[\"domain_a_image\"]\n",
    "    domain_b_images = features[\"domain_b_image\"]\n",
    "    print_obj(\"\\n\" + func_name, \"domain_a_images\", domain_a_images)\n",
    "    print_obj(func_name, \"domain_b_images\", domain_b_images)\n",
    "\n",
    "    # Instantiate `ImagePool` buffers to store generated images.\n",
    "    image_pool_domain_a = image_pool.ImagePool(\n",
    "        pool_domain=\"domain_a\",\n",
    "        pool_capacity=params[\"image_pool_capacity\"],\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    image_pool_domain_b = image_pool.ImagePool(\n",
    "        pool_domain=\"domain_b\",\n",
    "        pool_capacity=params[\"image_pool_capacity\"],\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Get logits and losses using domain a as source and domain b as target.\n",
    "    (real_logits_domain_b,\n",
    "     fake_logits_domain_b,\n",
    "     generator_domain_a2b_total_loss,\n",
    "     discriminator_domain_b_total_loss) = get_logits_and_losses(\n",
    "        source_images=domain_a_images,\n",
    "        real_target_images=domain_b_images,\n",
    "        generator=generator_domain_a2b,\n",
    "        discriminator=discriminator_domain_b,\n",
    "        image_pool=image_pool_domain_b,\n",
    "        mode=mode,\n",
    "        params=params,\n",
    "        domain_index=0\n",
    "    )\n",
    "\n",
    "    # Get logits and losses using domain b as source and domain a as target.\n",
    "    (real_logits_domain_a,\n",
    "     fake_logits_domain_a,\n",
    "     generator_domain_b2a_total_loss,\n",
    "     discriminator_domain_a_total_loss) = get_logits_and_losses(\n",
    "        source_images=domain_b_images,\n",
    "        real_target_images=domain_a_images,\n",
    "        generator=generator_domain_b2a,\n",
    "        discriminator=discriminator_domain_a,\n",
    "        image_pool=image_pool_domain_a,\n",
    "        mode=mode,\n",
    "        params=params,\n",
    "        domain_index=1\n",
    "    )\n",
    "\n",
    "    if params[\"forward_cycle_loss_lambda\"] > 0.0:\n",
    "        # Get forward cycle consistency loss.\n",
    "        forward_cycle_loss = tf.multiply(\n",
    "            x=get_cycle_consistency_loss(\n",
    "                source_images=domain_a_images,\n",
    "                generator1=generator_domain_a2b,\n",
    "                generator2=generator_domain_b2a,\n",
    "                params=params,\n",
    "                domain_index=0\n",
    "            ),\n",
    "            y=params[\"forward_cycle_loss_lambda\"],\n",
    "            name=\"forward_cycle_loss\"\n",
    "        )\n",
    "        print_obj(func_name, \"forward_cycle_loss\", forward_cycle_loss)\n",
    "    else:\n",
    "        forward_cycle_loss = 0.0\n",
    "\n",
    "    if params[\"backward_cycle_loss_lambda\"] > 0.0:\n",
    "        # Get backward cycle consistency loss.\n",
    "        backward_cycle_loss = tf.multiply(\n",
    "            x=get_cycle_consistency_loss(\n",
    "                source_images=domain_b_images,\n",
    "                generator1=generator_domain_b2a,\n",
    "                generator2=generator_domain_a2b,\n",
    "                params=params,\n",
    "                domain_index=1\n",
    "            ),\n",
    "            y=params[\"backward_cycle_loss_lambda\"],\n",
    "            name=\"backward_cycle_loss\"\n",
    "        )\n",
    "        print_obj(func_name, \"backward_cycle_loss\", backward_cycle_loss)\n",
    "    else:\n",
    "        backward_cycle_loss = 0.0\n",
    "\n",
    "    # Combine cycle consistency losses together for forward and backward.\n",
    "    cycle_loss = tf.add(\n",
    "        x=forward_cycle_loss, y=backward_cycle_loss, name=\"cycle_loss\"\n",
    "    )\n",
    "    print_obj(func_name, \"cycle_loss\", cycle_loss)\n",
    "\n",
    "    # Add summaries for TensorBoard.\n",
    "    tf.summary.scalar(\n",
    "        name=\"forward_cycle_loss\",\n",
    "        tensor=forward_cycle_loss,\n",
    "        family=\"losses\"\n",
    "    )\n",
    "    tf.summary.scalar(\n",
    "        name=\"backward_cycle_loss\",\n",
    "        tensor=backward_cycle_loss,\n",
    "        family=\"losses\"\n",
    "    )\n",
    "    tf.summary.scalar(\n",
    "        name=\"cycle_loss\",\n",
    "        tensor=cycle_loss,\n",
    "        family=\"losses\"\n",
    "    )\n",
    "\n",
    "    # Add cycle consistency losses to generator losses.\n",
    "    generator_domain_a2b_total_loss = tf.add(\n",
    "        x=generator_domain_a2b_total_loss,\n",
    "        y=cycle_loss,\n",
    "        name=\"generator_domain_a2b_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        func_name,\n",
    "        \"generator_domain_a2b_total_loss\",\n",
    "        generator_domain_a2b_total_loss\n",
    "    )\n",
    "\n",
    "    generator_domain_b2a_total_loss = tf.add(\n",
    "        x=generator_domain_b2a_total_loss,\n",
    "        y=cycle_loss,\n",
    "        name=\"generator_domain_b2a_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        func_name,\n",
    "        \"generator_domain_b2a_total_loss\",\n",
    "        generator_domain_b2a_total_loss\n",
    "    )\n",
    "\n",
    "    if params[\"identity_loss_lambda\"] > 0.0:\n",
    "        # Get identity loss for domain a2b generator.\n",
    "        identity_loss_domain_a2b = tf.multiply(\n",
    "            x=get_identity_loss(\n",
    "                real_target_images=domain_b_images,\n",
    "                generator=generator_domain_a2b,\n",
    "                params=params,\n",
    "                domain_index=1\n",
    "            ),\n",
    "            y=params[\"backward_cycle_loss_lambda\"] * params[\"identity_loss_lambda\"],\n",
    "            name=\"identity_loss_domain_a2b\"\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"identity_loss_domain_a2b\", identity_loss_domain_a2b\n",
    "        )\n",
    "\n",
    "        # Add identity loss to generator domain a2b.\n",
    "        generator_domain_a2b_total_loss = tf.add(\n",
    "            x=generator_domain_a2b_total_loss,\n",
    "            y=identity_loss_domain_a2b,\n",
    "            name=\"generator_domain_a2b_total_loss_w_identity\"\n",
    "        )\n",
    "\n",
    "        # Get identity loss for domain b2a generator.\n",
    "        identity_loss_domain_b2a = tf.multiply(\n",
    "            x=get_identity_loss(\n",
    "                real_target_images=domain_a_images,\n",
    "                generator=generator_domain_b2a,\n",
    "                params=params,\n",
    "                domain_index=0\n",
    "            ),\n",
    "            y=params[\"forward_cycle_loss_lambda\"] * params[\"identity_loss_lambda\"],\n",
    "            name=\"identity_loss_domain_b2a\"\n",
    "        )\n",
    "        print_obj(\n",
    "            func_name, \"identity_loss_domain_b2a\", identity_loss_domain_b2a\n",
    "        )\n",
    "\n",
    "        # Add identity loss to generator domain b2a.\n",
    "        generator_domain_b2a_total_loss = tf.add(\n",
    "            x=generator_domain_b2a_total_loss,\n",
    "            y=identity_loss_domain_b2a,\n",
    "            name=\"generator_domain_b2a_total_loss_w_identity\"\n",
    "        )\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        tf.summary.scalar(\n",
    "            name=\"identity_loss_domain_a2b\",\n",
    "            tensor=identity_loss_domain_a2b,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"identity_loss_domain_b2a\",\n",
    "            tensor=identity_loss_domain_b2a,\n",
    "            family=\"losses\"\n",
    "        )\n",
    "\n",
    "    # Add summaries for TensorBoard.\n",
    "    tf.summary.scalar(\n",
    "        name=\"generator_domain_a2b_total_loss\",\n",
    "        tensor=generator_domain_a2b_total_loss,\n",
    "        family=\"losses_for_minimization\"\n",
    "    )\n",
    "    tf.summary.scalar(\n",
    "        name=\"generator_domain_b2a_total_loss\",\n",
    "        tensor=generator_domain_b2a_total_loss,\n",
    "        family=\"losses_for_minimization\"\n",
    "    )\n",
    "    tf.summary.scalar(\n",
    "        name=\"discriminator_domain_a_total_loss\",\n",
    "        tensor=discriminator_domain_a_total_loss,\n",
    "        family=\"losses_for_minimization\"\n",
    "    )\n",
    "    tf.summary.scalar(\n",
    "        name=\"discriminator_domain_b_total_loss\",\n",
    "        tensor=discriminator_domain_b_total_loss,\n",
    "        family=\"losses_for_minimization\"\n",
    "    )\n",
    "\n",
    "    return (real_logits_domain_b,\n",
    "            fake_logits_domain_b,\n",
    "            generator_domain_a2b_total_loss,\n",
    "            discriminator_domain_b_total_loss,\n",
    "            real_logits_domain_a,\n",
    "            fake_logits_domain_a,\n",
    "            generator_domain_b2a_total_loss,\n",
    "            discriminator_domain_a_total_loss\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/train.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_variables_and_gradients(loss, scope, params):\n",
    "    \"\"\"Gets variables and their gradients wrt. loss.\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        scope: str, the network's name to find its variables to train.\n",
    "        params: dict, user passed parameters.\n",
    "    Returns:\n",
    "        Lists of variables and their gradients.\n",
    "    \"\"\"\n",
    "    func_name = \"get_variables_and_gradients\"\n",
    "    print_obj(\"\\n\" + func_name, \"loss\", loss)\n",
    "    print_obj(func_name, \"scope\", scope)\n",
    "\n",
    "    # Determine kind of network.\n",
    "    network_kind = scope.split(\"/\")[0]\n",
    "\n",
    "    # Get trainable variables.\n",
    "    variables = tf.trainable_variables(scope=scope)\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"variables\", variables)\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=variables,\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    # Clip gradients by value.\n",
    "    clip_by_value = params[\"{}_clip_gradients_by_value\".format(network_kind)]\n",
    "    if clip_by_value:\n",
    "        gradients = [\n",
    "            tf.clip_by_value(\n",
    "                t=g,\n",
    "                clip_value_min=-clip_by_value,\n",
    "                clip_value_max=clip_by_value,\n",
    "                name=\"{}_clip_by_by_value_gradients\".format(scope)\n",
    "            )\n",
    "            if tf.is_tensor(x=g) else g\n",
    "            for g, v in zip(gradients, variables)\n",
    "        ]\n",
    "        print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    # Clip gradients by global norm.\n",
    "    clip_by_norm = params[\"{}_clip_gradients_global_norm\".format(network_kind)]\n",
    "    if clip_by_norm:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=clip_by_norm,\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "        print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    # Add variable names back in for identification.\n",
    "    gradients = [\n",
    "        tf.identity(\n",
    "            input=g,\n",
    "            name=\"{}_{}_gradients\".format(func_name, v.name[:-2])\n",
    "        )\n",
    "        if tf.is_tensor(x=g) else g\n",
    "        for g, v in zip(gradients, variables)\n",
    "    ]\n",
    "    print_obj(\"\\n{}_{}\".format(func_name, scope), \"gradients\", gradients)\n",
    "\n",
    "    return variables, gradients\n",
    "\n",
    "\n",
    "def create_variable_and_gradient_histogram_summaries(loss_dict, params):\n",
    "    \"\"\"Creates variable and gradient histogram summaries.\n",
    "    Args:\n",
    "        loss_dict: dict, keys are kinds and values are dictionaries with\n",
    "            keys are domains and values are scalar loss tensors for each\n",
    "            network name.\n",
    "        params: dict, user passed parameters.\n",
    "    \"\"\"\n",
    "    func_name = \"create_variable_and_gradient_histogram_summaries\"\n",
    "    print_obj(\"\\n\" + func_name, \"loss_dict\", loss_dict)\n",
    "    # Loop through network kinds.\n",
    "    for kind, kind_dict in loss_dict.items():\n",
    "        # Loops through domains.\n",
    "        for domain, loss in kind_dict.items():\n",
    "            scope = \"{}/domain_{}\".format(kind, domain)\n",
    "\n",
    "            # Get variables and their gradients wrt. loss.\n",
    "            variables, gradients = get_variables_and_gradients(\n",
    "                loss, scope, params\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\n{}_{}_{}\".format(func_name, kind, domain),\n",
    "                \"variables\",\n",
    "                variables\n",
    "            )\n",
    "            print_obj(\n",
    "                \"\\n{}_{}_{}\".format(func_name, kind, domain),\n",
    "                \"gradients\",\n",
    "                gradients\n",
    "            )\n",
    "\n",
    "            # Add summaries for TensorBoard.\n",
    "            for g, v in zip(gradients, variables):\n",
    "                tf.summary.histogram(\n",
    "                    name=\"{}\".format(v.name[:-2]),\n",
    "                    values=v,\n",
    "                    family=\"{}_domain_{}_variables\".format(kind, domain)\n",
    "                )\n",
    "                if tf.is_tensor(x=g):\n",
    "                    tf.summary.histogram(\n",
    "                        name=\"{}\".format(v.name[:-2]),\n",
    "                        values=g,\n",
    "                        family=\"{}_domain_{}_gradients\".format(kind, domain)\n",
    "                    )\n",
    "\n",
    "\n",
    "def train_network(loss, global_step, params, scope):\n",
    "    \"\"\"Trains network and returns loss and train op.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the variables that to train.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor and training op.\n",
    "    \"\"\"\n",
    "    func_name = \"train_network\"\n",
    "    print_obj(\"\\n\" + func_name, \"scope\", scope)\n",
    "\n",
    "    # Determine kind of network.\n",
    "    network_kind = scope.split(\"/\")[0]\n",
    "\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adam\": tf.train.AdamOptimizer,\n",
    "        \"Adadelta\": tf.train.AdadeltaOptimizer,\n",
    "        \"AdagradDA\": tf.train.AdagradDAOptimizer,\n",
    "        \"Adagrad\": tf.train.AdagradOptimizer,\n",
    "        \"Ftrl\": tf.train.FtrlOptimizer,\n",
    "        \"GradientDescent\": tf.train.GradientDescentOptimizer,\n",
    "        \"Momentum\": tf.train.MomentumOptimizer,\n",
    "        \"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer,\n",
    "        \"ProximalGradientDescent\": tf.train.ProximalGradientDescentOptimizer,\n",
    "        \"RMSProp\": tf.train.RMSPropOptimizer\n",
    "    }\n",
    "\n",
    "    # Get effective learning rate.\n",
    "    lr_global_step = tf.train.get_or_create_global_step()\n",
    "    lr_decay_type = params[\"{}_learning_rate_decay_type\".format(network_kind)]\n",
    "\n",
    "    if lr_decay_type == \"constant\":\n",
    "        learning_rate = params[\"{}_learning_rate\".format(network_kind)]\n",
    "    elif lr_decay_type == \"polynomial\":\n",
    "        learning_rate = tf.train.polynomial_decay(\n",
    "            learning_rate=params[\"{}_learning_rate\".format(network_kind)],\n",
    "            global_step=lr_global_step,\n",
    "            decay_steps=params[\"{}_learning_rate_decay_steps\".format(network_kind)],\n",
    "            end_learning_rate=params[\"{}_learning_rate_end_learning_rate\".format(network_kind)],\n",
    "            power=params[\"{}_learning_rate_power\".format(network_kind)],\n",
    "            cycle=params[\"{}_learning_rate_cycle\".format(network_kind)],\n",
    "            name=\"{}_learning_rate_polynomial_decay\".format(network_kind)\n",
    "        )\n",
    "    elif lr_decay_type == \"exponential\":\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            learning_rate=params[\"{}_learning_rate\".format(network_kind)],\n",
    "            global_step=lr_global_step,\n",
    "            decay_steps=params[\"{}_learning_rate_decay_steps\".format(network_kind)],\n",
    "            decay_rate=params[\"{}_learning_rate_decay_rate\".format(network_kind)],\n",
    "            staircase=params[\"{}_learning_rate_staircase\".format(network_kind)],\n",
    "            name=\"{}_learning_rate_exponential_decay\".format(network_kind)\n",
    "        )\n",
    "    elif lr_decay_type == \"cosine\":\n",
    "        learning_rate = tf.train.cosine_decay(\n",
    "            learning_rate=params[\"{}_learning_rate\".format(network_kind)],\n",
    "            global_step=lr_global_step,\n",
    "            decay_steps=params[\"{}_learning_rate_decay_steps\".format(network_kind)],\n",
    "            alpha=params[\"{}_learning_rate_alpha\".format(network_kind)],\n",
    "            name=\"{}_learning_rate_cosine_decay\".format(network_kind)\n",
    "        )\n",
    "    elif lr_decay_type == \"piecewise_polynomial\":\n",
    "        learning_rate = tf.where(\n",
    "            condition=tf.less(\n",
    "                x=lr_global_step,\n",
    "                y=params[\"{}_learning_rate_constant_steps\".format(network_kind)]\n",
    "            ),\n",
    "            x=params[\"{}_learning_rate\".format(network_kind)],\n",
    "            y=tf.train.polynomial_decay(\n",
    "                learning_rate=params[\"{}_learning_rate\".format(network_kind)],\n",
    "                global_step=tf.subtract(\n",
    "                    x=lr_global_step,\n",
    "                    y=params[\"{}_learning_rate_constant_steps\".format(network_kind)]\n",
    "                ),\n",
    "                decay_steps=params[\"{}_learning_rate_decay_steps\".format(network_kind)],\n",
    "                end_learning_rate=params[\"{}_learning_rate_end_learning_rate\".format(network_kind)],\n",
    "                power=params[\"{}_learning_rate_power\".format(network_kind)],\n",
    "                cycle=params[\"{}_learning_rate_cycle\".format(network_kind)],\n",
    "                name=\"{}_learning_rate_polynomial_decay\".format(network_kind)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        learning_rate = params[\"{}_learning_rate\".format(network_kind)]\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    if params[\"{}_optimizer\".format(network_kind)] == \"Adam\":\n",
    "        optimizer = optimizers[params[\"{}_optimizer\".format(network_kind)]](\n",
    "            learning_rate=learning_rate,\n",
    "            beta1=params[\"{}_adam_beta1\".format(network_kind)],\n",
    "            beta2=params[\"{}_adam_beta2\".format(network_kind)],\n",
    "            epsilon=params[\"{}_adam_epsilon\".format(network_kind)],\n",
    "            name=\"{}_{}_optimizer\".format(\n",
    "                scope, params[\"{}_optimizer\".format(network_kind)].lower()\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optimizers[params[\"{}_optimizer\".format(network_kind)]](\n",
    "            learning_rate=learning_rate,\n",
    "            name=\"{}_{}_optimizer\".format(\n",
    "                scope, params[\"{}_optimizer\".format(network_kind)].lower()\n",
    "            )\n",
    "        )\n",
    "    print_obj(\"{}_{}\".format(func_name, scope), \"optimizer\", optimizer)\n",
    "\n",
    "    # Get variables and gradients.\n",
    "    variables, gradients = get_variables_and_gradients(loss, scope, params)\n",
    "\n",
    "    # Zip back together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, variables)\n",
    "    print_obj(\n",
    "        \"{}_{}\".format(func_name, scope), \"grads_and_vars\", grads_and_vars\n",
    "    )\n",
    "\n",
    "    # Create train op by applying gradients to variables and incrementing\n",
    "    # global step.\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        global_step=global_step,\n",
    "        name=\"{}_apply_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    return loss, train_op\n",
    "\n",
    "\n",
    "def train_network_kind(loss_dict, global_step, params, kind):\n",
    "    \"\"\"Gets loss and train op for train mode for network kind.\n",
    "\n",
    "    Args:\n",
    "        loss_dict: dict, keys are domains and values are scalar loss tensors\n",
    "            for each network name.\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        kind: str, the kind of network, generator or discriminator.\n",
    "    Returns:\n",
    "        Loss scalar tensor and train_op to be used by the EstimatorSpec.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    train_ops = []\n",
    "\n",
    "    # Pass global step once so that apply gradients only increments it once.\n",
    "    global_steps = [None for _ in list(loss_dict.keys())[:-1]] + [global_step]\n",
    "\n",
    "    # Train network for each domain.\n",
    "    for (domain, loss), g_step in zip(list(loss_dict.items()), global_steps):\n",
    "        loss_domain, train_op_domain = train_network(\n",
    "            loss=loss,\n",
    "            global_step=g_step,\n",
    "            params=params,\n",
    "            scope=\"{}/domain_{}\".format(kind, domain)\n",
    "        )\n",
    "\n",
    "        # Append to lists.\n",
    "        losses.append(loss_domain)\n",
    "        train_ops.append(train_op_domain)\n",
    "\n",
    "    # Combine losses.\n",
    "    loss = losses[0] + losses[1]\n",
    "\n",
    "    # Group train ops.\n",
    "    train_op = tf.group(train_ops, name=\"{}_train_op\".format(kind))\n",
    "\n",
    "    return loss, train_op\n",
    "\n",
    "\n",
    "def get_loss_and_train_op(loss_dict, params):\n",
    "    \"\"\"Gets loss and train op for train mode.\n",
    "\n",
    "    Args:\n",
    "        loss_dict: dict, keys are scopes and values are scalar loss tensors\n",
    "            for each network name.\n",
    "        params: dict, user passed parameters.\n",
    "    Returns:\n",
    "        Loss scalar tensor and train_op to be used by the EstimatorSpec.\n",
    "    \"\"\"\n",
    "    func_name = \"get_loss_and_train_op\"\n",
    "\n",
    "    # Get global step.\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Determine if it is time to train generator or discriminator.\n",
    "    cycle_step = tf.mod(\n",
    "        x=global_step,\n",
    "        y=tf.cast(\n",
    "            x=tf.add(\n",
    "                x=params[\"discriminator_train_steps\"],\n",
    "                y=params[\"generator_train_steps\"]\n",
    "            ),\n",
    "            dtype=tf.int64\n",
    "        ),\n",
    "        name=\"{}_learning_rate_cycle_step\".format(func_name)\n",
    "    )\n",
    "\n",
    "    # Create choose discriminator condition.\n",
    "    condition = tf.less(\n",
    "        x=cycle_step, y=params[\"discriminator_train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Needed for batch normalization, but has no effect otherwise.\n",
    "    update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Ensure update ops get updated.\n",
    "    with tf.control_dependencies(control_inputs=update_ops):\n",
    "        # Conditionally choose to train generator or discriminator subgraph.\n",
    "        loss, train_op = tf.cond(\n",
    "            pred=condition,\n",
    "            true_fn=lambda: train_network_kind(\n",
    "                loss_dict=loss_dict[\"discriminator\"],\n",
    "                global_step=global_step,\n",
    "                params=params,\n",
    "                kind=\"discriminator\"\n",
    "            ),\n",
    "            false_fn=lambda: train_network_kind(\n",
    "                loss_dict=loss_dict[\"generator\"],\n",
    "                global_step=global_step,\n",
    "                params=params,\n",
    "                kind=\"generator\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return loss, train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval_metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/eval_metrics.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_eval_metric_ops(fake_logits, real_logits, params):\n",
    "    \"\"\"Gets eval metric ops.\n",
    "\n",
    "    Args:\n",
    "        fake_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "            discriminator having processed generator's output image.\n",
    "        real_logits: tensor, shape of [cur_batch_size, 1] that came from\n",
    "            discriminator having processed real image.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of eval metric ops.\n",
    "    \"\"\"\n",
    "    func_name = \"get_eval_metric_ops\"\n",
    "    # Concatenate discriminator logits and labels.\n",
    "    discriminator_logits = tf.concat(\n",
    "        values=[real_logits, fake_logits],\n",
    "        axis=0,\n",
    "        name=\"discriminator_concat_logits\"\n",
    "    )\n",
    "    print_obj(\"\\n\" + func_name, \"discriminator_logits\", discriminator_logits)\n",
    "\n",
    "    discriminator_labels = tf.concat(\n",
    "        values=[\n",
    "            tf.ones_like(tensor=real_logits),\n",
    "            tf.zeros_like(tensor=fake_logits)\n",
    "        ],\n",
    "        axis=0,\n",
    "        name=\"discriminator_concat_labels\"\n",
    "    )\n",
    "    print_obj(func_name, \"discriminator_labels\", discriminator_labels)\n",
    "\n",
    "    # Calculate discriminator probabilities.\n",
    "    discriminator_probabilities = tf.nn.sigmoid(\n",
    "        x=discriminator_logits, name=\"discriminator_probabilities\"\n",
    "    )\n",
    "    print_obj(\n",
    "        func_name, \"discriminator_probabilities\", discriminator_probabilities\n",
    "    )\n",
    "\n",
    "    # Create eval metric ops dictionary.\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_accuracy\"\n",
    "        ),\n",
    "        \"precision\": tf.metrics.precision(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_precision\"\n",
    "        ),\n",
    "        \"recall\": tf.metrics.recall(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            name=\"discriminator_recall\"\n",
    "        ),\n",
    "        \"auc_roc\": tf.metrics.auc(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            num_thresholds=200,\n",
    "            curve=\"ROC\",\n",
    "            name=\"discriminator_auc_roc\"\n",
    "        ),\n",
    "        \"auc_pr\": tf.metrics.auc(\n",
    "            labels=discriminator_labels,\n",
    "            predictions=discriminator_probabilities,\n",
    "            num_thresholds=200,\n",
    "            curve=\"PR\",\n",
    "            name=\"discriminator_auc_pr\"\n",
    "        )\n",
    "    }\n",
    "    print_obj(func_name, \"eval_metric_ops\", eval_metric_ops)\n",
    "\n",
    "    return eval_metric_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/predict.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def get_predictions_and_export_outputs(\n",
    "        features, generator_domain_a2b, generator_domain_b2a, params):\n",
    "    \"\"\"Gets predictions and serving export outputs.\n",
    "\n",
    "    Args:\n",
    "        features: dict, feature tensors from serving input function.\n",
    "        generator_domain_a2b: instance of `Generator` for domain a to b.\n",
    "        generator_domain_b2a: instance of `Generator` for domain b to a.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Predictions dictionary and export outputs dictionary.\n",
    "    \"\"\"\n",
    "    func_name = \"get_predictions_and_export_outputs\"\n",
    "\n",
    "    # Extract given source images from features dictionary.\n",
    "    source_images = features[\"source_image\"]\n",
    "    print_obj(\"\\n\" + func_name, \"source_images\", source_images)\n",
    "\n",
    "    # Extract domain indices from features dictionary.\n",
    "    target_domain_indices = features[\"target_domain_index\"]\n",
    "    print_obj(func_name, \"target_domain_indices\", target_domain_indices)\n",
    "\n",
    "    # Get images from generator network for domain a.\n",
    "    generated_images_domain_b = generator_domain_a2b.get_fake_images(\n",
    "        source_images=source_images, params=params\n",
    "    )\n",
    "    print_obj(\n",
    "        func_name, \"generated_images_domain_b\", generated_images_domain_b\n",
    "    )\n",
    "\n",
    "    # Get images from generator network for domain b.\n",
    "    generated_images_domain_a = generator_domain_b2a.get_fake_images(\n",
    "        source_images=source_images, params=params\n",
    "    )\n",
    "    print_obj(\n",
    "        func_name, \"generated_images_domain_a\", generated_images_domain_a\n",
    "    )\n",
    "\n",
    "    # Interweave domain a and b generated images based on domain index.\n",
    "    generated_images = tf.where(\n",
    "        condition=tf.equal(x=target_domain_indices, y=0),\n",
    "        x=generated_images_domain_a,\n",
    "        y=generated_images_domain_b,\n",
    "        name=\"prediction_where_generated_images\"\n",
    "    )\n",
    "    print_obj(func_name, \"generated_images\", generated_images)\n",
    "\n",
    "    # Create predictions dictionary.\n",
    "    predictions_dict = {\n",
    "        \"generated_images\": generated_images\n",
    "    }\n",
    "    print_obj(func_name, \"predictions_dict\", predictions_dict)\n",
    "\n",
    "    # Create export outputs.\n",
    "    export_outputs = {\n",
    "        \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "            outputs=predictions_dict)\n",
    "    }\n",
    "    print_obj(func_name, \"export_outputs\", export_outputs)\n",
    "\n",
    "    return predictions_dict, export_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cyclegan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/cyclegan.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import discriminator\n",
    "from . import eval_metrics\n",
    "from . import generator\n",
    "from . import predict\n",
    "from . import train\n",
    "from . import train_and_eval\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def create_generator_discriminator_pair(name_suffix, params):\n",
    "    \"\"\"CycleGAN Unpaired Image Translation custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        name_suffix: str, suffix to add to the end of network names.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instances of `Generator` and `Discriminator` classes.\n",
    "    \"\"\"\n",
    "    # Instantiate generator.\n",
    "    cyclegan_generator = generator.Generator(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"generator_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"generator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        name=\"generator/domain_{}\".format(name_suffix)\n",
    "    )\n",
    "\n",
    "    # Instantiate discriminator.\n",
    "    cyclegan_discriminator = discriminator.Discriminator(\n",
    "        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=params[\"discriminator_l1_regularization_scale\"],\n",
    "            scale_l2=params[\"discriminator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        name=\"discriminator/domain_{}\".format(name_suffix[-1])\n",
    "    )\n",
    "\n",
    "    return cyclegan_generator, cyclegan_discriminator\n",
    "\n",
    "\n",
    "def cyclegan_model(features, labels, mode, params):\n",
    "    \"\"\"CycleGAN Unpaired Image Translation custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `tf.estimator.EstimatorSpec` class.\n",
    "    \"\"\"\n",
    "    func_name = \"cyclegan_model\"\n",
    "    print_obj(\"\\n\" + func_name, \"features\", features)\n",
    "    print_obj(func_name, \"labels\", labels)\n",
    "    print_obj(func_name, \"mode\", mode)\n",
    "    print_obj(func_name, \"params\", params)\n",
    "\n",
    "    # Loss function, training/eval ops, etc.\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "\n",
    "    # Instantiate generator and discriminator pair for domain a.\n",
    "    cyclegan_generator_domain_a2b, cyclegan_discriminator_domain_b = (\n",
    "        create_generator_discriminator_pair(\n",
    "            name_suffix=\"a2b\", params=params\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Instantiate generator and discriminator pair for domain b.\n",
    "    cyclegan_generator_domain_b2a, cyclegan_discriminator_domain_a = (\n",
    "        create_generator_discriminator_pair(\n",
    "            name_suffix=\"b2a\", params=params\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Get predictions and export outputs.\n",
    "        (predictions_dict,\n",
    "         export_outputs) = predict.get_predictions_and_export_outputs(\n",
    "            features=features,\n",
    "            generator_domain_a2b=cyclegan_generator_domain_a2b,\n",
    "            generator_domain_b2a=cyclegan_generator_domain_b2a,\n",
    "            params=params\n",
    "        )\n",
    "    else:\n",
    "        # Get logits and losses from networks for train and eval modes.\n",
    "        (real_logits_domain_b,\n",
    "         fake_logits_domain_b,\n",
    "         generator_domain_a2b_total_loss,\n",
    "         discriminator_domain_b_total_loss,\n",
    "         real_logits_domain_a,\n",
    "         fake_logits_domain_a,\n",
    "         generator_domain_b2a_total_loss,\n",
    "         discriminator_domain_a_total_loss) = (\n",
    "            train_and_eval.get_logits_and_losses_combined_domains(\n",
    "                features=features,\n",
    "                generator_domain_a2b=cyclegan_generator_domain_a2b,\n",
    "                discriminator_domain_b=cyclegan_discriminator_domain_b,\n",
    "                generator_domain_b2a=cyclegan_generator_domain_b2a,\n",
    "                discriminator_domain_a=cyclegan_discriminator_domain_a,\n",
    "                mode=mode,\n",
    "                params=params\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Create nested dictionary of losses.\n",
    "            loss_dict = {\n",
    "                \"generator\": {\n",
    "                    \"a2b\": generator_domain_a2b_total_loss,\n",
    "                    \"b2a\": generator_domain_b2a_total_loss\n",
    "                },\n",
    "                \"discriminator\": {\n",
    "                    \"a\": discriminator_domain_a_total_loss,\n",
    "                    \"b\": discriminator_domain_b_total_loss\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Create variable and gradient histogram summaries.\n",
    "            train.create_variable_and_gradient_histogram_summaries(\n",
    "                loss_dict=loss_dict, params=params\n",
    "            )\n",
    "\n",
    "            # Get loss and train op for EstimatorSpec.\n",
    "            loss, train_op = train.get_loss_and_train_op(\n",
    "                loss_dict=loss_dict, params=params\n",
    "            )\n",
    "        else:\n",
    "            # Set eval loss.\n",
    "            loss = tf.add(\n",
    "                x=discriminator_domain_a_total_loss,\n",
    "                y=discriminator_domain_b_total_loss\n",
    "            )\n",
    "\n",
    "            # Get eval metrics.\n",
    "            eval_metric_ops = eval_metrics.get_eval_metric_ops(\n",
    "                real_logits=tf.add(\n",
    "                    x=real_logits_domain_a, y=real_logits_domain_b\n",
    "                ),\n",
    "                fake_logits=tf.add(\n",
    "                    x=fake_logits_domain_a, y=fake_logits_domain_b\n",
    "                ),\n",
    "                params=params\n",
    "            )\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs=export_outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/serving.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import image_utils\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def serving_input_fn(params):\n",
    "    \"\"\"Serving input function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        ServingInputReceiver object containing features and receiver tensors.\n",
    "    \"\"\"\n",
    "    func_name = \"serving_input_fn\"\n",
    "    # Create placeholders to accept data sent to the model at serving time.\n",
    "    # shape = [batch_size, height, width, depth]\n",
    "    feature_placeholders = {\n",
    "        \"source_image\": tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, params[\"height\"], params[\"width\"], params[\"depth\"]],\n",
    "            name=\"serving_input_placeholder_source_image\"\n",
    "        ),\n",
    "        \"target_domain_index\": tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None],\n",
    "            name=\"serving_input_placeholder_target_domain_index\"\n",
    "        )\n",
    "    }\n",
    "    print_obj(\"\\n\" + func_name, \"feature_placeholders\", feature_placeholders)\n",
    "\n",
    "    # Create clones of the feature placeholder tensors so that the SavedModel\n",
    "    # SignatureDef will point to the placeholder.\n",
    "    features = {\n",
    "        key: tf.identity(\n",
    "            input=value,\n",
    "            name=\"{}_identity_placeholder_{}\".format(func_name, key)\n",
    "        )\n",
    "        for key, value in feature_placeholders.items()\n",
    "    }\n",
    "    print_obj(func_name, \"features\", features)\n",
    "\n",
    "    # Apply same preprocessing as before.\n",
    "    features[\"source_image\"] = image_utils.preprocess_image(\n",
    "        image=features[\"source_image\"],\n",
    "        mode=tf.estimator.ModeKeys.PREDICT,\n",
    "        params=params\n",
    "    )\n",
    "    print_obj(func_name, \"features\", features)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=feature_placeholders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/model.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import input\n",
    "from . import serving\n",
    "from . import cyclegan\n",
    "from .print_object import print_obj\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates custom Estimator model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        `Estimator` object.\n",
    "    \"\"\"\n",
    "    func_name = \"train_and_evaluate\"\n",
    "    print_obj(\"\\n\" + func_name, \"args\", args)\n",
    "    # Ensure filewriter cache is clear for TensorBoard events file.\n",
    "    tf.summary.FileWriterCache.clear()\n",
    "\n",
    "    # Set logging to be level of INFO.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Create a RunConfig for Estimator.\n",
    "    config = tf.estimator.RunConfig(\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        save_summary_steps=args[\"save_summary_steps\"],\n",
    "        save_checkpoints_steps=args[\"save_checkpoints_steps\"],\n",
    "        keep_checkpoint_max=args[\"keep_checkpoint_max\"]\n",
    "    )\n",
    "\n",
    "    # Create our custom estimator using our model function.\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=cyclegan.cyclegan_model,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        config=config,\n",
    "        params=args\n",
    "    )\n",
    "\n",
    "    # Create train spec to read in our training data.\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=input.read_dataset(\n",
    "            filename=args[\"train_file_pattern\"],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size=args[\"train_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        max_steps=args[\"train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Create exporter to save out the complete model to disk.\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name=\"exporter\",\n",
    "        serving_input_receiver_fn=lambda: serving.serving_input_fn(args)\n",
    "    )\n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model.\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=input.read_dataset(\n",
    "            filename=args[\"eval_file_pattern\"],\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size=args[\"eval_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        steps=args[\"eval_steps\"],\n",
    "        start_delay_secs=args[\"start_delay_secs\"],\n",
    "        throttle_secs=args[\"throttle_secs\"],\n",
    "        exporters=exporter\n",
    "    )\n",
    "\n",
    "    # Create train and evaluate loop to train and evaluate our estimator.\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cyclegan_module/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "def convert_string_to_bool(string):\n",
    "    \"\"\"Converts string to bool.\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "    Returns:\n",
    "        Boolean conversion of string.\n",
    "    \"\"\"\n",
    "    return False if string.lower() == \"false\" else True\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_float(string):\n",
    "    \"\"\"Converts string to None or float.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or float conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else float(string)\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_int(string):\n",
    "    \"\"\"Converts string to None or int.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or int conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else int(string)\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_bools(string, sep):\n",
    "    \"\"\"Converts string to list of bools.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of bools conversion of string.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    return [convert_string_to_bool(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_ints(string, sep):\n",
    "    \"\"\"Converts string to list of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of ints conversion of string.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    return [int(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_floats(string, sep):\n",
    "    \"\"\"Converts string to list of floats.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of floats conversion of string.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    return [float(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # File arguments.\n",
    "    parser.add_argument(\n",
    "        \"--train_file_pattern\",\n",
    "        help=\"GCS location to read training data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_file_pattern\",\n",
    "        help=\"GCS location to read evaluation data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud.\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "\n",
    "    # Training parameters.\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Number of examples in training batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Number of steps to train for.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_summary_steps\",\n",
    "        help=\"How many steps to train before saving a summary.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_checkpoints_steps\",\n",
    "        help=\"How many steps to train before saving a checkpoint.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_checkpoint_max\",\n",
    "        help=\"Max number of checkpoints to keep.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_fn_autotune\",\n",
    "        help=\"Whether to autotune input function performance.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_pool_capacity\",\n",
    "        help=\"The capacity of generated image pools. If zero, then no pools.\",\n",
    "        type=int,\n",
    "        default=50\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_least_squares_loss\",\n",
    "        help=\"Whether to use least squares loss.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "    # Eval parameters.\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        help=\"Number of examples in evaluation batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Number of steps to evaluate for.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--start_delay_secs\",\n",
    "        help=\"Number of seconds to wait before first evaluation.\",\n",
    "        type=int,\n",
    "        default=60\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--throttle_secs\",\n",
    "        help=\"Number of seconds to wait between evaluations.\",\n",
    "        type=int,\n",
    "        default=120\n",
    "    )\n",
    "\n",
    "    # Image parameters.\n",
    "    parser.add_argument(\n",
    "        \"--height\",\n",
    "        help=\"Height of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--width\",\n",
    "        help=\"Width of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--depth\",\n",
    "        help=\"Depth of image.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocess_image_resize_jitter_size\",\n",
    "        help=\"List of [height, width] to resize and crop to add jitter to image during training.\",\n",
    "        type=str,\n",
    "        default=\"286,286\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocess_image_use_random_mirroring\",\n",
    "        help=\"Whether to randomly mirror image during training.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "    # Generator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_use_unet\",\n",
    "        help=\"Whether generator users U-net or resnet.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_optimizer\",\n",
    "        help=\"Name of optimizer to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for generator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_decay_type\",\n",
    "        help=\"Decay type of learning rate. Constant, polynomial, exponential, piecewise polynomial.\",\n",
    "        type=str,\n",
    "        default=\"piecewise_polynomial\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_constant_steps\",\n",
    "        help=\"Number of steps to keep learning rate constant.\",\n",
    "        type=int,\n",
    "        default=100 * 1096\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_decay_steps\",\n",
    "        help=\"Number of steps to keep decay learning rate.\",\n",
    "        type=int,\n",
    "        default=100 * 1096\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_end_learning_rate\",\n",
    "        help=\"Minimum learning rate to decay to.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_power\",\n",
    "        help=\"Polynomial decay power.\",\n",
    "        type=float,\n",
    "        default=1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_cycle\",\n",
    "        help=\"Whether to cycle learning rate for polynomial decay types.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_decay_rate\",\n",
    "        help=\"Learning rate decay rate.\",\n",
    "        type=float,\n",
    "        default=0.99\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_staircase\",\n",
    "        help=\"Whether to staircase learning rate for exponential decay types.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate_alpha\",\n",
    "        help=\"Minimum learning rate value as a fraction of learning rate for cosine decay types.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_clip_gradients_by_value\",\n",
    "        help=\"Clipping to prevent gradient to exceed this value for generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_clip_gradients_global_norm\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_train_steps\",\n",
    "        help=\"Number of steps to train generator for per cycle.\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--forward_cycle_loss_lambda\",\n",
    "        help=\"Forward cycle loss lambda weight.\",\n",
    "        type=float,\n",
    "        default=10.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backward_cycle_loss_lambda\",\n",
    "        help=\"Backward cycle loss lambda weight.\",\n",
    "        type=float,\n",
    "        default=10.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--identity_loss_lambda\",\n",
    "        help=\"Identity loss lambda weight.\",\n",
    "        type=float,\n",
    "        default=0.5\n",
    "    )\n",
    "\n",
    "    # Generator U-net encoder parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_num_filters\",\n",
    "        help=\"Number of filters for generator U-net encoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"64,128,256,512,512,512,512,512\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_kernel_sizes\",\n",
    "        help=\"Kernel sizes for generator U-net encoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"4,4,4,4,4,4,4,4\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_strides\",\n",
    "        help=\"Strides for generator U-net encoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"2,2,2,2,2,2,2,2\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_downsample\",\n",
    "        help=\"Whether generator U-net encoder layers use batch norm.\",\n",
    "        type=str,\n",
    "        default=\"True,True,True,True,True,True,True,True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_pad_type\",\n",
    "        help=\"Generator U-net encoder padding type: constant, reflection, replication.\",\n",
    "        type=str,\n",
    "        default=\"constant\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_pad_constant\",\n",
    "        help=\"Generator U-net encoder constant padding constant.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_layer_norm_before_act\",\n",
    "        help=\"Whether generator U-net encoder layers have normalization before activation.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_dropout_before_act\",\n",
    "        help=\"Whether generator U-net encoder layers have dropout before activation.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_dropout_before_norm\",\n",
    "        help=\"Whether generator U-net encoder layers have dropout before normalization.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_layer_norm_type\",\n",
    "        help=\"Generator U-net encoder layer normalization types.\",\n",
    "        type=str,\n",
    "        default=\"None,instance,instance,instance,instance,instance,instance,None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_dropout_rates\",\n",
    "        help=\"Generator U-net encoder layer dropout rates.\",\n",
    "        type=str,\n",
    "        default=\"0.,0.,0.,0.,0.,0.,0.,0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_activation\",\n",
    "        help=\"Whether generator encoder layers use leaky relu activations.\",\n",
    "        type=str,\n",
    "        default=\"leaky_relu,leaky_relu,leaky_relu,leaky_relu,leaky_relu,leaky_relu,leaky_relu,relu\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_encoder_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator encoder's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "\n",
    "    # Generator U-net decoder parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_num_filters\",\n",
    "        help=\"Number of filters for generator U-net decoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"512,512,512,512,256,128,64,3\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_kernel_sizes\",\n",
    "        help=\"Kernel sizes for generator U-net decoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"4,4,4,4,4,4,4,4\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_strides\",\n",
    "        help=\"Strides for generator U-net decoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"2,2,2,2,2,2,2,2\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_downsample\",\n",
    "        help=\"Whether generator U-net decoder layers use batch norm.\",\n",
    "        type=str,\n",
    "        default=\"False,False,False,False,False,False,False,True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_pad_type\",\n",
    "        help=\"Generator U-net decoder padding type: constant, reflection, replication.\",\n",
    "        type=str,\n",
    "        default=\"constant\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_pad_constant\",\n",
    "        help=\"Generator U-net decoder constant padding constant.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_layer_norm_before_act\",\n",
    "        help=\"Whether generator U-net decoder layers have normalization before activation.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_dropout_before_act\",\n",
    "        help=\"Whether generator U-net decoder layers have dropout before activation.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_dropout_before_norm\",\n",
    "        help=\"Whether generator U-net decoder layers have dropout before normalization.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_layer_norm_type\",\n",
    "        help=\"Generator U-net decoder layer normalization types.\",\n",
    "        type=str,\n",
    "        default=\"instance,instance,instance,instance,instance,instance,instance,None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_dropout_rates\",\n",
    "        help=\"Generator U-net decoder layer dropout rates.\",\n",
    "        type=str,\n",
    "        default=\"0.5,0.5,0.5,0.,0.,0.,0.,0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_activation\",\n",
    "        help=\"Whether generator decoder layers use leaky relu activations.\",\n",
    "        type=str,\n",
    "        default=\"relu,relu,relu,relu,relu,relu,relu,tanh\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_unet_decoder_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator decoder's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "\n",
    "    # Generator resnet encoder parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_num_filters\",\n",
    "        help=\"Number of filters for generator resnet encoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"64,128,256\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_kernel_sizes\",\n",
    "        help=\"Kernel sizes for generator resnet encoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"7,3,3\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_strides\",\n",
    "        help=\"Strides for generator resnet encoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"1,2,2\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_downsample\",\n",
    "        help=\"Whether generator resnet encoder layers use batch norm.\",\n",
    "        type=str,\n",
    "        default=\"True,True,True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_pad_type\",\n",
    "        help=\"Generator resnet encoder padding type: constant, reflection, replication.\",\n",
    "        type=str,\n",
    "        default=\"reflection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_pad_constant\",\n",
    "        help=\"Generator resnet encoder constant padding constant.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_layer_norm_before_act\",\n",
    "        help=\"Whether generator resnet encoder layers have normalization before activation.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_dropout_before_act\",\n",
    "        help=\"Whether generator resnet encoder layers have dropout before activation.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_dropout_before_norm\",\n",
    "        help=\"Whether generator resnet encoder layers have dropout before normalization.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_layer_norm_type\",\n",
    "        help=\"Generator resnet encoder layer normalization types.\",\n",
    "        type=str,\n",
    "        default=\"instance,instance,instance\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_dropout_rates\",\n",
    "        help=\"Generator resnet encoder layer dropout rates.\",\n",
    "        type=str,\n",
    "        default=\"0.,0.,0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_activation\",\n",
    "        help=\"Whether generator encoder layers use leaky relu activations.\",\n",
    "        type=str,\n",
    "        default=\"relu,relu,relu\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_enc_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator encoder's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "\n",
    "    # Generator resnet res block parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_num_resnet_blocks\",\n",
    "        help=\"Generator number of resnet residual blocks.\",\n",
    "        type=int,\n",
    "        default=9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_num_filters\",\n",
    "        help=\"Number of filters for generator resnet residual block conv layers.\",\n",
    "        type=str,\n",
    "        default=\"256,256\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_kernel_sizes\",\n",
    "        help=\"Kernel sizes for generator resnet residual block conv layers.\",\n",
    "        type=str,\n",
    "        default=\"3,3\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_strides\",\n",
    "        help=\"Strides for generator resnet residual block conv layers.\",\n",
    "        type=str,\n",
    "        default=\"1,1\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_downsample\",\n",
    "        help=\"Whether generator resnet residual block layers use batch norm.\",\n",
    "        type=str,\n",
    "        default=\"True,True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_pad_type\",\n",
    "        help=\"Generator resnet residual block padding type: constant, reflection, replication.\",\n",
    "        type=str,\n",
    "        default=\"reflection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_pad_constant\",\n",
    "        help=\"Generator resnet residual block constant padding constant.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_layer_norm_before_act\",\n",
    "        help=\"Whether generator resnet residual block layers have normalization before activation.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_dropout_before_act\",\n",
    "        help=\"Whether generator resnet residual block layers have dropout before activation.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_dropout_before_norm\",\n",
    "        help=\"Whether generator resnet residual block layers have dropout before normalization.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_layer_norm_type\",\n",
    "        help=\"Generator resnet residual block layer normalization types.\",\n",
    "        type=str,\n",
    "        default=\"instance,instance\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_dropout_rates\",\n",
    "        help=\"Generator resnet residual block layer dropout rates.\",\n",
    "        type=str,\n",
    "        default=\"0.,0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_activation\",\n",
    "        help=\"Whether generator residual block layers use leaky relu activations.\",\n",
    "        type=str,\n",
    "        default=\"relu,none\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_res_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator residual block's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "\n",
    "    # Generator resnet decoder parameters.\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_num_filters\",\n",
    "        help=\"Number of filters for generator resnet decoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"128,64,3\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_kernel_sizes\",\n",
    "        help=\"Kernel sizes for generator resnet decoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"3,3,7\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_strides\",\n",
    "        help=\"Strides for generator resnet decoder conv layers.\",\n",
    "        type=str,\n",
    "        default=\"2,2,1\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_downsample\",\n",
    "        help=\"Whether generator resnet decoder layers use batch norm.\",\n",
    "        type=str,\n",
    "        default=\"False,False,True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_pad_type\",\n",
    "        help=\"Generator resnet decoder padding type: constant, reflection, replication.\",\n",
    "        type=str,\n",
    "        default=\"reflection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_pad_constant\",\n",
    "        help=\"Generator resnet decoder constant padding constant.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_layer_norm_before_act\",\n",
    "        help=\"Whether generator resnet decoder layers have normalization before activation.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_dropout_before_act\",\n",
    "        help=\"Whether generator resnet decoder layers have dropout before activation.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_dropout_before_norm\",\n",
    "        help=\"Whether generator resnet decoder layers have dropout before normalization.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_layer_norm_type\",\n",
    "        help=\"Generator resnet decoder layer normalization types.\",\n",
    "        type=str,\n",
    "        default=\"instance,instance,instance\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_dropout_rates\",\n",
    "        help=\"Generator resnet decoder layer dropout rates.\",\n",
    "        type=str,\n",
    "        default=\"0.,0.,0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_activation\",\n",
    "        help=\"Whether generator decoder layers use leaky relu activations.\",\n",
    "        type=str,\n",
    "        default=\"relu,relu,tanh\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_resnet_dec_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator decoder's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "\n",
    "    # Discriminator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_num_filters\",\n",
    "        help=\"Number of filters for discriminator conv layers.\",\n",
    "        type=str,\n",
    "        default=\"64,128,256,512,512,1\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_kernel_sizes\",\n",
    "        help=\"Kernel sizes for discriminator conv layers.\",\n",
    "        type=str,\n",
    "        default=\"4,4,4,4,4,4\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_strides\",\n",
    "        help=\"Strides for discriminator conv layers.\",\n",
    "        type=str,\n",
    "        default=\"2,2,2,2,1,1\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_downsample\",\n",
    "        help=\"Whether discriminator layers use batch norm.\",\n",
    "        type=str,\n",
    "        default=\"True,True,True,True,True,True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_pad_type\",\n",
    "        help=\"Discriminator padding type: constant, reflection, replication.\",\n",
    "        type=str,\n",
    "        default=\"reflection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_pad_constant\",\n",
    "        help=\"Discriminator constant padding constant.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_layer_norm_before_act\",\n",
    "        help=\"Whether discriminator layers have normalization before activation.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_dropout_before_act\",\n",
    "        help=\"Whether discriminator layers have dropout before activation.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_dropout_before_norm\",\n",
    "        help=\"Whether discriminator layers have dropout before normalization.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_layer_norm_type\",\n",
    "        help=\"Discriminator layer normalization types.\",\n",
    "        type=str,\n",
    "        default=\"None,instance,instance,instance,instance,None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_dropout_rates\",\n",
    "        help=\"Discriminator layer dropout rates.\",\n",
    "        type=str,\n",
    "        default=\"0.,0.,0.,0.,0.,0.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_activation\",\n",
    "        help=\"Whether discriminator layers use leaky relu activations.\",\n",
    "        type=str,\n",
    "        default=\"leaky_relu,leaky_relu,leaky_relu,leaky_relu,leaky_relu,None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of discriminator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_optimizer\",\n",
    "        help=\"Name of optimizer to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_decay_type\",\n",
    "        help=\"Decay type of learning rate. Constant, polynomial, exponential, piecewise polynomial.\",\n",
    "        type=str,\n",
    "        default=\"piecewise_polynomial\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_constant_steps\",\n",
    "        help=\"Number of steps to keep learning rate constant.\",\n",
    "        type=int,\n",
    "        default=100 * 1096\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_decay_steps\",\n",
    "        help=\"Number of steps to keep decay learning rate.\",\n",
    "        type=int,\n",
    "        default=100 * 1096\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_end_learning_rate\",\n",
    "        help=\"Minimum learning rate to decay to.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_power\",\n",
    "        help=\"Polynomial decay power.\",\n",
    "        type=float,\n",
    "        default=1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_cycle\",\n",
    "        help=\"Whether to cycle learning rate for polynomial decay types.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_decay_rate\",\n",
    "        help=\"Learning rate decay rate.\",\n",
    "        type=float,\n",
    "        default=0.99\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_staircase\",\n",
    "        help=\"Whether to staircase learning rate for exponential decay types.\",\n",
    "        type=str,\n",
    "        default=\"False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate_alpha\",\n",
    "        help=\"Minimum learning rate value as a fraction of learning rate for cosine decay types.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_clip_gradients_by_value\",\n",
    "        help=\"Clipping to prevent gradient to exceed this value for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_clip_gradients_global_norm\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_train_steps\",\n",
    "        help=\"Number of steps to train discriminator for per cycle.\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "\n",
    "    # Parse all arguments.\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service.\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Fix input_fn_autotune.\n",
    "    arguments[\"input_fn_autotune\"] = convert_string_to_bool(\n",
    "        string=arguments[\"input_fn_autotune\"]\n",
    "    )\n",
    "\n",
    "    # Fix use_least_squares_loss.\n",
    "    arguments[\"use_least_squares_loss\"] = convert_string_to_bool(\n",
    "        string=arguments[\"use_least_squares_loss\"]\n",
    "    )\n",
    "\n",
    "    # Fix eval steps.\n",
    "    arguments[\"eval_steps\"] = convert_string_to_none_or_int(\n",
    "        string=arguments[\"eval_steps\"])\n",
    "\n",
    "    # Fix preprocess_image_resize_jitter_size.\n",
    "    arguments[\"preprocess_image_resize_jitter_size\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"preprocess_image_resize_jitter_size\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix preprocess_image_use_random_mirroring.\n",
    "    arguments[\"preprocess_image_use_random_mirroring\"] = convert_string_to_bool(\n",
    "        string=arguments[\"preprocess_image_use_random_mirroring\"]\n",
    "    )\n",
    "\n",
    "    # Fix generator_use_unet.\n",
    "    arguments[\"generator_use_unet\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_use_unet\"]\n",
    "    )\n",
    "\n",
    "    # Fix learning_rate_cycle.\n",
    "    arguments[\"generator_learning_rate_cycle\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_learning_rate_cycle\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_learning_rate_cycle\"] = convert_string_to_bool(\n",
    "        string=arguments[\"discriminator_learning_rate_cycle\"]\n",
    "    )\n",
    "\n",
    "    # Fix learning_rate_staircase.\n",
    "    arguments[\"generator_learning_rate_staircase\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_learning_rate_staircase\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_learning_rate_staircase\"] = convert_string_to_bool(\n",
    "        string=arguments[\"discriminator_learning_rate_staircase\"]\n",
    "    )\n",
    "\n",
    "    # Fix num_filters.\n",
    "    arguments[\"generator_unet_encoder_num_filters\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_unet_encoder_num_filters\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_num_filters\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_unet_decoder_num_filters\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_num_filters\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_enc_num_filters\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_num_filters\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_res_num_filters\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_num_filters\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_dec_num_filters\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_num_filters\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"discriminator_num_filters\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix kernel_sizes.\n",
    "    arguments[\"generator_unet_encoder_kernel_sizes\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_unet_encoder_kernel_sizes\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_kernel_sizes\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_unet_decoder_kernel_sizes\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_kernel_sizes\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_enc_kernel_sizes\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_kernel_sizes\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_res_kernel_sizes\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_kernel_sizes\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_dec_kernel_sizes\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_kernel_sizes\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"discriminator_kernel_sizes\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix strides.\n",
    "    arguments[\"generator_unet_encoder_strides\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_unet_encoder_strides\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_strides\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_unet_decoder_strides\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_strides\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_enc_strides\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_strides\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_res_strides\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_strides\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_resnet_dec_strides\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_strides\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"discriminator_strides\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix downsample.\n",
    "    arguments[\"generator_unet_encoder_downsample\"] = convert_string_to_list_of_bools(\n",
    "        string=arguments[\"generator_unet_encoder_downsample\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_downsample\"] = convert_string_to_list_of_bools(\n",
    "        string=arguments[\"generator_unet_decoder_downsample\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_downsample\"] = convert_string_to_list_of_bools(\n",
    "        string=arguments[\"generator_resnet_enc_downsample\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_downsample\"] = convert_string_to_list_of_bools(\n",
    "        string=arguments[\"generator_resnet_res_downsample\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_downsample\"] = convert_string_to_list_of_bools(\n",
    "        string=arguments[\"generator_resnet_dec_downsample\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_downsample\"] = convert_string_to_list_of_bools(\n",
    "        string=arguments[\"discriminator_downsample\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix layer_norm_before_act.\n",
    "    arguments[\"generator_unet_encoder_layer_norm_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_unet_encoder_layer_norm_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_layer_norm_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_unet_decoder_layer_norm_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_layer_norm_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_enc_layer_norm_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_layer_norm_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_res_layer_norm_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_layer_norm_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_dec_layer_norm_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_encoder_layer_norm_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"discriminator_layer_norm_before_act\"]\n",
    "    )\n",
    "\n",
    "    # Fix dropout_before_act.\n",
    "    arguments[\"generator_unet_encoder_dropout_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_unet_encoder_dropout_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_dropout_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_unet_decoder_dropout_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_dropout_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_enc_dropout_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_dropout_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_res_dropout_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_dropout_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_dec_dropout_before_act\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_encoder_dropout_before_act\"] = convert_string_to_bool(\n",
    "        string=arguments[\"discriminator_dropout_before_act\"]\n",
    "    )\n",
    "\n",
    "    # Fix dropout_before_norm.\n",
    "    arguments[\"generator_unet_encoder_dropout_before_norm\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_unet_encoder_dropout_before_norm\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_dropout_before_norm\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_unet_decoder_dropout_before_norm\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_dropout_before_norm\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_enc_dropout_before_norm\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_dropout_before_norm\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_res_dropout_before_norm\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_dropout_before_norm\"] = convert_string_to_bool(\n",
    "        string=arguments[\"generator_resnet_dec_dropout_before_norm\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_encoder_dropout_before_norm\"] = convert_string_to_bool(\n",
    "        string=arguments[\"discriminator_dropout_before_norm\"]\n",
    "    )\n",
    "\n",
    "    # Fix layer_norm_type.\n",
    "    arguments[\"generator_unet_encoder_layer_norm_type\"] = arguments[\"generator_unet_encoder_layer_norm_type\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_unet_decoder_layer_norm_type\"] = arguments[\"generator_unet_decoder_layer_norm_type\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_resnet_enc_layer_norm_type\"] = arguments[\"generator_resnet_enc_layer_norm_type\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_resnet_res_layer_norm_type\"] = arguments[\"generator_resnet_res_layer_norm_type\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_resnet_dec_layer_norm_type\"] = arguments[\"generator_resnet_dec_layer_norm_type\"].split(\",\")\n",
    "\n",
    "    arguments[\"discriminator_layer_norm_type\"] = arguments[\"discriminator_layer_norm_type\"].split(\",\")\n",
    "\n",
    "    # Fix dropout_rates.\n",
    "    arguments[\"generator_unet_encoder_dropout_rates\"] = convert_string_to_list_of_floats(\n",
    "        string=arguments[\"generator_unet_encoder_dropout_rates\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_unet_decoder_dropout_rates\"] = convert_string_to_list_of_floats(\n",
    "        string=arguments[\"generator_unet_decoder_dropout_rates\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_enc_dropout_rates\"] = convert_string_to_list_of_floats(\n",
    "        string=arguments[\"generator_resnet_enc_dropout_rates\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_res_dropout_rates\"] = convert_string_to_list_of_floats(\n",
    "        string=arguments[\"generator_resnet_res_dropout_rates\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"generator_resnet_dec_dropout_rates\"] = convert_string_to_list_of_floats(\n",
    "        string=arguments[\"generator_resnet_dec_dropout_rates\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_dropout_rates\"] = convert_string_to_list_of_floats(\n",
    "        string=arguments[\"discriminator_dropout_rates\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix activation.\n",
    "    arguments[\"generator_unet_encoder_activation\"] = arguments[\"generator_unet_encoder_activation\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_unet_decoder_activation\"] = arguments[\"generator_unet_decoder_activation\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_resnet_enc_activation\"] = arguments[\"generator_resnet_enc_activation\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_resnet_res_activation\"] = arguments[\"generator_resnet_res_activation\"].split(\",\")\n",
    "\n",
    "    arguments[\"generator_resnet_dec_activation\"] = arguments[\"generator_resnet_dec_activation\"].split(\",\")\n",
    "\n",
    "    arguments[\"discriminator_activation\"] = arguments[\"discriminator_activation\"].split(\",\")\n",
    "\n",
    "    # Fix clip_gradients_by_value.\n",
    "    arguments[\"generator_clip_gradients_by_value\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"generator_clip_gradients_by_value\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_clip_gradients_by_value\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"discriminator_clip_gradients_by_value\"]\n",
    "    )\n",
    "\n",
    "    # Fix clip_gradients_global_norm.\n",
    "    arguments[\"generator_clip_gradients_global_norm\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"generator_clip_gradients_global_norm\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_clip_gradients_global_norm\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"discriminator_clip_gradients_global_norm\"]\n",
    "    )\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning.\n",
    "    # This code can be removed if you are not using hyperparameter tuning.\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\n",
    "                \"TF_CONFIG\", \"{}\"\n",
    "            )\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\"))\n",
    "\n",
    "    # Run the training job.\n",
    "    model.train_and_evaluate(arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
