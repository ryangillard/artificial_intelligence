{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2-dlenv_tfe\n",
      "1.18.1\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "arguments[\"train_steps\"] = 200\n",
    "arguments[\"eval_steps\"] = 100\n",
    "\n",
    "# Eval parameters\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120\n",
    "\n",
    "# Image parameters\n",
    "arguments[\"height\"] = 32\n",
    "arguments[\"width\"] = 32\n",
    "arguments[\"depth\"] = 3\n",
    "\n",
    "# Generator parameters\n",
    "arguments[\"latent_size\"] = 512\n",
    "arguments[\"generator_projection_dims\"] = [8, 8, 256]\n",
    "arguments[\"generator_num_filters\"] = [128, 64]\n",
    "arguments[\"generator_kernel_sizes\"] = [5, 5]\n",
    "arguments[\"generator_strides\"] = [1, 2]\n",
    "arguments[\"generator_final_num_filters\"] = 3\n",
    "arguments[\"generator_final_kernel_size\"] = 5\n",
    "arguments[\"generator_final_stride\"] = 2\n",
    "arguments[\"generator_l1_regularization_scale\"] = 0.01\n",
    "arguments[\"generator_l2_regularization_scale\"] = 0.01\n",
    "arguments[\"generator_learning_rate\"] = 0.0001\n",
    "arguments[\"generator_optimizer\"] = \"Adam\"\n",
    "arguments[\"generator_clip_gradients\"] = 5.0\n",
    "arguments[\"generator_train_steps\"] = 40\n",
    "\n",
    "# Discriminator hyperparameters\n",
    "arguments[\"discriminator_num_filters\"] = [64, 128]\n",
    "arguments[\"discriminator_kernel_sizes\"] = [5, 5]\n",
    "arguments[\"discriminator_strides\"] = [2, 2]\n",
    "arguments[\"discriminator_dropout_rates\"] = [0.3, 0.3]\n",
    "arguments[\"discriminator_l1_regularization_scale\"] = 0.01\n",
    "arguments[\"discriminator_l2_regularization_scale\"] = 0.01\n",
    "arguments[\"discriminator_learning_rate\"] = 0.0001\n",
    "arguments[\"discriminator_optimizer\"] = \"Adam\"\n",
    "arguments[\"discriminator_clip_gradients\"] = 5.0\n",
    "arguments[\"discriminator_train_steps\"] = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_obj(function_name, object_name, object_value):\n",
    "    \"\"\"Prints enclosing function, object name, and object value.\n",
    "\n",
    "    Args:\n",
    "        function_name: str, name of function.\n",
    "        object_name: str, name of object.\n",
    "        object_value: object, value of passed object.\n",
    "    \"\"\"\n",
    "#     pass\n",
    "    print(\"{}: {} = {}\".format(function_name, object_name, object_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "    print_obj(\"\\ndecode_example\", \"features\", features)\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "    print_obj(\"decode_example\", \"label\", label)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    \"\"\"Reads CSV time series data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read CSV dataset\n",
    "    using Dataset API, apply necessary preprocessing, and return an input\n",
    "    function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        mode: The estimator ModeKeys. Can be TRAIN or EVAL.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create list of files that match pattern.\n",
    "        file_list = tf.gfile.Glob(filename=filename)\n",
    "\n",
    "        # Create dataset from file list.\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=file_list, num_parallel_reads=40\n",
    "        )\n",
    "\n",
    "        # Shuffle and repeat if training with fused op.\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(\n",
    "                    buffer_size=50 * batch_size,\n",
    "                    count=None  # indefinitely\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decode CSV file into a features dictionary of tensors, then batch.\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                map_func=lambda x: decode_example(\n",
    "                    protos=x,\n",
    "                    params=params\n",
    "                ),\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=4\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(buffer_size=2)\n",
    "\n",
    "        # Create a iterator, then get batch of features from example queue.\n",
    "        batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return batched_dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_network(Z, mode, params, reuse=False):\n",
    "    \"\"\"Creates generator network and returns generated output.\n",
    "\n",
    "    Args:\n",
    "        Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "        reuse: bool, whether to reuse variables or not.\n",
    "\n",
    "    Returns:\n",
    "        Generated outputs tensor of shape\n",
    "            [cur_batch_size, height * width * depth].\n",
    "    \"\"\"\n",
    "    # Create regularizer for dense layer kernel weights.\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "        scale_l1=params[\"generator_l1_regularization_scale\"],\n",
    "        scale_l2=params[\"generator_l2_regularization_scale\"]\n",
    "    )\n",
    "\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        # Project latent vectors.\n",
    "        projection_height = params[\"generator_projection_dims\"][0]\n",
    "        projection_width = params[\"generator_projection_dims\"][1]\n",
    "        projection_depth = params[\"generator_projection_dims\"][2]\n",
    "\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height * projection_width * projection_depth\n",
    "        # )\n",
    "        projection = tf.layers.dense(\n",
    "            inputs=Z,\n",
    "            units=projection_height * projection_width * projection_depth,\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"projection_layer\"\n",
    "        )\n",
    "        print_obj(\"generator_network\", \"projection\", projection)\n",
    "\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height * projection_width * projection_depth\n",
    "        # )\n",
    "        projection_batch_norm = tf.layers.batch_normalization(\n",
    "            inputs=projection,\n",
    "            training=(mode == tf.estimator.ModeKeys.TRAIN),\n",
    "            name=\"projection_batch_norm\"\n",
    "        )\n",
    "        print_obj(\n",
    "            \"generator_network\", \"projection_batch_norm\", projection_batch_norm\n",
    "        )\n",
    "\n",
    "        # Reshape projection into \"image\".\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height,\n",
    "        #     projection_width,\n",
    "        #     projection_depth\n",
    "        # )\n",
    "        network = tf.reshape(\n",
    "            tensor=projection_batch_norm,\n",
    "            shape=[-1, projection_height, projection_width, projection_depth],\n",
    "            name=\"projection_reshaped\"\n",
    "        )\n",
    "        print_obj(\"generator_network\", \"network\", network)\n",
    "\n",
    "        # Iteratively build upsampling layers.\n",
    "        for i in range(len(params[\"generator_num_filters\"])):\n",
    "            # Add convolutional transpose layers with given params per layer.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     generator_kernel_sizes[i - 1] * generator_strides[i],\n",
    "            #     generator_kernel_sizes[i - 1] * generator_strides[i],\n",
    "            #     generator_num_filters[i]\n",
    "            # )\n",
    "            network = tf.layers.conv2d_transpose(\n",
    "                inputs=network,\n",
    "                filters=params[\"generator_num_filters\"][i],\n",
    "                kernel_size=params[\"generator_kernel_sizes\"][i],\n",
    "                strides=params[\"generator_strides\"][i],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                use_bias=False,\n",
    "                kernel_regularizer=regularizer,\n",
    "                name=\"layers_conv2d_tranpose_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"generator_network\", \"network\", network)\n",
    "\n",
    "            # Add batch normalization to keep the inputs from blowing up.\n",
    "            network = tf.layers.batch_normalization(\n",
    "                inputs=network,\n",
    "                training=(mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                name=\"layers_batch_norm_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"generator_network\", \"network\", network)\n",
    "\n",
    "        # Final conv2d transpose layer for image output.\n",
    "        # shape = (cur_batch_size, height * width * depth)\n",
    "        generated_outputs = tf.layers.conv2d_transpose(\n",
    "                inputs=network,\n",
    "                filters=params[\"generator_final_num_filters\"],\n",
    "                kernel_size=params[\"generator_final_kernel_size\"],\n",
    "                strides=params[\"generator_final_stride\"],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.tanh,\n",
    "                use_bias=False,\n",
    "                kernel_regularizer=regularizer,\n",
    "                name=\"layers_conv2d_tranpose_generated_outputs\"\n",
    "        )\n",
    "        print_obj(\"generator_network\", \"generated_outputs\", generated_outputs)\n",
    "\n",
    "    return generated_outputs\n",
    "\n",
    "\n",
    "def get_generator_loss(generated_logits):\n",
    "    \"\"\"Gets generator loss.\n",
    "\n",
    "    Args:\n",
    "        generated_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "\n",
    "    Returns:\n",
    "        Tensor of generator's total loss of shape [].\n",
    "    \"\"\"\n",
    "    # Calculate base generator loss.\n",
    "    generator_loss = tf.reduce_mean(\n",
    "        input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=generated_logits,\n",
    "            labels=tf.ones_like(tensor=generated_logits)\n",
    "        ),\n",
    "        name=\"generator_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\nget_generator_loss\",\n",
    "        \"generator_loss\",\n",
    "        generator_loss\n",
    "    )\n",
    "\n",
    "    # Get regularization losses.\n",
    "    generator_regularization_loss = tf.losses.get_regularization_loss(\n",
    "        scope=\"generator\",\n",
    "        name=\"generator_regularization_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_generator_loss\",\n",
    "        \"generator_regularization_loss\",\n",
    "        generator_regularization_loss\n",
    "    )\n",
    "\n",
    "    # Combine losses for total losses.\n",
    "    generator_total_loss = tf.math.add(\n",
    "        x=generator_loss,\n",
    "        y=generator_regularization_loss,\n",
    "        name=\"generator_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_generator_loss\", \"generator_total_loss\", generator_total_loss\n",
    "    )\n",
    "\n",
    "    return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_network(X, params, reuse=False):\n",
    "    \"\"\"Creates discriminator network and returns logits.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, image tensors of shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        params: dict, user passed parameters.\n",
    "        reuse: bool, whether to reuse variables or not.\n",
    "\n",
    "    Returns:\n",
    "        Logits tensor of shape [cur_batch_size, 1].\n",
    "    \"\"\"\n",
    "    # Create the input layer to our DNN.\n",
    "    # shape = (cur_batch_size, height * width * depth)\n",
    "    network = X\n",
    "    print_obj(\"\\ndiscriminator_network\", \"network\", network)\n",
    "\n",
    "    # Create regularizer for dense layer kernel weights.\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "        scale_l1=params[\"discriminator_l1_regularization_scale\"],\n",
    "        scale_l2=params[\"discriminator_l2_regularization_scale\"]\n",
    "    )\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        # Iteratively build downsampling layers.\n",
    "        for i in range(len(params[\"discriminator_num_filters\"])):\n",
    "            # Add convolutional transpose layers with given params per layer.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     discriminator_kernel_sizes[i - 1] / discriminator_strides[i],\n",
    "            #     discriminator_kernel_sizes[i - 1] / discriminator_strides[i],\n",
    "            #     discriminator_num_filters[i]\n",
    "            # )\n",
    "            network = tf.layers.conv2d(\n",
    "                inputs=network,\n",
    "                filters=params[\"discriminator_num_filters\"][i],\n",
    "                kernel_size=params[\"discriminator_kernel_sizes\"][i],\n",
    "                strides=params[\"discriminator_strides\"][i],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_regularizer=regularizer,\n",
    "                name=\"layers_conv2d_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"discriminator_network\", \"network\", network)\n",
    "\n",
    "            # Add some dropout for better regularization and stability.\n",
    "            network = tf.layers.dropout(\n",
    "                inputs=network,\n",
    "                rate=params[\"discriminator_dropout_rates\"][i],\n",
    "                name=\"layers_dropout_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"discriminator_network\", \"network\", network)\n",
    "\n",
    "        # Flatten network output.\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     (discriminator_kernel_sizes[-2] / discriminator_strides[-1]) ** 2 * discriminator_num_filters[-1]\n",
    "        # )\n",
    "        network_flat = tf.layers.Flatten()(inputs=network)\n",
    "        print_obj(\"discriminator_network\", \"network_flat\", network_flat)\n",
    "\n",
    "        # Final linear layer for logits.\n",
    "        # shape = (cur_batch_size, 1)\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=network_flat,\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=\"layers_dense_logits\"\n",
    "        )\n",
    "        print_obj(\"discriminator_network\", \"logits\", logits)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def get_discriminator_loss(generated_logits, real_logits):\n",
    "    \"\"\"Gets discriminator loss.\n",
    "\n",
    "    Args:\n",
    "        generated_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "        real_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "\n",
    "    Returns:\n",
    "        Tensor of discriminator's total loss of shape [].\n",
    "    \"\"\"\n",
    "    # Calculate base discriminator loss.\n",
    "    discriminator_real_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=real_logits,\n",
    "        labels=tf.ones_like(tensor=real_logits),\n",
    "        name=\"discriminator_real_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\nget_discriminator_loss\",\n",
    "        \"discriminator_real_loss\",\n",
    "        discriminator_real_loss\n",
    "    )\n",
    "\n",
    "    discriminator_generated_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=generated_logits,\n",
    "        labels=tf.zeros_like(tensor=generated_logits),\n",
    "        name=\"discriminator_generated_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_generated_loss\",\n",
    "        discriminator_generated_loss\n",
    "    )\n",
    "\n",
    "    discriminator_loss = tf.reduce_mean(\n",
    "        input_tensor=tf.add(\n",
    "            x=discriminator_real_loss, y=discriminator_generated_loss\n",
    "        ),\n",
    "        name=\"discriminator_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_loss\",\n",
    "        discriminator_loss\n",
    "    )\n",
    "\n",
    "    # Get regularization losses.\n",
    "    discriminator_regularization_loss = tf.losses.get_regularization_loss(\n",
    "        scope=\"discriminator\",\n",
    "        name=\"discriminator_regularization_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_regularization_loss\",\n",
    "        discriminator_regularization_loss\n",
    "    )\n",
    "\n",
    "    # Combine losses for total losses.\n",
    "    discriminator_total_loss = tf.math.add(\n",
    "        x=discriminator_loss,\n",
    "        y=discriminator_regularization_loss,\n",
    "        name=\"discriminator_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_discriminator_loss\",\n",
    "        \"discriminator_total_loss\",\n",
    "        discriminator_total_loss\n",
    "    )\n",
    "\n",
    "    return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dcgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(loss, global_step, params, scope):\n",
    "    \"\"\"Trains network and returns loss and train op.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the variables that to train.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor and training op.\n",
    "    \"\"\"\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adam\": tf.train.AdamOptimizer,\n",
    "        \"Adadelta\": tf.train.AdadeltaOptimizer,\n",
    "        \"AdagradDA\": tf.train.AdagradDAOptimizer,\n",
    "        \"Adagrad\": tf.train.AdagradOptimizer,\n",
    "        \"Ftrl\": tf.train.FtrlOptimizer,\n",
    "        \"GradientDescent\": tf.train.GradientDescentOptimizer,\n",
    "        \"Momentum\": tf.train.MomentumOptimizer,\n",
    "        \"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer,\n",
    "        \"ProximalGradientDescent\": tf.train.ProximalGradientDescentOptimizer,\n",
    "        \"RMSProp\": tf.train.RMSPropOptimizer\n",
    "    }\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=tf.trainable_variables(scope=scope),\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    # Clip gradients.\n",
    "    if params[\"{}_clip_gradients\".format(scope)]:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=params[\"{}_clip_gradients\".format(scope)],\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "\n",
    "    # Zip back together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, tf.trainable_variables(scope=scope))\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "        learning_rate=params[\"{}_learning_rate\".format(scope)]\n",
    "    )\n",
    "\n",
    "    # Create train op by applying gradients to variables and incrementing\n",
    "    # global step.\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        global_step=global_step,\n",
    "        name=\"{}_apply_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    return loss, train_op\n",
    "\n",
    "\n",
    "def dcgan_model(features, labels, mode, params):\n",
    "    \"\"\"Deep Convolutional GAN custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `tf.estimator.EstimatorSpec` class.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\ndcgan_model\", \"features\", features)\n",
    "    print_obj(\"dcgan_model\", \"labels\", labels)\n",
    "    print_obj(\"dcgan_model\", \"mode\", mode)\n",
    "    print_obj(\"dcgan_model\", \"params\", params)\n",
    "\n",
    "    # Loss function, training/eval ops, etc.\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Extract given latent vectors from features dictionary.\n",
    "        Z = tf.cast(x=features[\"Z\"], dtype=tf.float32)\n",
    "\n",
    "        # Get predictions from generator.\n",
    "        generated_images = generator_network(Z, mode, params, reuse=False)\n",
    "\n",
    "        # Create predictions dictionary.\n",
    "        predictions_dict = {\n",
    "            \"generated_images\": generated_images\n",
    "        }\n",
    "\n",
    "        # Create export outputs.\n",
    "        export_outputs = {\n",
    "            \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "                outputs=predictions_dict)\n",
    "        }\n",
    "    else:\n",
    "        # Extract image from features dictionary.\n",
    "        X = features[\"image\"]\n",
    "\n",
    "        # Get dynamic batch size in case of partial batch.\n",
    "        cur_batch_size = tf.shape(\n",
    "            input=X,\n",
    "            out_type=tf.int32,\n",
    "            name=\"dcgan_model_cur_batch_size\"\n",
    "        )[0]\n",
    "\n",
    "        # Create random noise latent vector for each batch example.\n",
    "        Z = tf.random.normal(\n",
    "            shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Establish generator network subgraph.\n",
    "        generator_outputs = generator_network(Z, mode, params, reuse=False)\n",
    "\n",
    "        # Establish discriminator network subgraph.\n",
    "        real_logits = discriminator_network(X, params, reuse=False)\n",
    "\n",
    "        # Get generated logits too.\n",
    "        generated_logits = discriminator_network(\n",
    "            generator_outputs, params, reuse=True\n",
    "        )\n",
    "\n",
    "        # Get generator total loss.\n",
    "        generator_total_loss = get_generator_loss(generated_logits)\n",
    "\n",
    "        # Get discriminator total loss.\n",
    "        discriminator_total_loss = get_discriminator_loss(\n",
    "            generated_logits, real_logits\n",
    "        )\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Get global step.\n",
    "            global_step = tf.train.get_global_step()\n",
    "\n",
    "            # Determine if it is time to train generator or discriminator.\n",
    "            cycle_step = tf.mod(\n",
    "                x=global_step,\n",
    "                y=tf.cast(\n",
    "                    x=tf.add(\n",
    "                        x=params[\"generator_train_steps\"],\n",
    "                        y=params[\"discriminator_train_steps\"]\n",
    "                    ),\n",
    "                    dtype=tf.int64\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create choose generator condition.\n",
    "            condition = tf.less(\n",
    "                x=cycle_step, y=params[\"generator_train_steps\"]\n",
    "            )\n",
    "\n",
    "            # Needed for batch normalization, but has no effect otherwise.\n",
    "            update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(control_inputs=update_ops):\n",
    "                # Conditionally choose to train generator or discriminator.\n",
    "                loss, train_op = tf.cond(\n",
    "                    pred=condition,\n",
    "                    true_fn=lambda: train_network(\n",
    "                        loss=generator_total_loss,\n",
    "                        global_step=global_step,\n",
    "                        params=params,\n",
    "                        scope=\"generator\"\n",
    "                    ),\n",
    "                    false_fn=lambda: train_network(\n",
    "                        loss=discriminator_total_loss,\n",
    "                        global_step=global_step,\n",
    "                        params=params,\n",
    "                        scope=\"discriminator\"\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            loss = discriminator_total_loss\n",
    "\n",
    "            # Concatenate discriminator logits and labels.\n",
    "            discriminator_logits = tf.concat(\n",
    "                values=[real_logits, generated_logits],\n",
    "                axis=0,\n",
    "                name=\"discriminator_concat_logits\"\n",
    "            )\n",
    "\n",
    "            discriminator_labels = tf.concat(\n",
    "                values=[\n",
    "                    tf.ones_like(tensor=real_logits),\n",
    "                    tf.zeros_like(tensor=generated_logits)\n",
    "                ],\n",
    "                axis=0,\n",
    "                name=\"discriminator_concat_labels\"\n",
    "            )\n",
    "\n",
    "            # Calculate discriminator probabilities.\n",
    "            discriminator_probabilities = tf.nn.sigmoid(\n",
    "                x=discriminator_logits, name=\"discriminator_probabilities\"\n",
    "            )\n",
    "\n",
    "            # Create eval metric ops dictionary.\n",
    "            eval_metric_ops = {\n",
    "                \"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    name=\"dcgan_model_accuracy\"\n",
    "                ),\n",
    "                \"precision\": tf.metrics.precision(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    name=\"dcgan_model_precision\"\n",
    "                ),\n",
    "                \"recall\": tf.metrics.recall(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    name=\"dcgan_model_recall\"\n",
    "                ),\n",
    "                \"auc_roc\": tf.metrics.auc(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    num_thresholds=200,\n",
    "                    curve=\"ROC\",\n",
    "                    name=\"dcgan_model_auc_roc\"\n",
    "                ),\n",
    "                \"auc_pr\": tf.metrics.auc(\n",
    "                    labels=discriminator_labels,\n",
    "                    predictions=discriminator_probabilities,\n",
    "                    num_thresholds=200,\n",
    "                    curve=\"PR\",\n",
    "                    name=\"dcgan_model_auc_pr\"\n",
    "                )\n",
    "            }\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs=export_outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn(params):\n",
    "    \"\"\"Serving input function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        ServingInputReceiver object containing features and receiver tensors.\n",
    "    \"\"\"\n",
    "    # Create placeholders to accept data sent to the model at serving time.\n",
    "    # shape = (batch_size,)\n",
    "    feature_placeholders = {\n",
    "        \"Z\": tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, params[\"latent_size\"]],\n",
    "            name=\"serving_input_placeholder_Z\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"feature_placeholders\",\n",
    "        feature_placeholders\n",
    "    )\n",
    "\n",
    "    # Create clones of the feature placeholder tensors so that the SavedModel\n",
    "    # SignatureDef will point to the placeholder.\n",
    "    features = {\n",
    "        key: tf.identity(\n",
    "            input=value,\n",
    "            name=\"serving_input_fn_identity_placeholder_{}\".format(key)\n",
    "        )\n",
    "        for key, value in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"features\",\n",
    "        features\n",
    "    )\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=feature_placeholders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates custom Estimator model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        `Estimator` object.\n",
    "    \"\"\"\n",
    "    # Set logging to be level of INFO.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Create our custom estimator using our model function.\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=dcgan_model,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        params=args\n",
    "    )\n",
    "\n",
    "    # Create train spec to read in our training data.\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset(\n",
    "            filename=\"data/train.tfrecord\",\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size=args[\"train_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        max_steps=args[\"train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Create exporter to save out the complete model to disk.\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name=\"exporter\",\n",
    "        serving_input_receiver_fn=lambda: serving_input_fn(args)\n",
    "    )\n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model.\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset(\n",
    "            filename=\"data/test.tfrecord\",\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size=args[\"eval_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        steps=args[\"eval_steps\"],\n",
    "        start_delay_secs=args[\"start_delay_secs\"],\n",
    "        throttle_secs=args[\"throttle_secs\"],\n",
    "        exporters=exporter\n",
    "    )\n",
    "\n",
    "    # Create train and evaluate loop to train and evaluate our estimator.\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5d498f0950>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:87: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/shuffle_ops.py:54: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:99: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "\n",
      "decode_example: features = {'image_raw': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "decode_example: image = Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "decode_example: image = Tensor(\"Reshape:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "decode_example: image = Tensor(\"sub:0\", shape=(32, 32, 3), dtype=float32)\n",
      "decode_example: label = Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:107: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "dcgan_model: features = {'image': <tf.Tensor 'IteratorGetNext:0' shape=(?, 32, 32, 3) dtype=float32>}\n",
      "dcgan_model: labels = Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "dcgan_model: mode = train\n",
      "dcgan_model: params = {'output_dir': 'trained_model', 'train_batch_size': 32, 'eval_batch_size': 32, 'train_steps': 200, 'eval_steps': 100, 'start_delay_secs': 60, 'throttle_secs': 120, 'height': 32, 'width': 32, 'depth': 3, 'latent_size': 512, 'generator_projection_dims': [8, 8, 256], 'generator_num_filters': [128, 64], 'generator_kernel_sizes': [5, 5], 'generator_strides': [1, 2], 'generator_final_num_filters': 3, 'generator_final_kernel_size': 5, 'generator_final_stride': 2, 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_learning_rate': 0.0001, 'generator_optimizer': 'Adam', 'generator_clip_gradients': 5.0, 'generator_train_steps': 40, 'discriminator_num_filters': [64, 128], 'discriminator_kernel_sizes': [5, 5], 'discriminator_strides': [2, 2], 'discriminator_dropout_rates': [0.3, 0.3], 'discriminator_l1_regularization_scale': 0.01, 'discriminator_l2_regularization_scale': 0.01, 'discriminator_learning_rate': 0.0001, 'discriminator_optimizer': 'Adam', 'discriminator_clip_gradients': 5.0, 'discriminator_train_steps': 25}\n",
      "WARNING:tensorflow:From <ipython-input-5-07ca92871957>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "generator_network: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 16384), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-07ca92871957>:46: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "generator_network: projection_batch_norm = Tensor(\"generator/projection_batch_norm/batchnorm/add_1:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/projection_reshaped:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-07ca92871957>:84: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_0/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_0/FusedBatchNormV3:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_1/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_1/FusedBatchNormV3:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "\n",
      "discriminator_network: network = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:From <ipython-input-6-2d271bae533b>:42: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-6-2d271bae533b>:50: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network_flat = Tensor(\"discriminator/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "discriminator_network: logits = Tensor(\"discriminator/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "discriminator_network: network = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network_flat = Tensor(\"discriminator_1/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "discriminator_network: logits = Tensor(\"discriminator_1/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n",
      "get_generator_loss: generator_loss = Tensor(\"generator_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_regularization_loss = Tensor(\"generator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_total_loss = Tensor(\"generator_total_loss:0\", shape=(), dtype=float32)\n",
      "\n",
      "get_discriminator_loss: discriminator_real_loss = Tensor(\"discriminator_real_loss:0\", shape=(?, 1), dtype=float32)\n",
      "get_discriminator_loss: discriminator_generated_loss = Tensor(\"discriminator_generated_loss:0\", shape=(?, 1), dtype=float32)\n",
      "get_discriminator_loss: discriminator_loss = Tensor(\"discriminator_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_regularization_loss = Tensor(\"discriminator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_total_loss = Tensor(\"discriminator_total_loss:0\", shape=(), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 142.01509, step = 1\n",
      "INFO:tensorflow:global_step/sec: 6.13215\n",
      "INFO:tensorflow:loss = 75.225655, step = 101 (16.314 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into trained_model/model.ckpt.\n",
      "\n",
      "decode_example: features = {'image_raw': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "decode_example: image = Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "decode_example: image = Tensor(\"Reshape:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "decode_example: image = Tensor(\"sub:0\", shape=(32, 32, 3), dtype=float32)\n",
      "decode_example: label = Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "dcgan_model: features = {'image': <tf.Tensor 'IteratorGetNext:0' shape=(?, 32, 32, 3) dtype=float32>}\n",
      "dcgan_model: labels = Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "dcgan_model: mode = eval\n",
      "dcgan_model: params = {'output_dir': 'trained_model', 'train_batch_size': 32, 'eval_batch_size': 32, 'train_steps': 200, 'eval_steps': 100, 'start_delay_secs': 60, 'throttle_secs': 120, 'height': 32, 'width': 32, 'depth': 3, 'latent_size': 512, 'generator_projection_dims': [8, 8, 256], 'generator_num_filters': [128, 64], 'generator_kernel_sizes': [5, 5], 'generator_strides': [1, 2], 'generator_final_num_filters': 3, 'generator_final_kernel_size': 5, 'generator_final_stride': 2, 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_learning_rate': 0.0001, 'generator_optimizer': 'Adam', 'generator_clip_gradients': 5.0, 'generator_train_steps': 40, 'discriminator_num_filters': [64, 128], 'discriminator_kernel_sizes': [5, 5], 'discriminator_strides': [2, 2], 'discriminator_dropout_rates': [0.3, 0.3], 'discriminator_l1_regularization_scale': 0.01, 'discriminator_l2_regularization_scale': 0.01, 'discriminator_learning_rate': 0.0001, 'discriminator_optimizer': 'Adam', 'discriminator_clip_gradients': 5.0, 'discriminator_train_steps': 25}\n",
      "generator_network: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: projection_batch_norm = Tensor(\"generator/projection_batch_norm/batchnorm/add_1:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/projection_reshaped:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_0/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_0/FusedBatchNormV3:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_1/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_1/FusedBatchNormV3:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "\n",
      "discriminator_network: network = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network_flat = Tensor(\"discriminator/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "discriminator_network: logits = Tensor(\"discriminator/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "discriminator_network: network = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network = Tensor(\"discriminator_1/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "discriminator_network: network_flat = Tensor(\"discriminator_1/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "discriminator_network: logits = Tensor(\"discriminator_1/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "get_generator_loss: generator_loss = Tensor(\"generator_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_regularization_loss = Tensor(\"generator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_total_loss = Tensor(\"generator_total_loss:0\", shape=(), dtype=float32)\n",
      "\n",
      "get_discriminator_loss: discriminator_real_loss = Tensor(\"discriminator_real_loss:0\", shape=(?, 1), dtype=float32)\n",
      "get_discriminator_loss: discriminator_generated_loss = Tensor(\"discriminator_generated_loss:0\", shape=(?, 1), dtype=float32)\n",
      "get_discriminator_loss: discriminator_loss = Tensor(\"discriminator_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_regularization_loss = Tensor(\"discriminator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_discriminator_loss: discriminator_total_loss = Tensor(\"discriminator_total_loss:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-06-01T05:36:41Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2020-06-01-05:36:48\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0, auc_pr = 0.99011195, auc_roc = 0.98031247, global_step = 200, loss = 25.865181, precision = 0.5, recall = 1.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: trained_model/model.ckpt-200\n",
      "serving_input_fn: feature_placeholders = {'Z': <tf.Tensor 'serving_input_placeholder_Z:0' shape=(?, 512) dtype=float32>}\n",
      "serving_input_fn: features = {'Z': <tf.Tensor 'serving_input_fn_identity_placeholder_Z:0' shape=(?, 512) dtype=float32>}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "dcgan_model: features = {'Z': <tf.Tensor 'serving_input_fn_identity_placeholder_Z:0' shape=(?, 512) dtype=float32>}\n",
      "dcgan_model: labels = None\n",
      "dcgan_model: mode = infer\n",
      "dcgan_model: params = {'output_dir': 'trained_model', 'train_batch_size': 32, 'eval_batch_size': 32, 'train_steps': 200, 'eval_steps': 100, 'start_delay_secs': 60, 'throttle_secs': 120, 'height': 32, 'width': 32, 'depth': 3, 'latent_size': 512, 'generator_projection_dims': [8, 8, 256], 'generator_num_filters': [128, 64], 'generator_kernel_sizes': [5, 5], 'generator_strides': [1, 2], 'generator_final_num_filters': 3, 'generator_final_kernel_size': 5, 'generator_final_stride': 2, 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_learning_rate': 0.0001, 'generator_optimizer': 'Adam', 'generator_clip_gradients': 5.0, 'generator_train_steps': 40, 'discriminator_num_filters': [64, 128], 'discriminator_kernel_sizes': [5, 5], 'discriminator_strides': [2, 2], 'discriminator_dropout_rates': [0.3, 0.3], 'discriminator_l1_regularization_scale': 0.01, 'discriminator_l2_regularization_scale': 0.01, 'discriminator_learning_rate': 0.0001, 'discriminator_optimizer': 'Adam', 'discriminator_clip_gradients': 5.0, 'discriminator_train_steps': 25}\n",
      "generator_network: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: projection_batch_norm = Tensor(\"generator/projection_batch_norm/batchnorm/add_1:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/projection_reshaped:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_0/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_0/FusedBatchNormV3:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_1/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_1/FusedBatchNormV3:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_model/export/exporter/temp-b'1590989809'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 43.4691.\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(path=arguments[\"output_dir\"], ignore_errors=True)\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590989809\n"
     ]
    }
   ],
   "source": [
    "!ls trained_model/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/export/exporter/1590989809/variables/variables\n"
     ]
    }
   ],
   "source": [
    "predict_fn = tf.contrib.predictor.from_saved_model(\n",
    "    \"trained_model/export/exporter/1590989809\"\n",
    ")\n",
    "predictions = predict_fn(\n",
    "    {\n",
    "        \"Z\": np.random.normal(size=(500, 512))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = np.clip(\n",
    "    a=((predictions[\"generated_images\"] + 1.0) * (255. / 2)).astype(np.int32),\n",
    "    a_min=0,\n",
    "    a_max=255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(generated_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAByCAYAAAC89bCsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO29ybIkSXYldm2efPY3x5hZVUABlCZF+Ets9AK94B9wTxFiy2HTFOEHccEWEIWqysyIjOENPrvNo3Jhnn6OJzO6EJJOguynZ2VhYW6mqveqmr179NxrKKVEQ0NDQ0NDQ+O5wPzXboCGhoaGhoaGxv+b0B8/GhoaGhoaGs8K+uNHQ0NDQ0ND41lBf/xoaGhoaGhoPCvojx8NDQ0NDQ2NZwX98aOhoaGhoaHxrGB/zcVhFKrJZCIiIiyQN1R3PFYmfU91uMqg80bXHo9bC+fNlu5qGnQ9zncWzvP9xcB5k6+ny38Og54hLfWB2sTnhfpw8gz+hOQHWvRT6ltLfTC+0GeTnts6/Y12663kSfqf6NG/HF+2Jf6lzF8ea+MLtmlpHEy6Kd+H+6vYlnTesOl884X2/CxFg2H8cltPn4ExNUwyTse2p5uS6Y0v9OHElt2XbEm+YveDtFtvJTuTLUV+pT15jBp0+qe2ipz2+XQu0/Vf8Au2wclYfMH+IiJi/fJc6MxfbpMinzGaL8xl9hn2VSxH/7K5yX22DnNzcz57hlGoJtPJ/63JX2tLo2G/+8JcO7EZrTnWX7b9ybz5wv1FTufmyXrxBb9g2xg0B0/Xcr6e3yG/7Csn74ovvIvUv5Ytac0xyBflxAb4j5N+ndz0l2156it0/xOf5vfZl9fZL65rX/BH+Zes/V9aN0/8kZ9LzePz7O8Oxujhw+elUupSfoav+viZTCbyd3//b0VEpCZPdKrieFyPfPwgqXBNFB6PrTg+HqcTXO/HNRrvO8djt6joepw3E1xveh7usy+Px0WALjYde5aIE+L/7C1+k4/RVnOf4zhCW90YbcoHZIACnmwMcOzuMEbJzMX5LfqgAtw/2GXH4/3NSERE/rf/4X+Uc2Eymci/+/u/ExGRmpzMrdGeaoB2qgzn7QjnnT3GIRlSf6vmeNyGsJm3hYNWZEtjh/uYM9jFWsMuVYDnSoXrRUTMAPa3yS+KMbl4jHvZYYRnJxjrmvpgZWirGeLZNtk+HeL+Ro7nWgH65lPfiovet/7DP/wvck5MJhP5t3//70REpKWF0mnInkP0wUiprUOMnbXBWMRT+KO3o7kWBcdjt8D8SMl/nRxj3YwwpoM1/CIm2/BcFhExB2iTt8fcyei8Tb6nJnR+gzY1Y7RJkW8bHtrkx/D//Qh2c8jOHc9N8pdyPhQRkf/wD/+TnAuT6UT+7t/362yL4RKbfL4Yoj12RnNnQH5Ha9p+jusNWotsWjd5HU/JJ9jf7QDzxoxpHC5oHuxO11nbQ5vsDG1KxrTG7+k3EV5cHp3PA9hMNWiTMcB67ZH/FvTOkQJ9U3Te3e+Px/V8LCIi/+s//M9yLkymE/lv/v3fHdoMP7NpXpa0hth7+hAgn3Y2yfE4H/5yv6wA89KOcX1J897M6OvHx3MDmn/5CM818tN1VtE66O3hnBWtmyol20Q0nza0Lk/RVqH5bY7o/jHmcUzzMkgxRk3E/o7nZlcYo//+v/3v3ssv4Ks+fpRhSmv1zu+QrzYX9OEhGPQqosG14WS7q+HxeObhRnsLHZ9TOOUxGByPB8HmeFyUuM9kgI5vXFoYDFo9fvYRGzZ4RjydHo9dD23tFN+LHOQaxnMUnm1Z5Mg0SCl9FA5pEvOHR1hiYUhfwHijsp/QluLP9l8Jw5TO7tsUVBiY9IKcz4DztSbaP7Jw/eYFro9yOPHWQvtfm7DBYj45Hrve+nhcVxhPk//KuRodj22LXna70+6M+BlXsGtIf7lkMyzc4xaL5NN0fjz2wxWeR30OaOyXl/QhRP7rUKTEt+iFOh0fjy2znx+GnNGW0kdcW68fw1GDsdhMMTfDDn90lC781G3x8b6+Q1vnLeZaObg6Hg+c7fE4poU4cnGfdos2mDRvkuvZ8TiwYcS6wxwXEfEdWlivaFGnOVzTB8yYfHh9C5+Z2bBPRgv/1ML5j5e4/kKhz1mEsZv7aM/OpZen9HPW+Pni8qtgSneYP0FNHzAztHMwTI/HFQUCXOrv5iWujxT83a7pZejg/ruIPghpoG36o9F14EPrAGvmyER78pD+ABaRoY11YUF/CA0DzOeqw7OHDvxoN8dYKwPXexV9OJvwx/SK5jIHSMjfQ6EPsGv0YWD0Y2Qa55ybpoj042HTGlKP6A8ID+2vHVpP6dWVXmHtGpgYh7pFv0YWbLC4wDw2PNjeJTrCtjAO2zl8xfNxvfGzT4QBfbPE9H73aD6ZBuaNGWHubweY+w75kUd9thqar5ewTeihb8rEc2eCd/X2Jfo87tCHL0Hv+dHQ0NDQ0NB4VtAfPxoaGhoaGhrPCl9Fe5ldJ0HahzCzAcJwslgcD6vBm+Nxt3k6Hi+n18fjYLs8HhczUFfuGpTZQihs1yE8bk+wb6nbIuR1r+6Ox176Z7R5cIPjDcKLIiIbCj36KcLdXYTfGAnauqd7+cUnHJvocxXf4/ohxsiN8exS3aIRMcZo7yD079xTKPeipxy6rzPXfxJGp8Q98LyZh+fWi4+4KITNLBqfNYUv3QVoomICamQW4/wnoglEHo5HjofxbFPyiRThzmGB8bGvMM67xeeT/mwm8KPhEiHVzgXNZtZo0zpCH6wa/tUK+mxucX7jIJxuLR+Px/4F/K5+xDzY0V4TM4afNi8uREREndGWIiJm00m06edmYsCvjQT+2I1eHI8t6tuaxs7/iOuzC9jBz+CPWxqjxoIdogBzs1kj7JxcET1CY2RNMQ/a5NSeuxrjGlTwgZbsMCAfW1IfjAXNtQA+2e5x/iNRM2EK+rW6u8B9TuyP9UgpPLeZ9GN6TnsanRL3sP+xIApfnjBHqhy2rFL4YzHDXPN/wDyor2muZWh/6cM2To7+WhFoECMHXfMpxLyZLGGzYk50E+3TExH5ROtg2GKNr1u0yVtjjiyI3rNS+Evo4/ouwfq7v8JYBORfrYc2GQVok2SC+aGesK7lk/7+Sp3Xll7Z81e5BVt2e5qXwcvjsZPAR3dTmpePuL6i91CTwfbvDZx317h+Ru/M7R7jo1z4ul1jXe5Gb3H+CedFRFZEE1r0HlfT18fjdPsO/Wkxj80d5rE1RZvUnv0Ra0uZfDgeBz7mcbXFfH2a4/7Wj+hbfI37fAk68qOhoaGhoaHxrKA/fjQ0NDQ0NDSeFb5S7WVIfdDPWwblVvARPosShJ7WJD++7RBqXJi43vyMcGc8QGj5UhAq3SrsGK8XJN0lFdhlSsoqoXA9UWnLAcKpIiJjC//OSHngLxHOi68xRJMYfa4thO2qFqHHJkSfZwq72HchSf2JDhS6/i5Eez5nGK/hUx8WNEnF82uhDCXNQankck4IF+HCwRpqr10I+mhI38x7QZuvdxjrR5KT3pa454YkGN4OYfbMw30mEYWrY9i1foRisZ0i1C0iMm/hLxtSa7woELZVTENWeN6WVBjXW9hyY8NPr0lhsHARjm53REWMYZ+LFj64jKBeGXzoqQizOpUD/1oo05D6IF01SU3o1KSu2iFcvA5gn8sCPrgbYlzGn0EVPE0RUr8UUm9tMAcNosa6Kc5frzBeG1JGTh5JKu2epuGYtaQYIcrJoj4sSEE5UuhPY5N6ZA9aQ5Gk+LrFuOzGpLB5jz4vfdxzbpB/mej/+BDKt9pTqf6vgiFi/KR4or5EAcZBMpZoo22zDPZbkpR5sn53PE4U5s7IgfoqL6DwcjqMw4bompsFfDkm2mT4AN/PSXElIvJbUmltSTlmrfGuSEiyHJG8feugb5MEc3kdwKeiFdamzgfVZ1KKlELgjy5tqShpLRs+9P5r1OdcZ0Xqg3LOcvEsx4L/2bQO5pSaZZpj3JMGdnUqzDNlo19v6f3wJLBN+YjzHaVxuVRYox5Ieh9+AGVWjjHOIiKjEm0qFMa9XhA9FsL+qsI8bjxS1q1JVUltmpJaep9iTTBKUGP5hNSZJe6/pPd+dH+6xeWXoCM/GhoaGhoaGs8K+uNHQ0NDQ0ND41nhq2gvwxD5KQ8hJdgUg9QGKx8hL9NFaHZlIDxn5z8cj5NL7BIPW1AIW0ou6JFiJR7j+jFRGk+kNAlzot5uEK4fJKeZ8eqUEuuVCMM9vUYfhkuE2fcUgh223x+PiyHtOKeMt3vOtFuib9kQKpeIkqo9UaI/V7DTfX/ZqwE6m2sv/DoYhiGO1Yead5T4yk+g9koGaKdp0k56m5QdAsXHJ6LMgga23/u4j/Hw7nj8+IKSWFW4vlpgnFuF+1d3eK4Tn6qDEhdqgAsFm21J1SUUOu2GCI/PHjHWmzkp3EiZtmxITVji+qcZ+uYklADOhR94Od//N32/zmhLERFDROxDbvySMvM3Hfq8miMUznTQ1ge14t/Dzk8vKdFmg/HedBjrqYP7L15R9t9P8PecKNBJSmM3IEVXi7ETEYlN+jeF+ashUS0tkufllODNSX48HndD+ExdUjZyhXsOt1AO3Y+gvBkJ07iUpZzUrU+X/fpSW2e0p6FEHZI8Ng38KCqwfm0HpHqtMXdSG+M22oJeX1CiyDmt17v2b4/HbvNPx+P7KWwzTUFP5pekQNpjnn28xDy4qGFjEZFH/xW6toJtVt/gN/4C/thS9vVwjfXoYfqb47HXYm5ubVzvbEi59wJ9GBK1lNaU0K+F/+5u+/b8VE7oHDBEiXNItJoRhR+YsE18DZ+zKeP0ltYom9a71KO1KId/PAhVS1B4P+UTet9QNvCdh/tHHsZhc/Pt8TiM/3jSn7IhpVwO25QzSoLa0lYWm7Zv7P90PN7P8QwrRTtWlEl9Rkrfp1vadkHv6g3R0VEL34qvv5G/BB350dDQ0NDQ0HhW0B8/GhoaGhoaGs8KX5fNqRORsv9eGgRUANFD2M4vEV4sSlBOszklJhKEyOYKoeUmpyR/dwiFZSmS2w1rPLc1EdKeRJTQq0NI2KIihF2OneEiIqMLhPo+p6A77oiC6Si5kkc1avY1he1y3MdrqNbKDGHOFY3RlUl92NNu+jHCrXmA9kQP/XPN+oz1gzpDuqoP5Qdj9KsxEFJ1hGzQgm4c1BRanv3ueOxREskR0U0b6//AfSgp4DWpSwpSa80iSi7nImQ+qRHiTTpSvojItx0UDR9CCsFyAc8BVC6tgJZJxnhGRtTVxECo9WIGWuW9jTDyhK73fPhd1FBCxSF8JUh7CtfozqgOkr5sXX2o6hyEGNddC5pokMEfixp9uyRKc0/J8HxKbmfR+F6GoDWWVxjr6APsFgn8vXRBWxc1wtFzRYoli4oGiUhYYh7FAezmUF0mByYRKuIse0qeGFGBXasm1d0UPy4MtHXC1a0jjNGkonpHROM4u54yO6vaS5kibR/+j3zYjNUsA4X2RzkVYh5Sfb0J5tqEEkK6I1w/rrEFoRn/9nj8qsRczklRdOOCingwMcfvOqwJZvqzAto2xmbv4P/8GG0dkELTob5taW7OLPiLa2DdtGpODPgW9yebhAa96og+tMYYo9lBSWi351NiKjGkOczL0QT3XRS0Rm2whrSkoHrRwdeXQ6y/IwPvzI62kwQ+xrNQuN7K4bsOJfc0R6Ct1huM8zefQIumNu4jImJPYOd48vZ4fFGgD05GdLnHNRRx/YDqWNpUEzEMcJ/7S9hmkOC5lgG/GzgYi8JBW6+Xp9sifgk68qOhoaGhoaHxrKA/fjQ0NDQ0NDSeFb6yiIkS85AMaQ8GQqwCYfAuRIhaFQifrUsoSryCkhbSzvWOavhQKRaxPYQ7HZ+Stm3QiLxAuD5t0J5Zg/Ni/ay2F/2f3yFMZtIzih2eXXgIzVsU/jVJtZI/4BlxCprNI6VZc4XQb25CtWCWpHIyMADlvL++s8/5rdqJ1fYhyYoSUXUGwuMO1UvzV0gwuL6GWiIq3x2PL22oMbZ7jKd7g3CkQXVcqjHG0yYVwieb6r2RQmvtgcJRHcKdIiKf9ggXWwPs+q9sUBoVhUI9qo9jO6B9JkKJtRKoCr6j5F4uUVrmDa6vvweVdj/GeXtHiRanB6rWPG9tL0MpcYo+rJ4QRdM1aJMdgT5uG/T5s0dqGQP+PqDEnO0aIfWVA/uYpG4MaS4XFvq8L0khRPMmaLjuD+4vIvJ5Bp+cNqDNnAC/KQ1StVWUqNNC/yuiTcoPsP+2o7lJtd3sOailjpRDXUj1lCq0p7k6KDEtrG+/Gl0nkvXjmjSkxLNgM8P5/fF4t4e/N6SUsipc74QYh4YUV1tK/GmPqO5aTvUNad780Se6ucX1Fx7W/W6L8RER+UTbHKa0XWLsYY60GejJ2MI6ZxW4l9uAMi2pPtn6Cn0ISrxzbAPXx7QdQ5nYXtBR7bBk1K/73RmVe2Yn4h2myKZEH4ekymupJmL1SLTiFvPPmGKeWQ2uZ19cu7BZSLUnhxNsDymWmN9JB5sNifreUb2shpSTIiJlgXV2RKpBn+o03oeUxNdBW3ntd2idTRVss3BB43kx3uM2vVvMnL4rAqgwHdqWsiHK7EvQkR8NDQ0NDQ2NZwX98aOhoaGhoaHxrPB1tb1MkTLsw8shURBcS6dNETI0ZwiRXVEdraeQ6no8UIiUIqrDDOGsbYP7GDGubw2Erq+GCJd9jvGsMEWoLb1AaFxE5DdUB+a7HZ5hvqNd8yZClYMZ+lYkFM5LER635wjrXlrow5pq7myfEHo0bIQ2X41x/Q8LUAV3WR8WPKsKwVBS2ofaXgbRTzREqqQd/C/R39sM475QGPeVA7vyfSaUgO6jhX6FJSlrLPzgwqZxWyCc6lFNqco+VQfdBhibH1K01bfQBxfMlTjk+d0WYeGtTYqlAVEOVBfKXoIWjT+hTfYI4ff5GH1bZKROiHu6zujOW9tLDEPaQ6ccSipobKmuFtU6Mmboz12D6x+2sO3aA63kOjiOOqrvI7DbMiUVzYTuPyFK8wkqxjXRAOYY6g8RkW+IYn4oQUU1H6ke2Av85qqAzyxpIVkRjSC36Nu3VJvwXUs+2aEPToh7Dgbo532DkP3wsaf3zOZ8ai9lGtKG/fi5PuZXsyFVV4y5Zk0oWWuIvlRLqo9mkrrtEv36K6rD9MNHSspq4v4lqd5mNE/3LWiTmrYgZGPYS0TkpYPf5DlR/ku0qYlgjwnVVFtvYMsioDZR3chruKZsSAWmMlA53gD3HFLtuy2pfJNDgsGuPX1P/BooQ6Q5JDQdDWGnjFTB6gEUvnMJe4/JL5+oRqVtgYbMbdjmrYf58I7qZ1YpbOPMYfuXE4z5w3usUc4Dxq2a4/4iIne07D5m8J0mprXcxTOuGlrLBetpRttArDsY8NJEH3YJfCWhZK2DCezzLW0F+WOFtnq6tpeGhoaGhoaGxin0x4+GhoaGhobGs8JX0V6mEgkOEac4Rcja2yIk9dOOeRERj2rybGNKrrSFEiS7ROjsWihBl8MhLCRj2rxAOM8vECL7+Ijj6z1ChAsf4dFxeko1/GAiDOfWtGt+Bvri0kM4L14iBGuvkRws/h1igdfUz0eiZhzBrvn2Aqobt8Ku/I8rXD8gBcrHYR9Grszzfasahimu0/c/VQgjGivspK+plpZVwcZtS7W9OthyTWq1l8Qx3bc0DgVCqosBqMAxJaNbg9mQyMS4rUmFd92d1ml7F0H9orI/49m3CCNHG/hFSMqA3EU43TThd4EBn1oQHeTtcV7N4B9BB3rncYe2uhXGdDnpx6g5oy1FRMRQYhzqQWVb3DtooZhoXcydoAZNsw7Rt/ABqr5kgj7UNcZo7/0V7vOEeVDQXAv3lKhuCHokFIxFMsJ8Gioyuoh8TkFTeC1svZvAZyY1/PZDi7kc7EiFcoGwe5RjXO4V+jZRuP5hBx+eDGDnrMR9guwd2nOoHdaeUb1nGEost1+rOkoEOqS14vMFxm4goDWMnGopuZhr9znaf+MQRVzBrh4pK5dzzJuQaqiVFSWcrGD7pYd5M69ICiwiS6qJ6FHyxP0U68iMKPYiQVtNnlM2+jytiR6pQWNFFXwlprpPQ0r0twgwD0a0jSJ/dUiOec66e6aI5fd+mpItvQzjnsyoZiHZYE9043gFGyRjzLOgwPVPRCkO+Z08AH02pPH58xbbA0YZqLTFGL78huaGiMgqf3s8NlL85nFA/kW1Aj+Qcm5E7+sPF3g/fJthG8iiQoLSYUMJFe+ItqRx/ANtwYhMzI99gHfsl6AjPxoaGhoaGhrPCvrjR0NDQ0NDQ+NZ4evUXmJIJ314yzWJNpgjrHZbI6S67khB5SLkVd8hJPU7qvvyXYMQ1uQJYUH5BrWRvqFaSomBkNqQwt77G6gN/maP0Hh5KkKQnBK3lTdQFd3ECLWWCkP00geF8PgaKoFXRBvVVLdrSon4diOqaVTi/LpGOG/mgypwSE1nHDI+Wup8KgQRkfZQ78arEEbMbhDW9oiGLEmxNKxgYzWnGjUtJeJSuKZdUl0hqpU2Iep0UCOknZsYZzV8ezwOGlLhvaCaaCJifyJ6J4Itpxv8JvTQB0UUaNfBNgMbNF4c4G+DV+QT+2+Jko0R+rVt+O/NDvfPp+jz+BDudrsz1mkTEaUMabt+bo48tDU2qZYS1ecyDaLAYtAj9bdEKy9JkTHHmJpUi0hRwsthDP9sLilZ5A7XGw5RppTIs8oxn0REJhH+b+PiXoGNvpmk7hgElPCUkrcZpCJql/CFq4Bo1gmuv6MEkYpC6gOH1rtLjIX7uQ/Z22dUCElniJn361nHtbpIwTMlcVzbEdXlYW1J5kguetHABuae+khKvGQCymFuk22ojt5E0fo7QALRl1SjkX1ORGRQwjb2HeZC0ODZuwI0yEVCc3mOZ7yO8a4wKVFhlvF2Blw/9kmJ6eF6L0Z7iguM0XjZJwS1mjOqapVIpfq5HhDdmJCa8dqD7yTmLyfZLGg7ySTFuKkB+tW4sJkRYV5ybTWnAz315h0orcc72Pg1vZ/qEehPERHvE7YhJFdvj8d/TfUwN8Rg/5c51JY/Eu3+7Qbv1WKEsbjJ4djrN1SX8JHqYQ5JGdiRWjxEIsRZ9r38JejIj4aGhoaGhsazgv740dDQ0NDQ0HhW+DqJQqfEONAFRUQ74lvs4C9CUFqDDc7nERIZWTnCavsBwt3DHeoQ7Si0HDZQoLQu1Scqsbs7ptI6TUVKliFC41VyWnOmmxB1sqLEg1SrS8UIDX4OEXq0VpQI6hahypySNOUOhaNz0CkN1ZsKWyjZnoQogRxtNaf9c5V1RrVXJ+IcaKc9UQB+iXFwnbfHY+XAlt0Etl/kCLW+mGAcyiXC45WJkKpHyiqfaoctOyhHxML5rMH4vLFAf64/n6q9jDHoQyvGsztSFFUFKATbwfV29Ue0T1ENpyUpSihJpyKl38iG/y738N/Ogw9uGrT1IuivV+eu7dUpsfOD2ovcxFZUh8zD+JXJu+Mxh+CjGva3qfZS+hlzbWRRraA1+jamhHnJnutrYXJWCmM0LjGXFdlZRCQhikcaqskmUJrVj1D1WUTZuA7mTpTAJ2NSIdkulEANtUl5mJtGgdB5TOqwOEXfRjcH/3LOODdFidn1/tMkGDunwxxsQ8yRYAPbPJKCyo1xPnQxDgkppeoW/j5MsM7uh7i/Q/Wv1grraTiC7csSdFbYYK0TEWkHaNM+BaV14cBm0wbvh8UA9/JytCmfYq5FK1IAjkGZjWrQLI6AzsxS+PXWx/UTony74zp7PrWXoZQ4h2SemRBX6WHdrAusUWUKm6krGofPpLoawkfrPdbTNEK/Osoy6xEVmG3++Xhs/xUoMNlRol7BvMxirB8iIt0U70zLwDrYRqQCXGOs/+katJlZYp7NqUZYTAo0fu9XHzHvDY++E56wji+nlICzhd/VL9DnL0FHfjQ0NDQ0NDSeFfTHj4aGhoaGhsazwtfF3g2Rxj7sXKeEZm366nicURhcXYNaGLcIa+63ULokVBOmfY3Q8u/HCD3+00dQDq1LSiAH189DhPmeUty/2CKM2FK4U0Rk7OL3jwHC4EZFIckSv7mJcN9liDBfukCotfHRjt95+O2Pgt+ae4xRSzVnvqE6SX92EKYebQ9hxDPX9qr9vk1DF6HDtKbiLQmptKgOjG/B9n5FirYH0Gcd1YL6N3T9P9JufqNASLuimkQvxhQCz9C2HztQI6F1qkKwKMFi6mIcwwJjzbnLVI2Qb2rClsOWakdRkrtvpmjT95+IGqtAS5g2bBzVGIs9JXzcHajXtj1fLSiRvu5edRjmyKRaZUK2KhAiD8eglRyFsHtKydeUg/B1SOHuMAIdVpfwl3JBoXZKehbVlPCyxjhWW8zlfIh2ioi8MXHdxxZ0nd3gN9WUntFgHdl36L8ilU8zwT2nBvxnt4KthhFC86mH5XFOdd7yFe7Tbvu1QjXnU3t1hiHFoa6aS4lSzYz6soL/7Kew5W0DH3w0sb7lJqjg1sd9vrVx/gfzt8fj8XdY09op/H0+pjp9S6zLM6KzC4/oFBGZhhjHdAGqqx7jGZVPqloLdE+cYG5uSdmzG6NvbwL45tqAz9Zb2DKP4F//FdUO/GcXY9Gu+z5051R7GYaUbt83y4HPtUuqh0kKPSfANdctxvdTCx+VJc3REOdvOlz/2GHtauM/HY8bUlbZQ8z7egW/yW2ss6Wi/SQick0K2N0KFHQl8KM6xLvrJkI7VhvY7J62h3CVxolBfbOJSiU6Povwi986sP0nhT6MP+C9/yXoyI+GhoaGhobGs4L++NHQ0NDQ0NB4Vvg62ssUsQ+qrTJDmNcqiPaZUD0o2kGe8S5xH6qFHSWAi2iX+A87hFddSqxVWgg5ezUlaWreoplLhO2aKzzXCzjAJlLiERJs8JtkDAplHCEEuk0RJgwKKE1iUkaYVHPqTxSmvqR6Y58uQXVFlBzsCbmrZES1tFbjPlTcmmesOaNEzEOoPu5gS5+UNY8+wumzDWzTXCFsbGQIX+7vQBmNC4RjP0eUzPARncymsMckxw7+FtFqGSlSuFAYlFVJIiKlhTGdWxyEP1cAACAASURBVKDTfqCkii9IpZP58KkB+e/HIULHA0pc90eidKwO199TnR1OtPmZkokNnmDL7cueQjinokSk/ysmMPqwb5Jj3g0VwsgboQSGNVQuBiVZC1OMywJuLf4OIeWqAMXUrN8dj3MK2TuUwG7oU42iPcYuoRpCQx/zQETkSUC1OOvvjsfxGPO5C6h2YIVQu0eqzrZGmwYl+vAjJcCbUF2xdx3+HrxWsNEjiQuZPtxe9SqU9oz1oAxDiWP16w4ts2LVSDC3m4AKnOegAmNS9EUrrGkrqlcYkOrtD5QocuhA3bYlKi2k+k7bGLSVLZhnGw/zJuwwl0VE4hR2ti1M7jWpYf0NJce9JYrnAfO8u4Atxzlsv84x94schuoo0d9QYbF/X4GGjVL0bX1I8KrOaEtTRELVv7N2REn6Ntq/pJqFVkP11XIYv3KgZNoQZT1vsP4+2BhDL8UcyKhO29CAf1t/xv1HHcYh5rpu91gPREQSUuh2Jubl54hVV/R+X0F1ZW7hX2oC9ZZJa8V+g3mfNbh/5qJNEwv++8HGc6Wj+SF4V38JOvKjoaGhoaGh8aygP340NDQ0NDQ0nhW+rraXMqVu+hBjECBktncQYrJIpWEKQlXBRzwqv0Lo9FVG6gqLaCVSOawnCPNdbSjxnCBUfk1hxOIN1VJaI9S4KU7rKdkWQt/q9u3xeJKgD4GNsGtF9UuSEcJ/cxNhu25IipcFwuOrOUKw3xi45qGkGkst+qOGqDd1faDSbDlj/SDTOCphAkH7K4V2XhsIPycBxnTYUM2cN7DZbIPzdQc66MEBBTSfQSk08qFCqFqiSxVCpaWH+7yi/qc/U+6FJak2XNjmJYkVKqqB5HugOjwTVMGMknr5AfxlUMG/nkL0+WWFMdoQlXVDqrHVFRQrt1l/H6c7b502JYYosx9nZ0Z1jyhMPSE6LLXJDi38+ruXsMOrDPZMB1Qjq4S/XE6QEG1bY7wsE2H3pw2oiHCOMPUFKd7a8vTvsJHCb7YDUFpqDn/wtmjHiOr87Tz4qrhok0O0g0tJ9ZIx2vSGVC9GSPXCDKoFR4nYbjd9P50zqveUMqRpe1t6RLvXGfzo2icausUa4tJWgHqCulUzAd04HWNd3qQYz3CEcW47rNdmQ3PNxDyoWvjWNdWTyzbE34uI2mBNSUg1OSA1VjSmWlQV/HR1SWpbooEyUjRetLDZJ6JfRh7Gzm/Qn84CLVdRzbpX6749LiurfiU6ZUjW9IuQQ7W9lgGoumlN84wSwgrVDXQmaOdvClBRuclrGubMfgTl3k0DmylKAFzT+HQ+3slXOeai556qagtatxpSS//VAOfzR4x11aIP5i1RY2zuHHN3/4S2Rq9AjQU7elcbmJdBBr8WSsrr1rjPl6AjPxoaGhoaGhrPCvrjR0NDQ0NDQ+NZ4atoL1N14h+UAhtKnmY0CFP6NtXFakFRLS8QqhrkCGF1BhIktg0UKAlRDvKE0JlMkVhp8hnhv0dSZlRjqu1ESpM8P925ngf4v+n9O/zmAjTI5xIhUkWJ9awW4TblImToPKDPcUjJsjL0bTnBGI1JtbTxoWRxafe9Nzjc/5z1oFolZtyPcTxFm80MqggrBP002iFp1OcRKTuoNk7rUnI5qt82MSlB3BbjVhsIy3sUTn0MQZn4KSWE9BASzik5pohIPqMQOkU8XRMql7pDONvOKTFa9+54PJy+PR4nFRQWjx78cVhivJ5C0EeVgp8WOdUCMzA/nIu+D+es0ybS1/b6SXmniAZwBdRrSfRRcI/zH67R1gkppfKaqMgcNF43QKj9qcJYOJT41NiSonFAyS8L3KcpMHamhTESEVlSIjpLYY4MWoTalwq/yQdYU4Y21eqiOkVtAsdoJqBTVIa5lkzRJoOosbrA+uLe47m7Vz3l1Nrnm5tGp8Q62DIlBsZ14fNdDmrPIgVdRvWv3BLngxDnHzeYs9Mh7pnuQVeNSRmb5ji/NWFXp353PN6XoFms5FSJ6YywhjoN1hd7DBov3sEGakC1AFus2SvaCjDYQGm2IFsaKfzao0SxW3rVFUTbDzdoTzrr/f2cqlpDdeLW/RjnlDRz0KFfakxbCogizgZ433Tku6VJ7zSiWw2u6bfDWlSRmiyJYUvLo3qWFtbrbE/zvjulMPekbq1jtHXdYp6Jg2dnI/iLl8IPjDVsVpMy2J5SJxbw05r8tytJbUzbYwyifONLrPtfgo78aGhoaGhoaDwr6I8fDQ0NDQ0NjWeFr4rVtoZIfKhf5NMu80ZA+ygKI9a3CNu9JPXAogQd9LBHiMwdgx4JSV2SNqDGlqs/Ho8rCrO/ofyF6zXUD7EJOsV0T8OZLzuE4O8n6IO5RZsiHze+o2R9P2yonlmNMLs3puR2NkJv6wyhx+hH9L8coc8vKHT+Y4nQo10fQpJnrTkj8lNZqmGHMPNOof3FPWyZX2IcXqToy6pAH+2EaMVvQKWYRBk1MVQLBYW6U6ov9VsDIdFth5DwbkVquLtTtdcrUs3tZrjvjpIqhnegMSIHIdhkiWdkFagCo4V//Y7s+ocWfjQmtYHpow8hJZVbUT2rQXmgGc6s9uoskZ/Kmymm+hqqDUWqvoJ886qC3+070IEt1RCSGfzUpbpoDdFByYrGYobnXlG9u2UMKsKiZHhdcPp32DcOwvl/FFBR0RZqvICowwH5TCKglb2E6GZKXnpB9a2sHCH7XUEU2xj+9tZDf/60hZ3Hn/r7m9X5FEJiGqL8frwHtL0gqWEnQ4huHsI2lgsbt5SUNUkxbqaNdc8k6r+gbQHdAsfWNe4f2aAx1gvMm8mO6FX3dCxCopEMi1RkT6BYS5o7lyHm8pYot6tPUId+IGXwlQXbNzQ394/wIesGtry9xFq/eId2Dn5KynvWGoqGtIc+BzZRRkRb1lvMg4LeAZch+lWVoH1WCb1jSQ377QDrzLsU6++O3sl+SIkWp5iLyY62MpBi+94+pb1uFf5972A+hVtsEYgrtHXa4tlFhu+BhcJaMR6TWpzWnz1l+PQ7UntRHbg3VCPsI9Hxzup0W8QvQUd+NDQ0NDQ0NJ4V9MePhoaGhoaGxrPC16m9DEN8p/9eSluE6twEIdV8RmHzHKHQPe289yra0X7N9Vco/FdSfZECO/vzOcKFFxlCs+uMFAIGat08zmgn/WnJGfn8hJCZ7SLcnY9AzUxMnH+kXfZZh9pe9oiSjJkI861WoIfG1rvj8f4WKoerBuHYXUg1lkraMe/1Yf/ujJ+qhiFiHyjBfYxQaEQ1n3av3x6P3QSDlw8QUvRTtHM1RRh0sqa6NBFoEr+DUmY5gK+8LnD/Pyew/SxFqLt+C3XYYHsa1twY6IOZ4V7pCGHwYYJw/KcRbMn+W1LdsikltXxP1O6kQrj+kVQxt6S82FD4fdRSfS2v79v5Aus9TCXyk0Bym6Of1o5qAo3gdz7V11ua6Fu4QluXL+DXY4U6UZmD8+YG9Xq6l7D/NSk6N1uEwb0a/rIMcP2Ln7FG95SQdNBgPv8YYj4HDX4UbNFuq8AcfJyCFpiWpNJqoOgMs386HldXNJcTorp80D2BBeXj8ran2BoXfvProcQ+JFVMTIzDhBSjmzGUmF0GG7sd6HKjxtzZTbAWjWosJNsVqfVS2Kwa45qIklpmE5pPnygJ5CXVXvpZksDMBo3iprDNmuqBXQwx1+JHWi9ITbgkJdCYaiJmC/Q5IDplHVJCTbLllijKQYzr4+veT7sz1t0zTCVO0D+vSuFDqqJaklQPc6iwMmw2lDC3RN2qmhSm8w5+/1TgHTuqaEvBgOq6UULemGpPGhnm5YJUeBMafxGRNfmXSe/fD0OM74WDvi1crNmT7D8ej9VbqLYrUltWBmwWUCLaagrfnK3hB98Nee3CO3k3xJrzJejIj4aGhoaGhsazgv740dDQ0NDQ0HhW0B8/GhoaGhoaGs8KX1fYVAypVc/tOR6+m+Jr7N0IU/CIqgEH6VDxtmoO6eTrFvxuDYpPTAEPaF1CVh4ZtL8owN6cNzm42+8btOc2B2deuafy6HmI7m9M/J8TgqMuC/CutYk+jEbYM+BntG/HBa878IgTt8C5X1DWzFKhDUOSTe8dyG/DuL+/ddaNIoaYRm+HCW2GehBwui9i7Au5p+zTF7QvZDsG/3wr4O2LAv4RdJRF9iVs9u0Oe79Kh7Jt0zh3PsbZIilu22Avh4hIZGMvQTYH3/uC7FehCzKl/QrtW/jpDRXlSy346YWF+8cj9Pk3Ne6zIZnqRY19BcWEMisfpNqmOu+uHyWGVF3fXi46XLgYi9eKstlW6NvlBqka1jPY/44K/2Yt+jzd4fr6Cn5qFJTCwcS43PqYH+kEe3b8DGNUOlSBVkSGIZ6dL/HsbwR7EBoLbXUMKjpM+wAmCtePffjPwMA+wvYS+xLmJN0vaLnwauzzGoxxffip39/gnFPqLoZ0Ru9LEwtr0c6jfZOUGd4UrI8TC31MaU+Nu8VvC0pbcZHCV+K/pqzRCcbN2mCuRR9pnr3EPssXC7RzE5zutxiM8Juu/mu0ScGP2gS2HCiM5fotbH+zxvq4sTCZJwH8qxT04bKmtrq4furg/vU19k6ND3vzLHXGNBTKkFb1vt0OMD/aCOvgPKHUA9S2iGTmn2kevzBgj8LGPWcl2f4Gc+BawTaLGL+d0Z6feEDFoAXt+bE6fWdOqOBwNsWYXtKQWULZ0Nfw03KMtfx1hvmelJQ1fE/Pu6biyzneRU4Iu5okhx8F2KsUrrE/7kvQkR8NDQ0NDQ2NZwX98aOhoaGhoaHxrPBVtJfRtmLv+7BXGiCk6pB82Q5+dzxWS5KcX4DWuCwQ0t65CJ2aQhk/W5K6L7/DMRURrUmK/N0I1wcUZq8qhPzMHVW8FJFdBFlrmkIuarVv0YcSv7c7KrZINFvpgNIKciqudoUQo7VHW3cuwtQOFQZdmBhTiwqbtlf92HUO2vKr0SlpD9lTMxduEFKm1ix4ezwOdpBa7m5hS5MKXpYlQsjlHraUGexdvoctJ5e/Px5vHiCZNol6cWqEWW0L4dGfJeuWe6JNpo8Yu+I1wsXW9xhr55ZogD38txjAf/MYPrG3EeI1hfyIsikbC1A9cYSQtbMh6nXah7uVdU5ptIgoEbPtQ8AJSe7rjuiggIoIF++Ox5+uKQt6ibD71IBfFwp9K6mYYKYw1o0g7CwL0NMZZXs2VrCBMcL88H+WkPWRxmfsg5atKPN7kVBWYaLABhYV+izgk8kWEtzqd2ir2qFNlwq0yZKozpKyypob+EV5SL3RnLGwqSgl5qHY44poL6fDmHY2qDezfHc8/rHE3PSJAjMuiI7/jMF+nGD9jZ5AQ8kY62wXUsFeD/e3K8ybaor1d1ufUg7TJ6LBfLwTLMGcdR6JVr7C9eF7koRTZuaaaI0VUfJ2S8VfKXPwaIl1ak2Zlp2CClTP+zWhO2fRYSViHSoamBVR3TbWtW5K6+ZHWmdpftQVFZweYB6bP9K43WK+NlSUuZ2QT2zgE3vKBm7yloUc4+xTIWIRkR1R+FaNe9kB+rBdUDqDOfldjr4tCzzjZAPDBVHqVJjXiLAuxzu02/OxXj0VaI//Avf/EnTkR0NDQ0NDQ+NZQX/8aGhoaGhoaDwrfJ3ayzSlig4F90KEObMtdmV3lC3XmmLn9ssCIc4nEyHtuwRZGZ8oDPqWaLXvKoT57A6hLd7B/4oK5j1Qxs9BTUUyqVChiEhE7VMtwnlRRlSBwvfh0EafnxyELSdL9Dm+Qaj1ukNAb0WFO4cPoE3qMag3uwE9siGGa7zow39Gfc7iiSJyyCjtugiXdlQoT56oGCIVhbyiMX0kNUCewZZCGT8HJvwj8RHSLtYIcYZDPHdAWZmfNgjZzrcIs8dzqmQrIr+fgLr5rsBvrt8jbBsTvdNUCMcaOwov+wjZej58cOyjn4sUvrYLYZPZAP479+FP77lY5qJX0Rhk63OgEyW59G3xiZpQNfrcpvBZ8wrtmxjwzQ1lSl5b6JszoMK8lDVZPWIONgFRfURDs3hma9PcXENRZPin6dfvLqkw7I+YDCMkV5YOJpRJiPapGOPt1OhPN4TC5i5HH96bON4r/FYo0/hvxnjYA2XJ9cr+/udU7ylDpDwUA/aosLK5AVU3IHonnpLfCdbWZQM/uKEixZsJfO81KT3/tEN/X2xBTz2QX48LzN+kwTxoSN132eE+IiLWNdZQ6wFze3ePZ29f43gQYizTAv7lbjD/XSqwOgvhv/c79E19xDpbwt1PCoY+5Zjj3sNhbtZnnJuGEuX0E8AZYlySBexkpJQx/5K2FFQYK6fG+fVnUH7uEH0fEE39McWcsTqMp0tq5Lc23j2PHfqcZe/QtilNMhF5SdsNPpFSe0NUqprhN/ME52vnGzyD5qVJ6mrbwVw0SGHabEC9dpT1/JWD+38npBTb4931JejIj4aGhoaGhsazgv740dDQ0NDQ0HhW+Dq1lyjxzT6EFtMu8Ih2/W/m2GVt56A10ghhbO97hFQ/XeP80EBzfhCECG3zD7g/Fauzcyp4WGI3eCjvjscLKgI3ME4TqdVbhN3DLcLCD68pqddnKGfWU9AG3ga/3d5RoqUcYbj3NsLUUYkw3/sZYoc3HhXhNBCGvI0R3//hqld21D9LBPdrYRj9t2+xpwKvVDxxd0sJ6TrY8oH4BlXA9jXRRFMb4/BE4XEj+eF4vB4hTDkyEELd7vFN7pOy5tMd7n+9P1UhvHtPhUptKJDeUTLOKRhTyTJQF6GCCqH1SCVgwPZxi7GflKBPV0S9VpS88vsKx5Ma4ffdtKd9OuuMyj0RMQ0R/6BSySlcHuXw2fUYYfFJTn1w4L++DzvHlJBQlvDHvCaam8LXNVGdBsnxmj0V+BXYPxljjnuUkE5EZJNQokID9knJZyyLiu0WaEdICQfXFLW/oIK8nxP4hbNDSD2eom9OjbH4USEJXET23IeHYpjGGYthGoZ4Zj9nsgwhfiPH3PwYwN+9iqjNCMeXLWz8pwuitAyM9aeSaMgU4/zRx/2jDD4UWxjD6PHd8fiB5qazOaUcZj7WtTjFetF8Q4k2W6y5TykV4iRV146eERrk1zHuP6RC0fFLGN+iBJybnOZHBVuuRv1cbs9nShExxDD7uZ5naIPXYhzSCGPtkFIzm2BtGe2wpiUXVHzXxvjsK8y/YYt+5REVI70nlRzR1/Un3H9zC+XWtXM6Lz/vMDgBFQpeXmFeXu9hm90UNpuT+rudYm1xHNxnX6L/wRZtrd+QmpsS137oyO/Sd8fjZIox+hJ05EdDQ0NDQ0PjWUF//GhoaGhoaGg8K3xlZi5TlOpDkgMPtMN2jPDcjGod1R3V9qIwZfYSCbomVL8kzHHs56A7qvFbNHiNa1ID5ycmwt6Vg5DXbERJDo3Tmi01qUISCqnOU4TNPUTsT1RB6wiqhzHVlSopqdzrEvWDViFCvN9mCBEmVPdqQIoXk2psvT0kH/POqfYSU4yub0foIBy7v4J64Caj0KmiMHhJibWmUNBYBsKXrYJPWGuqpXMLOiOqEaLelmjD0KDx/x1CsL+h61PrVO01MBAWbUOq70PJ+Sofz7huYae9T5RGCBsUNVOyePaeFBZzStHVhaTqIhrE9DBG46q3pXXm2l6iTOkOKrzBmChHg8aCqOqshQ++QqRZHsZQVs73oH2cISiExgF/uPPgp69qLCelBbrmgupx/XlCFMWGElgSnS0i4u+R4KymkLoozKPBE6lhKAHoxxr2+UYwzskeHXVY0BO+PR56LtadSKGt5Rb931+gPUHR04GmokH8tVCGqENC1QEp6DYCSu61wvnEgu+bGebmfooxfbmjtcOCTwwrrJvJADT9nQ1/Lzrc54oSfH73itYoF20ovsV5ERG1+OPx2Pgt7jUh+1U+bTWgZLfdG6o1SPUClY05FUbwo3tSWb4gSr4gqt4SjFFNiRBv132fnfZ8tb2UIVIftnMElAhzSwl2hy3mpWFiXRutMG/qEWwz7mAb20S/bOrv0oM97kxcX89pzD34rPU3WANf03vusT1Vvv3XLebZhyHW5m8stDWjteIuxnq6HsHvJrSlICPF8K1gbVn9zVtcv6M+U+JicbFGuXdQkzkL+MSXoCM/GhoaGhoaGs8K+uNHQ0NDQ0ND41nh62ivTomR92GszEA4zKcaH8nst8fj8B7hzl2JsJpDdUdqD+G/5RahLUW1l+oU9NHQonpcJUJexRDnxwXVxbJAsxir02+9eIwQur8mKdAQlFa6B4UQRAihOwV2rjseJW+iUOLWoR3tArVMS7Vo3Ao72huqT7QNELZr3T7c3dpnlCGoToymD1XmFdEBpBIogzfH43yN9ntDcIFtgnGY+uhvQZRGNIdtkgXV9vFInUCKlXwCu8QxlDjlHmFWw0C4XkSkmIMmsWKiUwLY36V6Y/cDCv3nqB9UUGIto4DtN3OqBxQjvKyI/myeKPkfJdxSFtqTXPfh+tY6Yy0oERHpRFQ/N/dr3DugZ29J+WiT6uZPF7BDRKrHhmq7SY6xyztcf0X1d7JrhOaLJdW+IxWjm2JuRiPcP96e2lOIBilJNTpVoMxbobpiVHtrGtKc8uDDaoX5mw1x/YjUKZ2HMdo8Ueicpp5J9ZGKA43XmWe0Z9dJd1CNpopqitVQUVVjrKf5imoijtH+lmqfCc0Dr8V6WkRYZ40A9w9S0ErrGHN2S3X9BjQOPymaRET2+9OElWGIdc34yBQPaOXmCefTIQZ7nsFPM6r5ZT1SvbErqrtXgw7c0jshWGBdi0tscehM2Lga9etae0Zbmp0S96Bg2hFl5Be0DWQEn7Y2eGfmQ8wnp4RK0lPw6XqN+fr4kubxE+ZfNYUSelORarPB+3BMqs10AOXt1epU7fUfaV6aFvxlZJK6agv6/0eixV1+Bxq/OR6HpOhdUg06+zPONxGtLTVUiR0lq81LUocNT7dF/BJ05EdDQ0NDQ0PjWUF//GhoaGhoaGg8K3xlbS+Rn8o6OYp295cIhdmPFO4OEVIdlqABdg7OBztQLu4lwtJRgLDVpx0lHqOkVE6EkNdNh/DohwCURojTosYI54mIvKVQ+accu/4NPEKsGdWKaaGYMB2EYPdEjbl0T5cUIFmHPpQdwvV7SmR1Tbv7txFCs1Z1CGu3Z1SUGIZ0h6R43hDPSkmJZm0QvuaaMH4DikpRra5qj3Dnboz7/MbEuN3blLirouRpI6rrVmI8VznGre7eHY8L7zTh429dhLs/dQiRuveUQGsAn/2Gamt9puSMJlFmFoV/BxV8PBbcv4pB1XYhxuh3VPPrn9egCYcfegrHPKtyr1eVdAfXC0iNlsZot7tBCNu4xDWTGDRD0ZE68gF0RzGH79kF5teSVBvWJ/RpNMA9/RDXLEuq2fWZasdNYD8RkRchaJ0fyN+2FVEtAewzD9DuhzV+O35An7tLXPNmCPv/0MBXO+pDNMAaFIzgt7scv40O9abM5py1vZS0dn+/yAHNVFCtwJb81KS58Lom1RspRoMca47hYtxGM/jm8gH33/igoWyi4+cOKVUF4/PQwJbR4DSBpz2nTJP/O35/X5H9XqAPv41w/OcU10cbouRnsMfbGPP3e0rsaKfow95BP187+Jv/nYH7XMS9b9nd+WzZGSLlYSo4PtW7o7pYPlFOxRBza2RjrhgCGrIiSqu9QPvf0Lh9L9giMFljDRxT6cZvQ/z2/SeMSZdivq1HWCdERC5Mqru4BJVqUY2tjKjUF5Qx8okSl1oKfYhJHfbCgL/vSCGdbTBGQQS/C12q3UnJTbvyNAnuL0FHfjQ0NDQ0NDSeFfTHj4aGhoaGhsazwtfV9jKUuFYfEtx2+G4KO6ovEjFFhdCTZVP9oBghrPIaYTU7RVjwPkP48oZopeUcIed5C1rm0cAO+Cj/5+NxEyJcGBmgZUREHhuE4Zziz8fjpxnudbVAKK0ZImYYdKAE3lPypmtSrO1eoK3Xf0TI9ukFQnvTAte/n4JO+WbzJ5y/eCsiIq11Stv9GhiixDvQUXGJ0KS7p533E4SKLYWxq0lF4X7EuK0jjOeIagC9b+AHoxTh8Q2FrqekSvpQUKI9Uidsb9DOKYXDRUS+p9pAYwtt+jR4ezx+YUE19H0HXxsn8KMV0QCzEjZeUcI7d0X+NQUlEOYIl/+BkpWFOUK8q8uf1F5nLSAkpoh4du9vRQn61Kd6UDFl7IwSjHdtgiYyae7sr9C3aY35G1PStEjhPssZ6IdoBduuKaydURK+9hL+fFOfUg3/TIk0vSdQXea3uG++orpMS1J7WVhfPlxi/t+RknGdI6Ru5bi+s+BHFiV4S6i2m7PD9c2gX++UecbaXmKIe0iSmRE9GmXw348T+O9QcM2DR4rUz1RzcUZUlw86ZRuDkqxb3H9L9MjkGs9abkFPmgXWhFTRuFHCOxGRYEP17yL4l/kG89wjleGfGqyDvqCuY0ZbDS43WF8ebHrP0Bzf0rvlagEfenSpRtgGffgw66+vrPPFBEwR8azDOlvRNoIM62w8wLrpK6yPjQ36sHtEv+xLqn1G9e0+rWCDS6rR98GiOnBEC+Z7SjBsYp39eA312SyBskpE5IMNm12HWB8/k0I6rGD/BSnn/AJ9XpGqyxPYYN2Qr6ypntlLvGOtNebfEyV5vNzCJ/bXsPGXoCM/GhoaGhoaGs8K+uNHQ0NDQ0ND41nhq2t7NUYfznc9hNv2FkLLkwYh2DLA8UBRCDZCOH1YILRcExXwOqakWbeg0kxSe7QjUAve6js0M0LSwdBC2N9rflY/iOrarMLfH4/ftAj1ZROEJG1FibVCJI766+rd8Tilmi1uC0qgnGAH/EyBsqkp/P5bB2HBrf83x+O32z78555RUdIZpmSH+lgcdswjhDVnBsKXrYH2e2t8M28jjNs4wngaa1Kg+BiHpyuEL+8E4eq8AdV4N4GvLIhi8+ie+fiUwpxT/ahuQHVqAe7oUQAAEPBJREFUqA5OU5G/JEgMV1F4/GWF8HsquM+QbNnefns8tkrMAzVA+25b3Cf7Bj5087Hvm92et7aXEkPqI/WAaZ0H6Ns4B8VhkErCsHG+e/VvjsfDHfxdUU0tg1WHCnP/VYW5XESYd7eUwPLRQTh6VmNMFzaoFRGRS5dqqd1ivXBWsLsKYJ+LCiHvT+TDL2sksCypftjAI6qL5rJNtYKcHO02E/yWuizOU/9csztfPSgxDencn+pBwU8Kj2ifGP6e0NhNaB7UM/jdhGjrrATdUZJCs5hhnb0rsc6uTbRhVoM+Kybws289+FlDylsRkS7GelpO0IcXDrWVqNGbFnOzvkLS3HFG1PsI8y4waSxMUnU9wU8LSs7nEvVaX2Bde9n07XbljMo9MaWRnqYKAozXrqVaXSboWLMDpdWWuL75PWxjrUltS1T7XNH7g2x/scU884lWkgDjkNxAHfZ6g7lXDjHHRET+tgAt988e6LErokytHO+HlBTSxgx+OqKSYV1OCWQ9+F1+Raox8uvUxDvkRYp1SdF2knB5mpzxl6AjPxoaGhoaGhrPCvrjR0NDQ0NDQ+NZ4evUXk0nziHxXUnRs0GLcGRJtbTcJe2wHyGcF5HqQo0Q2ureI1S+nCMMZ9e4/4jqZWUP3+NZt4hFdylCcLUgzF63p2XuEw+dsGLUVKlttKnMaVd+xLXAoFqopgj/lXuEeGsTbXIFodzQRVg+JjqwsRAWNF2E6+OoD+V2Z1QIGW0nTtKH7DMb38CmTeHLCu0sK9QDymecFBDjE5Ro/75FmPIpQEi7SUE3OCbGuW1BsWxsXB/l8JV8gPEM41NFSRIi3G1U8BebEhsaFOKvr7lGEUKkrFY07XfH45WBcL29gXrrxQB0XfIA23++g60GD/TccX8fde7aXkrEOiiyipwSAbpELV6ibxEppfZj2FwtoPoYuJhrO6qXZxF1WSfkLxnV8dlh3N+/As0SPcL+9QvMm/oBzxURaYjisai2V0u1ACWGHZZEX8y2oMCzOewzucf6sqAaWIaBukmjGn5R7TF25Q1oNesH+Evxsr/+rHX3OiWS9XRGQQkD2xjjYI5B7XsrrIO7CvPAIeVQGWE9NXekABxi7kefseaoq786Hk+JtlqSeifKqB4irbPb+DSBZx2RSnWHdXBtoA8DSo4b32Fdbvf/J55no03NA/xoeYf7j0tQQpsI9wkWGIt8QEpRekftDmtCa5xxne2UOFnv/wmpMB0ftJ1hgFoqC8wDFYKmLz+9Ox7blxg3e0l1LOeg0owV1VCcvz0eZ2uqq0lJDuuU3skDzDF7hXVfROQfJ1iDgxq2FFLipS3G2qDwSlpiHbwOUNtr26DPKxc/mFLtOGuCPjRrPHdzg3nvruBD3RzfIV+CjvxoaGhoaGhoPCvojx8NDQ0NDQ2NZ4Wvir13lkh1CMPaJiigtkbI0yYFT3eN8OLrDqHJxwShKucddqubM4Scb0k19oOL8N8lJ2lyEeIdKxwvaJe8p3B9EZ7Wg3rZ4L6LDmGynFQFMgIFcztGOHCdoX11Rkm2PFKgeLhm0YIGSDKEGLsBVBKXZI0fUozRrd1fb55ThWCI1E4f3rXmGLuOdsmXBsLsxhVseV1iHD9WaGdBap9uhhDvhYuObXYYw6wF5ads2H5uUi2ngBLnbTFulUX1gkRkYCCMvKGx9ki1UsPcclOiHXuFvwGsFKH/0KD7kBjpaYw+P+YI8doT2PKW2v0Qoz9hcaAczlg/SKS3Z3FIJOoMifrYo9PNZ9C++zew1csCIeiVibD4mlRaJqkVDQsqmsZCny1SVmUvqWaSh99mCnZOFwjZe1QnSkTEV2P6DeaRTUolx8YYX13ivosctEWUg079QNe/EFyfePDt9iNC8M2cEicq+NHqEs4QPR5qe52zVpshIt5hnfUwLi3VRzQKUizNQDfeEv20pmUsW4O+KALY5nekglpQktLdBuPWXqAN8xZzpVSgOjpKRGtPT1W1d1TP7T7A84ZU4zC5wnx8FeKaH1P4Qb0j6nGO98nfdBiXHw26/yPovYSVRg4l3STFWbfs1xDVnE+51xlKMqef6wEpgXNKFhkbmJcd1UQMqf5XWb09Ho8/w9cyUkJfBfDRdzHo3mANW5bk6y9oO8LHEutRt8a635CNRURuYTLJyC+We6ob6GL+vUIzRN7hH8UWtlETtONti/4klIwzoQSG1pi2FJhYx7Y1KDnzHrTol6AjPxoaGhoaGhrPCvrjR0NDQ0NDQ+NZ4evUXiJiHMQHbYefhvLueLx1oeCwqO7PtgNVEtYIq31+gVDVZQr66GNI6rANQmR/8BAim48RsrzPEO4MTYT5tiNSuGQ/U3v5UFKYBSiYbIrfBFSH7JNCnyfJu+Px+gpUl21Q8rwEY+EmXGcHtMTdHrTJ+waKjFGG3fr3B4VLfcZvVcMUcd0+fFrEoIysDGOXjtAeoyB1RYuw+UDBlgsftpwhqitPVAfO7nB9eYvxvNiDnvwouM9gCSXLmmjHKdGZIiK7DuF+K4U65YEol9Cl5G4dOIFJDZXA0xy2j6nuj0dUnL+DLeMAyqdxDps9VPBHs0B78oO/d2esHyTS193zDnX3ygwhbD8nKmqGtnobzLW1h2NFc628whwMM7R3T4nFhgWoyMUQKpS7EmNapQjfKxP2XJByZFqjDSIiVY0xNgXjV9gIw48U/Hb3ET6ZUQLL+AZ+MaAElmsXYfToHuqwNdWzGxG9t6pBF/ikMnu87RWRjXNO9Z4SU/XPTmncfaJk0wjKHpvG+pEUrIMKttm8Qr8uqTbZewWqK8pB+S2pVtN8R+u4i/MBKT0fA/jEPIVdRETeE9UySbDePRFtEsWYzw8u1niT1qMnosYuE8zljy6pkzZQ7u3uMPdHtF48thiLSQ4eZz/q73POuWmaIoHbv2diotOsEvPAmpDaK8U7zaAElyOTFE4Rrnf2sP2nHD4RKdCcCw9zIHBhmwXlojSeMA7LV6Aab5ZYr0VEHmaw/8UaNFs8JbVtByrq0z1R1qTCXQ0wXy47XPNIyV+HZJudj+eaNvmXD9v7JtaJ9Q3Wui9BR340NDQ0NDQ0nhX0x4+GhoaGhobGs8JXxWqVmKK6PpzvhUQhNG+PxwOF83aO8JlBdanaMcLdbyuqI+KTimKP0OyekurdFQivepQoziFlSk6h8SuidPKIkluJyGRJodbXCNvNYoTEkwBKiqscofn4FZ4x25LySxBGrn1SQHi4zwXVRcuJArtY4vrqGiHM0YGKsM5aD8qU1uzH2zEQpowviHIq0a+uAR002ZGK7QLnb0xS+nWw/agGvbgZYHzGCWgI5eE7fF4gPGoNqbaaCyqpyk7VNUEM25q3NHYpqIu8oxpRQkomokaDFGNskBKG64oVc4RUb21KIki291YUmr1Cskh/14eRz2tLERFDlNG30R9SzR5KFhjuSMUypCylRI84V+jDPEAfVlSj5zXVCEup9tqlwlhsW1Ji2ZSw8xLKst9zGNyGbUREihjPsEao7zQdYn1pNvCZOEUM3x6jb9dEh6cz0G+TJdF+E6hQRgYljVNYjy4S9L+a4vrruKdl7PaMai8xpTmoGUekGN1RzTqf6m21LfxrkFPCWUoUe0cK08qD/16VoKFiStb6XxAN/TSEbcYVrfsjtOelC2rMLiipoYhIh/nfzeAv45RUeVRfzkxA/RQu+uBWVAvMxH2cnChWouqnNMf2Dda46xa2b6fk75v+nWO3Z1R7KVOqtl+bAh9jl4zJZpQQ1ujQr0hIqRxgjSJxqngDHgem47EG3tT0biS15EUA2nJ1g/v89Q7vpOUV/ENE5A0pCDcz0IoDvLrEIsWd46MPixBr0YTo3GBMvpNTAlFSx936NNdJ2e1tKKHiDP57GZ/Sdb8EHfnR0NDQ0NDQeFbQHz8aGhoaGhoazwpfp/ZSnRhNHwrfrxGytEbYZW20v8cPdv94PGxeIcxnJAhTDnyEKWtSAuwcSp6WY6d7YCHUWjwilFtOkFir8CkBWIPQWZvhuSIiiwBqAzvFM0oKhdpPVBNmSMkcc4TqTKJyaqr7ZLnoQxIhnB4UUMW0OcYuowRgBYXxraueilLnVJR0nRiH2l4JqRscCl+3FHI2YrT/6ZqSIrakDqpBB1Sk0igH6JdR4rzj4fr9Akopf4AQb+7DLlYHH2rzU3VQNoM/ujX8wg3xG7WnGlZDStBFlIDjUX2cAoqJNakTnIwSaPmwvZVjLGqiSy1SN9ajfuzUGeu0iYhIp8TMf6ohREk+yZ7dEGPRkSKqnkHVle3hd45DlMMe9vlAyfA8UrgFl6TISPDcB0q8V7R/OB7n4d8ejxuqxyUi4oak4KkROlcVzitKftiFoHJcF3ZLDKof9wlzcxuC3i06zMFghPu0Kzx3Q8kGVfUODT2E8pV5RnsqJXbRUyFcPcuyKClohxB/V//5eHxSmy4G3WiMyTakVkwnlDiR5viCtiaYGc5/pvtMS4xbZcBXHqvTdTYYwx+rHPyIE+Be6x2UZiYpiurdu+PxhfXt8Thp4HeNRXR2RzXCLPi70LtlE5GtSkqed1BDdmes02aoTqymt2VBVI/jwbJFhfabJaioD6xuLbGG2IJx6HL4ekpKRaMmZTPVJfRpS8d9CNtXFcanoeTB1sOpqnYREaVlgkKzxlDo5QXa2gnuFZqUQJbWkHYPen1NiZGdDH5aVuhDRQk+8wjP9RP4kEEKui9BR340NDQ0NDQ0nhX0x4+GhoaGhobGs8LX1fYylBR2H2oObezgz0qEWqVE6MmmcvODNa5PKFHhdoOwmhdhV/rAAK2xr6C6KDKoAkJEfsXfY2d/nmNnf0bXB8PT7lLJKdlnCKtlpDa4GSO03uZ4Rm2DKthQsiczRD/dgNqRUNh8g+vtSwo7W7inInWc+VPdlDOqEJQoKc1D32y0oRSiCVfY2h/cQO3k1BjTOIEt0wp0UDDGOIQR1B9LqsOTkzLFo/oullC9lgLj5hHVpYY/q+1FSRVTokyLDUKkkxnuW1gY38akhIQx1V2bgK4Z1nCWVQ01UVLgwUPyX9dGeLmoKCFodrB9dz5bivSCmvxQ28ukuZkU6ENHofBgAHv6Ofy9rXB+sSf7jDEHb6km22MIO5QPRGmM8HfVzIfNVxv4V02KDCc8VXvZDsanrPD75RZ0XUh1kCY51pElKbzcHexvUfG8NkLfVAx7bj/Atyl3noxIqfNE9aZmSR+yN85oT2UoKX5SPynMnaLFc5sS/Q0DUvy06GPuEY1OtMlwRGosSj5bW/CViuama8PGVy3Gc9VhfV8uQZuMp6eq2irBupApUEoe0SPODO02W/hRLVwjD+d9WssDD/6+o/VIkdLXj7COhBX6/0Siywujn/vGGevudaIkld53DAvPzTOMnUlriE90/IQYp8wkdSap2yKi+1tSHecuPWsL2tK9wlwPDVKBrel6BXsNZqfvTM9CW7uU1nKq3Rl2WCtq2lKRG6BSqwJjHAxgP2ePNb6ucc/ERZvsEOt40GC+7h28P0cpE8a/DB350dDQ0NDQ0HhW0B8/GhoaGhoaGs8KhlLqL1/108WGsRCR93/xQo3/p/BGKXX5ly/7y9C2/FfH2Wwpou35/wHoufmfD7Qt//PCL9rzqz5+NDQ0NDQ0NDT+/w5Ne2loaGhoaGg8K+iPHw0NDQ0NDY1nBf3xo6GhoaGhofGsoD9+NDQ0NDQ0NJ4V9MePhoaGhoaGxrOC/vjR0NDQ0NDQeFbQHz8aGhoaGhoazwr640dDQ0NDQ0PjWUF//GhoaGhoaGg8K/xfgKKzB2GAoJYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(generated_images[i], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
