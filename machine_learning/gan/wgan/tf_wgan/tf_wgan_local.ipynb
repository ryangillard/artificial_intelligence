{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2-dlenv_tfe\n",
      "1.18.1\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "arguments[\"train_steps\"] = 200\n",
    "arguments[\"eval_steps\"] = 100\n",
    "\n",
    "# Eval parameters\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120\n",
    "\n",
    "# Image parameters\n",
    "arguments[\"height\"] = 32\n",
    "arguments[\"width\"] = 32\n",
    "arguments[\"depth\"] = 3\n",
    "\n",
    "# Generator parameters\n",
    "arguments[\"latent_size\"] = 512\n",
    "arguments[\"generator_projection_dims\"] = [8, 8, 256]\n",
    "arguments[\"generator_num_filters\"] = [128, 64]\n",
    "arguments[\"generator_kernel_sizes\"] = [5, 5]\n",
    "arguments[\"generator_strides\"] = [1, 2]\n",
    "arguments[\"generator_final_num_filters\"] = 3\n",
    "arguments[\"generator_final_kernel_size\"] = 5\n",
    "arguments[\"generator_final_stride\"] = 2\n",
    "arguments[\"generator_l1_regularization_scale\"] = 0.01\n",
    "arguments[\"generator_l2_regularization_scale\"] = 0.01\n",
    "arguments[\"generator_learning_rate\"] = 0.00005\n",
    "arguments[\"generator_optimizer\"] = \"Adam\"\n",
    "arguments[\"generator_clip_gradients\"] = 5.0\n",
    "arguments[\"generator_clip_weights\"] = None\n",
    "arguments[\"generator_train_steps\"] = 1\n",
    "\n",
    "# Critic hyperparameters\n",
    "arguments[\"critic_num_filters\"] = [64, 128]\n",
    "arguments[\"critic_kernel_sizes\"] = [5, 5]\n",
    "arguments[\"critic_strides\"] = [2, 2]\n",
    "arguments[\"critic_dropout_rates\"] = [0.3, 0.3]\n",
    "arguments[\"critic_l1_regularization_scale\"] = 0.01\n",
    "arguments[\"critic_l2_regularization_scale\"] = 0.01\n",
    "arguments[\"critic_learning_rate\"] = 0.00005\n",
    "arguments[\"critic_optimizer\"] = \"RMSProp\"\n",
    "arguments[\"critic_clip_gradients\"] = 5.0\n",
    "arguments[\"critic_clip_weights\"] = [-0.01, 0.01]\n",
    "arguments[\"critic_train_steps\"] = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_obj(function_name, object_name, object_value):\n",
    "    \"\"\"Prints enclosing function, object name, and object value.\n",
    "\n",
    "    Args:\n",
    "        function_name: str, name of function.\n",
    "        object_name: str, name of object.\n",
    "        object_value: object, value of passed object.\n",
    "    \"\"\"\n",
    "#     pass\n",
    "    print(\"{}: {} = {}\".format(function_name, object_name, object_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "    print_obj(\"\\ndecode_example\", \"features\", features)\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "    print_obj(\"decode_example\", \"image\", image)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "    print_obj(\"decode_example\", \"label\", label)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    \"\"\"Reads CSV time series data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read CSV dataset\n",
    "    using Dataset API, apply necessary preprocessing, and return an input\n",
    "    function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        mode: The estimator ModeKeys. Can be TRAIN or EVAL.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create list of files that match pattern.\n",
    "        file_list = tf.gfile.Glob(filename=filename)\n",
    "\n",
    "        # Create dataset from file list.\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=file_list, num_parallel_reads=40\n",
    "        )\n",
    "\n",
    "        # Shuffle and repeat if training with fused op.\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(\n",
    "                    buffer_size=50 * batch_size,\n",
    "                    count=None  # indefinitely\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decode CSV file into a features dictionary of tensors, then batch.\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                map_func=lambda x: decode_example(\n",
    "                    protos=x,\n",
    "                    params=params\n",
    "                ),\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=4\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(buffer_size=2)\n",
    "\n",
    "        # Create a iterator, then get batch of features from example queue.\n",
    "        batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return batched_dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_network(Z, mode, params, reuse=False):\n",
    "    \"\"\"Creates generator network and returns generated output.\n",
    "\n",
    "    Args:\n",
    "        Z: tensor, latent vectors of shape [cur_batch_size, latent_size].\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "        reuse: bool, whether to reuse variables or not.\n",
    "\n",
    "    Returns:\n",
    "        Generated outputs tensor of shape\n",
    "            [cur_batch_size, height * width * depth].\n",
    "    \"\"\"\n",
    "    # Create regularizer for dense layer kernel weights.\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "        scale_l1=params[\"generator_l1_regularization_scale\"],\n",
    "        scale_l2=params[\"generator_l2_regularization_scale\"]\n",
    "    )\n",
    "\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        # Project latent vectors.\n",
    "        projection_height = params[\"generator_projection_dims\"][0]\n",
    "        projection_width = params[\"generator_projection_dims\"][1]\n",
    "        projection_depth = params[\"generator_projection_dims\"][2]\n",
    "\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height * projection_width * projection_depth\n",
    "        # )\n",
    "        projection = tf.layers.dense(\n",
    "            inputs=Z,\n",
    "            units=projection_height * projection_width * projection_depth,\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"projection_layer\"\n",
    "        )\n",
    "        print_obj(\"generator_network\", \"projection\", projection)\n",
    "\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height * projection_width * projection_depth\n",
    "        # )\n",
    "        projection_batch_norm = tf.layers.batch_normalization(\n",
    "            inputs=projection,\n",
    "            training=(mode == tf.estimator.ModeKeys.TRAIN),\n",
    "            name=\"projection_batch_norm\"\n",
    "        )\n",
    "        print_obj(\n",
    "            \"generator_network\",\n",
    "            \"projection_batch_norm\",\n",
    "            projection_batch_norm\n",
    "        )\n",
    "\n",
    "        # Reshape projection into \"image\".\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     projection_height,\n",
    "        #     projection_width,\n",
    "        #     projection_depth\n",
    "        # )\n",
    "        network = tf.reshape(\n",
    "            tensor=projection_batch_norm,\n",
    "            shape=[-1, projection_height, projection_width, projection_depth],\n",
    "            name=\"projection_reshaped\"\n",
    "        )\n",
    "        print_obj(\"generator_network\", \"network\", network)\n",
    "\n",
    "        # Iteratively build upsampling layers.\n",
    "        for i in range(len(params[\"generator_num_filters\"])):\n",
    "            # Add convolutional transpose layers with given params per layer.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     generator_kernel_sizes[i - 1] * generator_strides[i],\n",
    "            #     generator_kernel_sizes[i - 1] * generator_strides[i],\n",
    "            #     generator_num_filters[i]\n",
    "            # )\n",
    "            network = tf.layers.conv2d_transpose(\n",
    "                inputs=network,\n",
    "                filters=params[\"generator_num_filters\"][i],\n",
    "                kernel_size=params[\"generator_kernel_sizes\"][i],\n",
    "                strides=params[\"generator_strides\"][i],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                use_bias=False,\n",
    "                kernel_regularizer=regularizer,\n",
    "                name=\"layers_conv2d_tranpose_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"generator_network\", \"network\", network)\n",
    "\n",
    "            # Add batch normalization to keep the inputs from blowing up.\n",
    "            network = tf.layers.batch_normalization(\n",
    "                inputs=network,\n",
    "                training=(mode == tf.estimator.ModeKeys.TRAIN),\n",
    "                name=\"layers_batch_norm_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"generator_network\", \"network\", network)\n",
    "\n",
    "        # Final conv2d transpose layer for image output.\n",
    "        # shape = (cur_batch_size, height * width * depth)\n",
    "        generated_outputs = tf.layers.conv2d_transpose(\n",
    "                inputs=network,\n",
    "                filters=params[\"generator_final_num_filters\"],\n",
    "                kernel_size=params[\"generator_final_kernel_size\"],\n",
    "                strides=params[\"generator_final_stride\"],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.tanh,\n",
    "                use_bias=False,\n",
    "                kernel_regularizer=regularizer,\n",
    "                name=\"layers_conv2d_tranpose_generated_outputs\"\n",
    "        )\n",
    "        print_obj(\"generator_network\", \"generated_outputs\", generated_outputs)\n",
    "\n",
    "    return generated_outputs\n",
    "\n",
    "\n",
    "def get_generator_loss(generated_logits):\n",
    "    \"\"\"Gets generator loss.\n",
    "\n",
    "    Args:\n",
    "        generated_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "\n",
    "    Returns:\n",
    "        Tensor of generator's total loss of shape [].\n",
    "    \"\"\"\n",
    "    # Calculate base generator loss.\n",
    "    generator_loss = -tf.reduce_mean(\n",
    "        input_tensor=generated_logits,\n",
    "        name=\"generator_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\nget_generator_loss\",\n",
    "        \"generator_loss\",\n",
    "        generator_loss\n",
    "    )\n",
    "\n",
    "    # Get regularization losses.\n",
    "    generator_regularization_loss = tf.losses.get_regularization_loss(\n",
    "        scope=\"generator\",\n",
    "        name=\"generator_regularization_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_generator_loss\",\n",
    "        \"generator_regularization_loss\",\n",
    "        generator_regularization_loss\n",
    "    )\n",
    "\n",
    "    # Combine losses for total losses.\n",
    "    generator_total_loss = tf.math.add(\n",
    "        x=generator_loss,\n",
    "        y=generator_regularization_loss,\n",
    "        name=\"generator_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_generator_loss\", \"generator_total_loss\", generator_total_loss\n",
    "    )\n",
    "\n",
    "    return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## critic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_network(X, params, reuse=False):\n",
    "    \"\"\"Creates critic network and returns logits.\n",
    "\n",
    "    Args:\n",
    "        X: tensor, image tensors of shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        params: dict, user passed parameters.\n",
    "        reuse: bool, whether to reuse variables or not.\n",
    "\n",
    "    Returns:\n",
    "        Logits tensor of shape [cur_batch_size, 1].\n",
    "    \"\"\"\n",
    "    # Create the input layer to our DNN.\n",
    "    # shape = (cur_batch_size, height * width * depth)\n",
    "    network = X\n",
    "    print_obj(\"\\ncritic_network\", \"network\", network)\n",
    "\n",
    "    # Create regularizer for dense layer kernel weights.\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "        scale_l1=params[\"critic_l1_regularization_scale\"],\n",
    "        scale_l2=params[\"critic_l2_regularization_scale\"]\n",
    "    )\n",
    "\n",
    "    with tf.variable_scope(\"critic\", reuse=reuse):\n",
    "        # Iteratively build downsampling layers.\n",
    "        for i in range(len(params[\"critic_num_filters\"])):\n",
    "            # Add convolutional transpose layers with given params per layer.\n",
    "            # shape = (\n",
    "            #     cur_batch_size,\n",
    "            #     critic_kernel_sizes[i - 1] / critic_strides[i],\n",
    "            #     critic_kernel_sizes[i - 1] / critic_strides[i],\n",
    "            #     critic_num_filters[i]\n",
    "            # )\n",
    "            network = tf.layers.conv2d(\n",
    "                inputs=network,\n",
    "                filters=params[\"critic_num_filters\"][i],\n",
    "                kernel_size=params[\"critic_kernel_sizes\"][i],\n",
    "                strides=params[\"critic_strides\"][i],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_regularizer=regularizer,\n",
    "                name=\"layers_conv2d_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"critic_network\", \"network\", network)\n",
    "\n",
    "            # Add some dropout for better regularization and stability.\n",
    "            network = tf.layers.dropout(\n",
    "                inputs=network,\n",
    "                rate=params[\"critic_dropout_rates\"][i],\n",
    "                name=\"layers_dropout_{}\".format(i)\n",
    "            )\n",
    "            print_obj(\"critic_network\", \"network\", network)\n",
    "\n",
    "        # Flatten network output.\n",
    "        # shape = (\n",
    "        #     cur_batch_size,\n",
    "        #     (critic_kernel_sizes[-2] / critic_strides[-1]) ** 2 * critic_num_filters[-1]\n",
    "        # )\n",
    "        network_flat = tf.layers.Flatten()(inputs=network)\n",
    "        print_obj(\"critic_network\", \"network_flat\", network_flat)\n",
    "\n",
    "        # Final linear layer for logits.\n",
    "        # shape = (cur_batch_size, 1)\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=network_flat,\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=\"layers_dense_logits\"\n",
    "        )\n",
    "        print_obj(\"critic_network\", \"logits\", logits)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def get_critic_loss(generated_logits, real_logits):\n",
    "    \"\"\"Gets critic loss.\n",
    "\n",
    "    Args:\n",
    "        generated_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "        real_logits: tensor, shape of\n",
    "            [cur_batch_size, height * width * depth].\n",
    "\n",
    "    Returns:\n",
    "        Tensor of critic's total loss of shape [].\n",
    "    \"\"\"\n",
    "    # Calculate base critic loss.\n",
    "    critic_real_loss = tf.reduce_mean(\n",
    "        input_tensor=real_logits,\n",
    "        name=\"critic_real_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"\\nget_critic_loss\",\n",
    "        \"critic_real_loss\",\n",
    "        critic_real_loss\n",
    "    )\n",
    "\n",
    "    critic_generated_loss = tf.reduce_mean(\n",
    "        input_tensor=generated_logits,\n",
    "        name=\"critic_generated_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_critic_loss\",\n",
    "        \"critic_generated_loss\",\n",
    "        critic_generated_loss\n",
    "    )\n",
    "\n",
    "    critic_loss = tf.add(\n",
    "        x=critic_real_loss, y=-critic_generated_loss,\n",
    "        name=\"critic_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_critic_loss\",\n",
    "        \"critic_loss\",\n",
    "        critic_loss\n",
    "    )\n",
    "\n",
    "    # Get regularization losses.\n",
    "    critic_regularization_loss = tf.losses.get_regularization_loss(\n",
    "        scope=\"critic\",\n",
    "        name=\"critic_regularization_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_critic_loss\",\n",
    "        \"critic_regularization_loss\",\n",
    "        critic_regularization_loss\n",
    "    )\n",
    "\n",
    "    # Combine losses for total losses.\n",
    "    critic_total_loss = tf.math.add(\n",
    "        x=critic_loss,\n",
    "        y=critic_regularization_loss,\n",
    "        name=\"critic_total_loss\"\n",
    "    )\n",
    "    print_obj(\n",
    "        \"get_critic_loss\",\n",
    "        \"critic_total_loss\",\n",
    "        critic_total_loss\n",
    "    )\n",
    "\n",
    "    return critic_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(loss, global_step, params, scope):\n",
    "    \"\"\"Trains network and returns loss and train op.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        global_step: tensor, the current training step or batch in the\n",
    "            training loop.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the variables that to train.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor and training op.\n",
    "    \"\"\"\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adam\": tf.train.AdamOptimizer,\n",
    "        \"Adadelta\": tf.train.AdadeltaOptimizer,\n",
    "        \"AdagradDA\": tf.train.AdagradDAOptimizer,\n",
    "        \"Adagrad\": tf.train.AdagradOptimizer,\n",
    "        \"Ftrl\": tf.train.FtrlOptimizer,\n",
    "        \"GradientDescent\": tf.train.GradientDescentOptimizer,\n",
    "        \"Momentum\": tf.train.MomentumOptimizer,\n",
    "        \"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer,\n",
    "        \"ProximalGradientDescent\": tf.train.ProximalGradientDescentOptimizer,\n",
    "        \"RMSProp\": tf.train.RMSPropOptimizer\n",
    "    }\n",
    "\n",
    "    # Get gradients.\n",
    "    gradients = tf.gradients(\n",
    "        ys=loss,\n",
    "        xs=tf.trainable_variables(scope=scope),\n",
    "        name=\"{}_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    # Clip gradients.\n",
    "    if params[\"{}_clip_gradients\".format(scope)]:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=params[\"{}_clip_gradients\".format(scope)],\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "\n",
    "    # Zip back together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, tf.trainable_variables(scope=scope))\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "        learning_rate=params[\"{}_learning_rate\".format(scope)]\n",
    "    )\n",
    "\n",
    "    # Create train op by applying gradients to variables and incrementing\n",
    "    # global step.\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        global_step=global_step,\n",
    "        name=\"{}_apply_gradients\".format(scope)\n",
    "    )\n",
    "\n",
    "    if params[\"{}_clip_weights\".format(scope)]:\n",
    "        clipped_weight_vars = [\n",
    "            tf.clip_by_value(\n",
    "                t=tensor,\n",
    "                clip_value_min=params[\"{}_clip_weights\".format(scope)][0],\n",
    "                clip_value_max=params[\"{}_clip_weights\".format(scope)][1],\n",
    "                name=\"{}_clip_by_value_weights\".format(scope)\n",
    "            )\n",
    "            for tensor in tf.trainable_variables(scope=scope)\n",
    "        ]\n",
    "\n",
    "        with tf.control_dependencies(control_inputs=clipped_weight_vars):\n",
    "            return loss, train_op\n",
    "    return loss, train_op\n",
    "\n",
    "\n",
    "def wgan_model(features, labels, mode, params):\n",
    "    \"\"\"Wasserstein GAN custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        features: dict, keys are feature names and values are feature tensors.\n",
    "        labels: tensor, label data.\n",
    "        mode: tf.estimator.ModeKeys with values of either TRAIN, EVAL, or\n",
    "            PREDICT.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `tf.estimator.EstimatorSpec` class.\n",
    "    \"\"\"\n",
    "    print_obj(\"\\nwgan_model\", \"features\", features)\n",
    "    print_obj(\"wgan_model\", \"labels\", labels)\n",
    "    print_obj(\"wgan_model\", \"mode\", mode)\n",
    "    print_obj(\"wgan_model\", \"params\", params)\n",
    "\n",
    "    # Loss function, training/eval ops, etc.\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Extract given latent vectors from features dictionary.\n",
    "        Z = tf.cast(x=features[\"Z\"], dtype=tf.float32)\n",
    "\n",
    "        # Get predictions from generator.\n",
    "        generated_images = generator_network(Z, mode, params, reuse=False)\n",
    "\n",
    "        # Create predictions dictionary.\n",
    "        predictions_dict = {\n",
    "            \"generated_images\": generated_images\n",
    "        }\n",
    "\n",
    "        # Create export outputs.\n",
    "        export_outputs = {\n",
    "            \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "                outputs=predictions_dict)\n",
    "        }\n",
    "    else:\n",
    "        # Extract image from features dictionary.\n",
    "        X = features[\"image\"]\n",
    "\n",
    "        # Get dynamic batch size in case of partial batch.\n",
    "        cur_batch_size = tf.shape(\n",
    "            input=X,\n",
    "            out_type=tf.int32,\n",
    "            name=\"wgan_model_cur_batch_size\"\n",
    "        )[0]\n",
    "\n",
    "        # Create random noise latent vector for each batch example.\n",
    "        Z = tf.random.normal(\n",
    "            shape=[cur_batch_size, params[\"latent_size\"]],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Establish generator network subgraph.\n",
    "        generator_outputs = generator_network(Z, mode, params, reuse=False)\n",
    "\n",
    "        # Establish critic network subgraph.\n",
    "        real_logits = critic_network(X, params, reuse=False)\n",
    "\n",
    "        # Get generated logits too.\n",
    "        generated_logits = critic_network(\n",
    "            generator_outputs, params, reuse=True\n",
    "        )\n",
    "\n",
    "        # Get generator total loss.\n",
    "        generator_total_loss = get_generator_loss(generated_logits)\n",
    "\n",
    "        # Get critic total loss.\n",
    "        critic_total_loss = get_critic_loss(\n",
    "            generated_logits, real_logits\n",
    "        )\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Get global step.\n",
    "            global_step = tf.train.get_global_step()\n",
    "\n",
    "            # Determine if it is time to train generator or critic.\n",
    "            cycle_step = tf.mod(\n",
    "                x=global_step,\n",
    "                y=tf.cast(\n",
    "                    x=tf.add(\n",
    "                        x=params[\"generator_train_steps\"],\n",
    "                        y=params[\"critic_train_steps\"]\n",
    "                    ),\n",
    "                    dtype=tf.int64\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create choose generator condition.\n",
    "            condition = tf.less(\n",
    "                x=cycle_step, y=params[\"generator_train_steps\"]\n",
    "            )\n",
    "\n",
    "            # Needed for batch normalization, but has no effect otherwise.\n",
    "            update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(control_inputs=update_ops):\n",
    "                # Conditionally choose to train generator or critic.\n",
    "                loss, train_op = tf.cond(\n",
    "                    pred=condition,\n",
    "                    true_fn=lambda: train_network(\n",
    "                        loss=generator_total_loss,\n",
    "                        global_step=global_step,\n",
    "                        params=params,\n",
    "                        scope=\"generator\"\n",
    "                    ),\n",
    "                    false_fn=lambda: train_network(\n",
    "                        loss=critic_total_loss,\n",
    "                        global_step=global_step,\n",
    "                        params=params,\n",
    "                        scope=\"critic\"\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            loss = critic_total_loss\n",
    "\n",
    "            # Concatenate critic logits and labels.\n",
    "            critic_logits = tf.concat(\n",
    "                values=[real_logits, generated_logits],\n",
    "                axis=0,\n",
    "                name=\"critic_concat_logits\"\n",
    "            )\n",
    "\n",
    "            critic_labels = tf.concat(\n",
    "                values=[\n",
    "                    tf.ones_like(tensor=real_logits),\n",
    "                    tf.zeros_like(tensor=generated_logits)\n",
    "                ],\n",
    "                axis=0,\n",
    "                name=\"critic_concat_labels\"\n",
    "            )\n",
    "\n",
    "            # Calculate critic probabilities.\n",
    "            critic_probabilities = tf.nn.sigmoid(\n",
    "                x=critic_logits, name=\"critic_probabilities\"\n",
    "            )\n",
    "\n",
    "            # Create eval metric ops dictionary.\n",
    "            eval_metric_ops = {\n",
    "                \"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=critic_labels,\n",
    "                    predictions=critic_probabilities,\n",
    "                    name=\"wgan_model_accuracy\"\n",
    "                ),\n",
    "                \"precision\": tf.metrics.precision(\n",
    "                    labels=critic_labels,\n",
    "                    predictions=critic_probabilities,\n",
    "                    name=\"wgan_model_precision\"\n",
    "                ),\n",
    "                \"recall\": tf.metrics.recall(\n",
    "                    labels=critic_labels,\n",
    "                    predictions=critic_probabilities,\n",
    "                    name=\"wgan_model_recall\"\n",
    "                ),\n",
    "                \"auc_roc\": tf.metrics.auc(\n",
    "                    labels=critic_labels,\n",
    "                    predictions=critic_probabilities,\n",
    "                    num_thresholds=200,\n",
    "                    curve=\"ROC\",\n",
    "                    name=\"wgan_model_auc_roc\"\n",
    "                ),\n",
    "                \"auc_pr\": tf.metrics.auc(\n",
    "                    labels=critic_labels,\n",
    "                    predictions=critic_probabilities,\n",
    "                    num_thresholds=200,\n",
    "                    curve=\"PR\",\n",
    "                    name=\"wgan_model_auc_pr\"\n",
    "                )\n",
    "            }\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs=export_outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn(params):\n",
    "    \"\"\"Serving input function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        ServingInputReceiver object containing features and receiver tensors.\n",
    "    \"\"\"\n",
    "    # Create placeholders to accept data sent to the model at serving time.\n",
    "    # shape = (batch_size,)\n",
    "    feature_placeholders = {\n",
    "        \"Z\": tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, params[\"latent_size\"]],\n",
    "            name=\"serving_input_placeholder_Z\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"feature_placeholders\",\n",
    "        feature_placeholders\n",
    "    )\n",
    "\n",
    "    # Create clones of the feature placeholder tensors so that the SavedModel\n",
    "    # SignatureDef will point to the placeholder.\n",
    "    features = {\n",
    "        key: tf.identity(\n",
    "            input=value,\n",
    "            name=\"serving_input_fn_identity_placeholder_{}\".format(key)\n",
    "        )\n",
    "        for key, value in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    print_obj(\n",
    "        \"serving_input_fn\",\n",
    "        \"features\",\n",
    "        features\n",
    "    )\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=feature_placeholders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates custom Estimator model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        `Estimator` object.\n",
    "    \"\"\"\n",
    "    # Set logging to be level of INFO.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Create our custom estimator using our model function.\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=wgan_model,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        params=args\n",
    "    )\n",
    "\n",
    "    # Create train spec to read in our training data.\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset(\n",
    "            filename=\"data/train.tfrecord\",\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size=args[\"train_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        max_steps=args[\"train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Create exporter to save out the complete model to disk.\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name=\"exporter\",\n",
    "        serving_input_receiver_fn=lambda: serving_input_fn(args)\n",
    "    )\n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model.\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset(\n",
    "            filename=\"data/test.tfrecord\",\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size=args[\"eval_batch_size\"],\n",
    "            params=args\n",
    "        ),\n",
    "        steps=args[\"eval_steps\"],\n",
    "        start_delay_secs=args[\"start_delay_secs\"],\n",
    "        throttle_secs=args[\"throttle_secs\"],\n",
    "        exporters=exporter\n",
    "    )\n",
    "\n",
    "    # Create train and evaluate loop to train and evaluate our estimator.\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fec3d6a1ed0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:87: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/shuffle_ops.py:54: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:99: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "\n",
      "decode_example: features = {'image_raw': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "decode_example: image = Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "decode_example: image = Tensor(\"Reshape:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "decode_example: image = Tensor(\"sub:0\", shape=(32, 32, 3), dtype=float32)\n",
      "decode_example: label = Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "WARNING:tensorflow:From <ipython-input-4-90b050af9c1b>:107: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "wgan_model: features = {'image': <tf.Tensor 'IteratorGetNext:0' shape=(?, 32, 32, 3) dtype=float32>}\n",
      "wgan_model: labels = Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "wgan_model: mode = train\n",
      "wgan_model: params = {'output_dir': 'trained_model', 'train_batch_size': 32, 'eval_batch_size': 32, 'train_steps': 200, 'eval_steps': 100, 'start_delay_secs': 60, 'throttle_secs': 120, 'height': 32, 'width': 32, 'depth': 3, 'latent_size': 512, 'generator_projection_dims': [8, 8, 256], 'generator_num_filters': [128, 64], 'generator_kernel_sizes': [5, 5], 'generator_strides': [1, 2], 'generator_final_num_filters': 3, 'generator_final_kernel_size': 5, 'generator_final_stride': 2, 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_learning_rate': 5e-05, 'generator_optimizer': 'Adam', 'generator_clip_gradients': 5.0, 'generator_clip_weights': None, 'generator_train_steps': 1, 'critic_num_filters': [64, 128], 'critic_kernel_sizes': [5, 5], 'critic_strides': [2, 2], 'critic_dropout_rates': [0.3, 0.3], 'critic_l1_regularization_scale': 0.01, 'critic_l2_regularization_scale': 0.01, 'critic_learning_rate': 5e-05, 'critic_optimizer': 'RMSProp', 'critic_clip_gradients': 5.0, 'critic_clip_weights': [-0.01, 0.01], 'critic_train_steps': 5}\n",
      "WARNING:tensorflow:From <ipython-input-5-49614946eb1d>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "generator_network: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 16384), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-49614946eb1d>:46: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "generator_network: projection_batch_norm = Tensor(\"generator/projection_batch_norm/batchnorm/add_1:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/projection_reshaped:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-49614946eb1d>:86: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_0/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_0/FusedBatchNormV3:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_1/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_1/FusedBatchNormV3:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "\n",
      "critic_network: network = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:From <ipython-input-6-5bf65b988ed3>:42: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "critic_network: network = Tensor(\"critic/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-6-5bf65b988ed3>:50: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "critic_network: network = Tensor(\"critic/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network_flat = Tensor(\"critic/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "critic_network: logits = Tensor(\"critic/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "critic_network: network = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network_flat = Tensor(\"critic_1/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "critic_network: logits = Tensor(\"critic_1/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "get_generator_loss: generator_loss = Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_regularization_loss = Tensor(\"generator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_total_loss = Tensor(\"generator_total_loss:0\", shape=(), dtype=float32)\n",
      "\n",
      "get_critic_loss: critic_real_loss = Tensor(\"critic_real_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_generated_loss = Tensor(\"critic_generated_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_loss = Tensor(\"critic_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_regularization_loss = Tensor(\"critic_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_total_loss = Tensor(\"critic_total_loss:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 141.36847, step = 1\n",
      "INFO:tensorflow:global_step/sec: 8.55978\n",
      "INFO:tensorflow:loss = 37.41293, step = 101 (11.684 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into trained_model/model.ckpt.\n",
      "\n",
      "decode_example: features = {'image_raw': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "decode_example: image = Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "decode_example: image = Tensor(\"Reshape:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "decode_example: image = Tensor(\"sub:0\", shape=(32, 32, 3), dtype=float32)\n",
      "decode_example: label = Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "wgan_model: features = {'image': <tf.Tensor 'IteratorGetNext:0' shape=(?, 32, 32, 3) dtype=float32>}\n",
      "wgan_model: labels = Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "wgan_model: mode = eval\n",
      "wgan_model: params = {'output_dir': 'trained_model', 'train_batch_size': 32, 'eval_batch_size': 32, 'train_steps': 200, 'eval_steps': 100, 'start_delay_secs': 60, 'throttle_secs': 120, 'height': 32, 'width': 32, 'depth': 3, 'latent_size': 512, 'generator_projection_dims': [8, 8, 256], 'generator_num_filters': [128, 64], 'generator_kernel_sizes': [5, 5], 'generator_strides': [1, 2], 'generator_final_num_filters': 3, 'generator_final_kernel_size': 5, 'generator_final_stride': 2, 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_learning_rate': 5e-05, 'generator_optimizer': 'Adam', 'generator_clip_gradients': 5.0, 'generator_clip_weights': None, 'generator_train_steps': 1, 'critic_num_filters': [64, 128], 'critic_kernel_sizes': [5, 5], 'critic_strides': [2, 2], 'critic_dropout_rates': [0.3, 0.3], 'critic_l1_regularization_scale': 0.01, 'critic_l2_regularization_scale': 0.01, 'critic_learning_rate': 5e-05, 'critic_optimizer': 'RMSProp', 'critic_clip_gradients': 5.0, 'critic_clip_weights': [-0.01, 0.01], 'critic_train_steps': 5}\n",
      "generator_network: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: projection_batch_norm = Tensor(\"generator/projection_batch_norm/batchnorm/add_1:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/projection_reshaped:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_0/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_0/FusedBatchNormV3:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_1/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_1/FusedBatchNormV3:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "\n",
      "critic_network: network = Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32, device=/device:CPU:0)\n",
      "critic_network: network = Tensor(\"critic/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network_flat = Tensor(\"critic/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "critic_network: logits = Tensor(\"critic/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "critic_network: network = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_conv2d_0/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_dropout_0/Identity:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_conv2d_1/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network = Tensor(\"critic_1/layers_dropout_1/Identity:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "critic_network: network_flat = Tensor(\"critic_1/flatten/Reshape:0\", shape=(?, 8192), dtype=float32)\n",
      "critic_network: logits = Tensor(\"critic_1/layers_dense_logits/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "get_generator_loss: generator_loss = Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_regularization_loss = Tensor(\"generator_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_generator_loss: generator_total_loss = Tensor(\"generator_total_loss:0\", shape=(), dtype=float32)\n",
      "\n",
      "get_critic_loss: critic_real_loss = Tensor(\"critic_real_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_generated_loss = Tensor(\"critic_generated_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_loss = Tensor(\"critic_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_regularization_loss = Tensor(\"critic_regularization_loss:0\", shape=(), dtype=float32)\n",
      "get_critic_loss: critic_total_loss = Tensor(\"critic_total_loss:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-06-01T05:39:38Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2020-06-01-05:39:45\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0, auc_pr = 0.394816, auc_roc = 0.104645215, global_step = 200, loss = 29.837385, precision = 0.5, recall = 1.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: trained_model/model.ckpt-200\n",
      "serving_input_fn: feature_placeholders = {'Z': <tf.Tensor 'serving_input_placeholder_Z:0' shape=(?, 512) dtype=float32>}\n",
      "serving_input_fn: features = {'Z': <tf.Tensor 'serving_input_fn_identity_placeholder_Z:0' shape=(?, 512) dtype=float32>}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "wgan_model: features = {'Z': <tf.Tensor 'serving_input_fn_identity_placeholder_Z:0' shape=(?, 512) dtype=float32>}\n",
      "wgan_model: labels = None\n",
      "wgan_model: mode = infer\n",
      "wgan_model: params = {'output_dir': 'trained_model', 'train_batch_size': 32, 'eval_batch_size': 32, 'train_steps': 200, 'eval_steps': 100, 'start_delay_secs': 60, 'throttle_secs': 120, 'height': 32, 'width': 32, 'depth': 3, 'latent_size': 512, 'generator_projection_dims': [8, 8, 256], 'generator_num_filters': [128, 64], 'generator_kernel_sizes': [5, 5], 'generator_strides': [1, 2], 'generator_final_num_filters': 3, 'generator_final_kernel_size': 5, 'generator_final_stride': 2, 'generator_l1_regularization_scale': 0.01, 'generator_l2_regularization_scale': 0.01, 'generator_learning_rate': 5e-05, 'generator_optimizer': 'Adam', 'generator_clip_gradients': 5.0, 'generator_clip_weights': None, 'generator_train_steps': 1, 'critic_num_filters': [64, 128], 'critic_kernel_sizes': [5, 5], 'critic_strides': [2, 2], 'critic_dropout_rates': [0.3, 0.3], 'critic_l1_regularization_scale': 0.01, 'critic_l2_regularization_scale': 0.01, 'critic_learning_rate': 5e-05, 'critic_optimizer': 'RMSProp', 'critic_clip_gradients': 5.0, 'critic_clip_weights': [-0.01, 0.01], 'critic_train_steps': 5}\n",
      "generator_network: projection = Tensor(\"generator/projection_layer/LeakyRelu:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: projection_batch_norm = Tensor(\"generator/projection_batch_norm/batchnorm/add_1:0\", shape=(?, 16384), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/projection_reshaped:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_0/LeakyRelu:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_0/FusedBatchNormV3:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_conv2d_tranpose_1/LeakyRelu:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: network = Tensor(\"generator/layers_batch_norm_1/FusedBatchNormV3:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "generator_network: generated_outputs = Tensor(\"generator/layers_conv2d_tranpose_generated_outputs/Tanh:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_model/export/exporter/temp-b'1590989985'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 29.41071.\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(path=arguments[\"output_dir\"], ignore_errors=True)\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590989985\n"
     ]
    }
   ],
   "source": [
    "!ls trained_model/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/export/exporter/1590989985/variables/variables\n"
     ]
    }
   ],
   "source": [
    "predict_fn = tf.contrib.predictor.from_saved_model(\n",
    "    \"trained_model/export/exporter/1590989985\"\n",
    ")\n",
    "predictions = predict_fn(\n",
    "    {\n",
    "        \"Z\": np.random.normal(size=(500, 512))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert image back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = np.clip(\n",
    "    a=((predictions[\"generated_images\"] + 1.0) * (255. / 2)).astype(np.int32),\n",
    "    a_min=0,\n",
    "    a_max=255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(generated_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(generated_images[i], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
