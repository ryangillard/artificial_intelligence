{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile README.md\n",
    "Implementation of [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/input.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def preprocess_image(image, params):\n",
    "    \"\"\"Preprocess image tensor.\n",
    "\n",
    "    Args:\n",
    "        image: tensor, input image with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed image tensor with shape\n",
    "            [cur_batch_size, height, width, depth].\n",
    "    \"\"\"\n",
    "    # Convert from [0, 255] -> [-1.0, 1.0] floats.\n",
    "    image = tf.cast(x=image, dtype=tf.float32) * (2. / 255) - 1.0\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def decode_example(protos, params):\n",
    "    \"\"\"Decodes TFRecord file into tensors.\n",
    "\n",
    "    Given protobufs, decode into image and label tensors.\n",
    "\n",
    "    Args:\n",
    "        protos: protobufs from TFRecord file.\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Image and label tensors.\n",
    "    \"\"\"\n",
    "    # Create feature schema map for protos.\n",
    "    features = {\n",
    "        \"image_raw\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse features from tf.Example.\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=protos, features=features\n",
    "    )\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length height * width * depth) to a uint8 tensor with shape\n",
    "    # [height * width * depth].\n",
    "    image = tf.io.decode_raw(\n",
    "        input_bytes=parsed_features[\"image_raw\"], out_type=tf.uint8\n",
    "    )\n",
    "\n",
    "    # Reshape flattened image back into normal dimensions.\n",
    "    image = tf.reshape(\n",
    "        tensor=image,\n",
    "        shape=[params[\"height\"], params[\"width\"], params[\"depth\"]]\n",
    "    )\n",
    "\n",
    "    # Preprocess image.\n",
    "    image = preprocess_image(image=image, params=params)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(x=parsed_features[\"label\"], dtype=tf.int32)\n",
    "\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "\n",
    "def read_dataset(filename, batch_size, params, training):\n",
    "    \"\"\"Reads TF Record data using tf.data, doing necessary preprocessing.\n",
    "\n",
    "    Given filename, mode, batch size, and other parameters, read TF Record\n",
    "    dataset using Dataset API, apply necessary preprocessing, and return an\n",
    "    input function to the Estimator API.\n",
    "\n",
    "    Args:\n",
    "        filename: str, file pattern that to read into our tf.data dataset.\n",
    "        batch_size: int, number of examples per batch.\n",
    "        params: dict, dictionary of user passed parameters.\n",
    "        training: bool, if training or not.\n",
    "\n",
    "    Returns:\n",
    "        An input function.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"Wrapper input function used by Estimator API to get data tensors.\n",
    "\n",
    "        Returns:\n",
    "            Batched dataset object of dictionary of feature tensors and label\n",
    "                tensor.\n",
    "        \"\"\"\n",
    "        # Create list of files that match pattern.\n",
    "        file_list = tf.data.Dataset.list_files(file_pattern=filename)\n",
    "\n",
    "        # Create dataset from file list.\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames=file_list,\n",
    "            num_parallel_reads=(\n",
    "                tf.contrib.data.AUTOTUNE\n",
    "                if params[\"input_fn_autotune\"]\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Shuffle and repeat if training with fused op.\n",
    "        if training:\n",
    "            dataset = dataset.apply(\n",
    "                tf.data.experimental.shuffle_and_repeat(\n",
    "                    buffer_size=50 * batch_size,\n",
    "                    count=None  # indefinitely\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decode CSV file into a features dictionary of tensors, then batch.\n",
    "        dataset = dataset.apply(\n",
    "            tf.data.experimental.map_and_batch(\n",
    "                map_func=lambda x: decode_example(\n",
    "                    protos=x,\n",
    "                    params=params\n",
    "                ),\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=(\n",
    "                    tf.contrib.data.AUTOTUNE\n",
    "                    if params[\"input_fn_autotune\"]\n",
    "                    else None\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prefetch data to improve latency.\n",
    "        dataset = dataset.prefetch(\n",
    "            buffer_size=(\n",
    "                tf.data.experimental.AUTOTUNE\n",
    "                if params[\"input_fn_autotune\"]\n",
    "                else 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generators.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/generators.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    \"\"\"Generator that takes latent vector input and outputs image.\n",
    "\n",
    "    Fields:\n",
    "        name: str, name of `Generator`.\n",
    "        model: instance of generator `Model`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape,\n",
    "            kernel_regularizer,\n",
    "            bias_regularizer,\n",
    "            name,\n",
    "            params):\n",
    "        \"\"\"Instantiates and builds generator network.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of generator.\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        # Set name of generator.\n",
    "        self.name = name\n",
    "\n",
    "        # Instantiate generator `Model`.\n",
    "        self.model = self._define_generator(\n",
    "            input_shape, kernel_regularizer, bias_regularizer, params\n",
    "        )\n",
    "\n",
    "    def _define_generator(\n",
    "            self, input_shape, kernel_regularizer, bias_regularizer, params):\n",
    "        \"\"\"Defines generator network.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of latent vector input of shape\n",
    "                [batch_size, latent_size].\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to our DNN.\n",
    "        # shape = (batch_size, latent_size)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape, name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "        network = inputs\n",
    "\n",
    "        # Dictionary containing possible final activations.\n",
    "        final_activation_set = {\"sigmoid\", \"relu\", \"tanh\"}\n",
    "\n",
    "        # Add hidden layers with given number of units/neurons per layer.\n",
    "        for i, units in enumerate(params[\"generator_hidden_units\"]):\n",
    "            # shape = (batch_size, generator_hidden_units[i])\n",
    "            network = tf.keras.layers.Dense(\n",
    "                units=units,\n",
    "                activation=None,\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                bias_regularizer=bias_regularizer,\n",
    "                name=\"{}_layers_dense_{}\".format(self.name, i)\n",
    "            )(inputs=network)\n",
    "\n",
    "            network = tf.keras.layers.LeakyReLU(\n",
    "                alpha=params[\"generator_leaky_relu_alpha\"],\n",
    "                name=\"{}_leaky_relu_{}\".format(self.name, i)\n",
    "            )(inputs=network)\n",
    "\n",
    "        # Final linear layer for outputs.\n",
    "        # shape = (batch_size, height * width * depth)\n",
    "        generated_outputs = tf.keras.layers.Dense(\n",
    "            units=params[\"height\"] * params[\"width\"] * params[\"depth\"],\n",
    "            activation=(\n",
    "                params[\"generator_final_activation\"].lower()\n",
    "                if params[\"generator_final_activation\"].lower()\n",
    "                in final_activation_set\n",
    "                else None\n",
    "            ),\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            name=\"{}_layers_dense_generated_outputs\".format(self.name)\n",
    "        )(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs, outputs=generated_outputs, name=self.name\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"Returns generator's `Model` object.\n",
    "\n",
    "        Returns:\n",
    "            Generator's `Model` object.\n",
    "        \"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def get_generator_loss(\n",
    "        self,\n",
    "        global_batch_size,\n",
    "        fake_logits,\n",
    "        params,\n",
    "        global_step,\n",
    "        summary_file_writer\n",
    "    ):\n",
    "        \"\"\"Gets generator loss.\n",
    "\n",
    "        Args:\n",
    "            global_batch_size: int, global batch size for distribution.\n",
    "            fake_logits: tensor, shape of\n",
    "                [batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "            global_step: int, current global step for training.\n",
    "            summary_file_writer: summary file writer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of generator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        # Calculate base generator loss.\n",
    "        generator_loss = tf.nn.compute_average_loss(\n",
    "            per_example_loss=tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "                reduction=tf.keras.losses.Reduction.NONE\n",
    "            )(\n",
    "                y_true=tf.ones_like(input=fake_logits), y_pred=fake_logits\n",
    "            ),\n",
    "            global_batch_size=global_batch_size\n",
    "        )\n",
    "\n",
    "        # Get regularization losses.\n",
    "        generator_reg_loss = tf.nn.scale_regularization_loss(\n",
    "            regularization_loss=sum(self.model.losses)\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        generator_total_loss = tf.math.add(\n",
    "            x=generator_loss,\n",
    "            y=generator_reg_loss,\n",
    "            name=\"generator_total_loss\"\n",
    "        )\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        with summary_file_writer.as_default():\n",
    "            with tf.summary.record_if(\n",
    "                global_step % params[\"save_summary_steps\"] == 0\n",
    "            ):\n",
    "                tf.summary.scalar(\n",
    "                    name=\"losses/generator_loss\",\n",
    "                    data=generator_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    name=\"losses/generator_reg_loss\",\n",
    "                    data=generator_reg_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    name=\"optimized_losses/generator_total_loss\",\n",
    "                    data=generator_total_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                summary_file_writer.flush()\n",
    "\n",
    "        return generator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminators.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/discriminators.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Discriminator(object):\n",
    "    \"\"\"Discriminator that takes image input and outputs logits.\n",
    "\n",
    "    Fields:\n",
    "        name: str, name of `Discriminator`.\n",
    "        model: instance of discriminator `Model`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape,\n",
    "            kernel_regularizer,\n",
    "            bias_regularizer,\n",
    "            name,\n",
    "            params):\n",
    "        \"\"\"Instantiates and builds discriminator network.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of image vector input of shape\n",
    "                [batch_size, height * width * depth].\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            name: str, name of discriminator.\n",
    "            params: dict, user passed parameters.\n",
    "        \"\"\"\n",
    "        # Set name of discriminator.\n",
    "        self.name = name\n",
    "\n",
    "        # Regularizer for kernel weights.\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        # Regularizer for bias weights.\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        # Instantiate discriminator `Model`.\n",
    "        self.model = self._define_discriminator(\n",
    "            input_shape, kernel_regularizer, bias_regularizer, params\n",
    "        )\n",
    "\n",
    "    def _define_discriminator(\n",
    "            self, input_shape, kernel_regularizer, bias_regularizer, params):\n",
    "        \"\"\"Defines discriminator network.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple, shape of image vector input of shape\n",
    "                [batch_size, height * width * depth].\n",
    "            kernel_regularizer: `l1_l2_regularizer` object, regularizar for\n",
    "                kernel variables.\n",
    "            bias_regularizer: `l1_l2_regularizer` object, regularizar for bias\n",
    "                variables.\n",
    "            params: dict, user passed parameters.\n",
    "\n",
    "        Returns:\n",
    "            Instance of `Model` object.\n",
    "        \"\"\"\n",
    "        # Create the input layer to our DNN.\n",
    "        # shape = (batch_size, height * width * depth)\n",
    "        inputs = tf.keras.Input(\n",
    "            shape=input_shape,\n",
    "            name=\"{}_inputs\".format(self.name)\n",
    "        )\n",
    "        network = inputs\n",
    "\n",
    "        # Add hidden layers with given number of units/neurons per layer.\n",
    "        for i, units in enumerate(params[\"discriminator_hidden_units\"]):\n",
    "            # shape = (batch_size, discriminator_hidden_units[i])\n",
    "            network = tf.keras.layers.Dense(\n",
    "                units=units,\n",
    "                activation=None,\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                bias_regularizer=bias_regularizer,\n",
    "                name=\"{}_layers_dense_{}\".format(self.name, i)\n",
    "            )(inputs=network)\n",
    "\n",
    "            network = tf.keras.layers.LeakyReLU(\n",
    "                alpha=params[\"discriminator_leaky_relu_alpha\"],\n",
    "                name=\"{}_leaky_relu_{}\".format(self.name, i)\n",
    "            )(inputs=network)\n",
    "\n",
    "        # Final linear layer for logits.\n",
    "        # shape = (batch_size, 1)\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            name=\"{}_layers_dense_logits\".format(self.name)\n",
    "        )(inputs=network)\n",
    "\n",
    "        # Define model.\n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs, outputs=logits, name=self.name\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"Returns discriminator's `Model` object.\n",
    "\n",
    "        Returns:\n",
    "            Discriminator's `Model` object.\n",
    "        \"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def get_discriminator_loss(\n",
    "        self,\n",
    "        global_batch_size,\n",
    "        fake_logits,\n",
    "        real_logits,\n",
    "        params,\n",
    "        global_step,\n",
    "        summary_file_writer\n",
    "    ):\n",
    "        \"\"\"Gets discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            global_batch_size: int, global batch size for distribution.\n",
    "            fake_logits: tensor, shape of\n",
    "                [batch_size, 1].\n",
    "            real_logits: tensor, shape of\n",
    "                [batch_size, 1].\n",
    "            params: dict, user passed parameters.\n",
    "            global_step: int, current global step for training.\n",
    "            summary_file_writer: summary file writer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of discriminator's total loss of shape [].\n",
    "        \"\"\"\n",
    "        # Calculate base discriminator loss.\n",
    "        discriminator_real_loss = tf.nn.compute_average_loss(\n",
    "            per_example_loss=tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "                label_smoothing=params[\"label_smoothing\"],\n",
    "                reduction=tf.keras.losses.Reduction.NONE\n",
    "            )(\n",
    "                y_true=tf.ones_like(input=real_logits), y_pred=real_logits\n",
    "            ),\n",
    "            global_batch_size=global_batch_size\n",
    "        )\n",
    "\n",
    "        discriminator_fake_loss = tf.nn.compute_average_loss(\n",
    "            per_example_loss=tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "                reduction=tf.keras.losses.Reduction.NONE\n",
    "            )(\n",
    "                y_true=tf.zeros_like(input=fake_logits), y_pred=fake_logits\n",
    "            ),\n",
    "            global_batch_size=global_batch_size\n",
    "        )\n",
    "\n",
    "        discriminator_loss = tf.add(\n",
    "            x=discriminator_real_loss,\n",
    "            y=discriminator_fake_loss,\n",
    "            name=\"discriminator_loss\"\n",
    "        )\n",
    "\n",
    "        # Get regularization losses.\n",
    "        discriminator_reg_loss = tf.nn.scale_regularization_loss(\n",
    "            regularization_loss=sum(self.model.losses)\n",
    "        )\n",
    "\n",
    "        # Combine losses for total losses.\n",
    "        discriminator_total_loss = tf.math.add(\n",
    "            x=discriminator_loss,\n",
    "            y=discriminator_reg_loss,\n",
    "            name=\"discriminator_total_loss\"\n",
    "        )\n",
    "\n",
    "        # Add summaries for TensorBoard.\n",
    "        with summary_file_writer.as_default():\n",
    "            with tf.summary.record_if(\n",
    "                global_step % params[\"save_summary_steps\"] == 0\n",
    "            ):\n",
    "                tf.summary.scalar(\n",
    "                    name=\"losses/discriminator_real_loss\",\n",
    "                    data=discriminator_real_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    name=\"losses/discriminator_fake_loss\",\n",
    "                    data=discriminator_fake_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    name=\"losses/discriminator_loss\",\n",
    "                    data=discriminator_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    name=\"losses/discriminator_reg_loss\",\n",
    "                    data=discriminator_reg_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    name=\"optimized_losses/discriminator_total_loss\",\n",
    "                    data=discriminator_total_loss,\n",
    "                    step=global_step\n",
    "                )\n",
    "                summary_file_writer.flush()\n",
    "\n",
    "        return discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/train_and_eval.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generator_loss_phase(\n",
    "    global_batch_size,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer,\n",
    "    mode,\n",
    "    training\n",
    "):\n",
    "    \"\"\"Gets fake logits and loss for generator.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "        training: bool, if in training mode.\n",
    "\n",
    "    Returns:\n",
    "        Fake logits of shape [batch_size, 1] and generator loss.\n",
    "    \"\"\"\n",
    "    batch_size = (\n",
    "        params[\"train_batch_size\"]\n",
    "        if mode == \"TRAIN\"\n",
    "        else params[\"eval_batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Create random noise latent vector for each batch example.\n",
    "    Z = tf.random.normal(\n",
    "        shape=[batch_size, params[\"latent_size\"]],\n",
    "        mean=0.0,\n",
    "        stddev=1.0,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    # Get generated image from generator network from gaussian noise.\n",
    "    fake_images = generator.get_model()(inputs=Z, training=training)\n",
    "\n",
    "    if mode == \"TRAIN\":\n",
    "        # Add summaries for TensorBoard.\n",
    "        with summary_file_writer.as_default():\n",
    "            with tf.summary.record_if(\n",
    "                global_step % params[\"save_summary_steps\"] == 0\n",
    "            ):\n",
    "                tf.summary.image(\n",
    "                    name=\"fake_images\",\n",
    "                    data=tf.reshape(\n",
    "                        tensor=fake_images,\n",
    "                        shape=[\n",
    "                            -1,\n",
    "                            params[\"height\"],\n",
    "                            params[\"width\"],\n",
    "                            params[\"depth\"]\n",
    "                        ]\n",
    "                    ),\n",
    "                    step=global_step,\n",
    "                    max_outputs=5,\n",
    "                )\n",
    "                summary_file_writer.flush()\n",
    "\n",
    "    # Get fake logits from discriminator using generator's output image.\n",
    "    fake_logits = discriminator.get_model()(\n",
    "        inputs=fake_images, training=False\n",
    "    )\n",
    "\n",
    "    # Get generator total loss.\n",
    "    generator_total_loss = generator.get_generator_loss(\n",
    "        global_batch_size=global_batch_size,\n",
    "        fake_logits=fake_logits,\n",
    "        params=params,\n",
    "        global_step=global_step,\n",
    "        summary_file_writer=summary_file_writer\n",
    "    )\n",
    "\n",
    "    return fake_logits, generator_total_loss\n",
    "\n",
    "\n",
    "def discriminator_loss_phase(\n",
    "    global_batch_size,\n",
    "    discriminator,\n",
    "    real_images,\n",
    "    fake_logits,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer,\n",
    "    mode,\n",
    "    training\n",
    "):\n",
    "    \"\"\"Gets real logits and loss for discriminator.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        real_images: tensor, real images of shape\n",
    "            [batch_size, height * width * depth].\n",
    "        fake_logits: tensor, discriminator logits of fake images of shape\n",
    "            [batch_size, 1].\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "        training: bool, if in training mode.\n",
    "\n",
    "    Returns:\n",
    "        Real logits and discriminator loss.\n",
    "    \"\"\"\n",
    "    # Get real logits from discriminator using real image.\n",
    "    real_logits = discriminator.get_model()(\n",
    "        inputs=real_images, training=training\n",
    "    )\n",
    "\n",
    "    # Get discriminator total loss.\n",
    "    discriminator_total_loss = discriminator.get_discriminator_loss(\n",
    "        global_batch_size=global_batch_size,\n",
    "        fake_logits=fake_logits,\n",
    "        real_logits=real_logits,\n",
    "        params=params,\n",
    "        global_step=global_step,\n",
    "        summary_file_writer=summary_file_writer\n",
    "    )\n",
    "\n",
    "    return real_logits, discriminator_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/train.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import train_and_eval\n",
    "\n",
    "\n",
    "def get_variables_and_gradients(\n",
    "    loss,\n",
    "    network,\n",
    "    gradient_tape,\n",
    "    params,\n",
    "    scope\n",
    "):\n",
    "    \"\"\"Gets variables and gradients from model wrt. loss.\n",
    "\n",
    "    Args:\n",
    "        loss: tensor, shape of [].\n",
    "        network: instance of network; either `Generator` or `Discriminator`.\n",
    "        gradient_tape: instance of `GradientTape`.\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the name of the network of interest.\n",
    "\n",
    "    Returns:\n",
    "        Lists of network's variables and gradients.\n",
    "    \"\"\"\n",
    "    # Get trainable variables.\n",
    "    variables = network.get_model().trainable_variables\n",
    "\n",
    "    # Get gradients from gradient tape.\n",
    "    gradients = gradient_tape.gradient(\n",
    "        target=loss, sources=variables\n",
    "    )\n",
    "\n",
    "    # Clip gradients.\n",
    "    if params[\"{}_clip_gradients\".format(scope)]:\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            t_list=gradients,\n",
    "            clip_norm=params[\"{}_clip_gradients\".format(scope)],\n",
    "            name=\"{}_clip_by_global_norm_gradients\".format(scope)\n",
    "        )\n",
    "\n",
    "    # Add variable names back in for identification.\n",
    "    gradients = [\n",
    "        tf.identity(\n",
    "            input=g,\n",
    "            name=\"{}_{}_gradients\".format(scope, v.name[:-2])\n",
    "        )\n",
    "        if tf.is_tensor(x=g) else g\n",
    "        for g, v in zip(gradients, variables)\n",
    "    ]\n",
    "\n",
    "    return variables, gradients\n",
    "\n",
    "\n",
    "def get_generator_loss_variables_and_gradients(\n",
    "    global_batch_size,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    global_step,\n",
    "    summary_file_writer,\n",
    "    params\n",
    "):\n",
    "    \"\"\"Gets generator's loss, variables, and gradients.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "\n",
    "    Returns:\n",
    "        Generator's loss, variables, and gradients.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as generator_tape:\n",
    "        # Get generator loss.\n",
    "        _, generator_loss = train_and_eval.generator_loss_phase(\n",
    "            global_batch_size,\n",
    "            generator,\n",
    "            discriminator,\n",
    "            params,\n",
    "            global_step,\n",
    "            summary_file_writer,\n",
    "            mode=\"TRAIN\",\n",
    "            training=True\n",
    "        )\n",
    "\n",
    "    # Get variables and gradients from generator wrt. loss.\n",
    "    variables, gradients = get_variables_and_gradients(\n",
    "        loss=generator_loss,\n",
    "        network=generator,\n",
    "        gradient_tape=generator_tape,\n",
    "        params=params,\n",
    "        scope=\"generator\"\n",
    "    )\n",
    "\n",
    "    return generator_loss, variables, gradients\n",
    "\n",
    "\n",
    "def get_discriminator_loss_variables_and_gradients(\n",
    "    global_batch_size,\n",
    "    real_images,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    global_step,\n",
    "    summary_file_writer,\n",
    "    params\n",
    "):\n",
    "    \"\"\"Gets discriminator's loss, variables, and gradients.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        real_images: tensor, real images of shape\n",
    "            [batch_size, height * width * depth].\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "\n",
    "    Returns:\n",
    "        Discriminator's loss, variables, and gradients.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as discriminator_tape:\n",
    "        # Get fake logits from generator.\n",
    "        fake_logits, _ = train_and_eval.generator_loss_phase(\n",
    "            global_batch_size,\n",
    "            generator,\n",
    "            discriminator,\n",
    "            params,\n",
    "            global_step,\n",
    "            summary_file_writer,\n",
    "            mode=\"TRAIN\",\n",
    "            training=False\n",
    "        )\n",
    "\n",
    "        # Get discriminator loss.\n",
    "        _, discriminator_loss = train_and_eval.discriminator_loss_phase(\n",
    "            global_batch_size,\n",
    "            discriminator,\n",
    "            real_images,\n",
    "            fake_logits,\n",
    "            params,\n",
    "            global_step,\n",
    "            summary_file_writer,\n",
    "            mode=\"TRAIN\",\n",
    "            training=True\n",
    "        )\n",
    "\n",
    "    # Get variables and gradients from discriminator wrt. loss.\n",
    "    variables, gradients = get_variables_and_gradients(\n",
    "        loss=discriminator_loss,\n",
    "        network=discriminator,\n",
    "        gradient_tape=discriminator_tape,\n",
    "        params=params,\n",
    "        scope=\"discriminator\"\n",
    "    )\n",
    "\n",
    "    return discriminator_loss, variables, gradients\n",
    "\n",
    "\n",
    "def create_variable_and_gradient_histogram_summaries(\n",
    "    variables,\n",
    "    gradients,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer,\n",
    "    scope\n",
    "):\n",
    "    \"\"\"Creates variable and gradient histogram summaries.\n",
    "\n",
    "    Args:\n",
    "        variables: list, network's trainable variables.\n",
    "        gradients: list, gradients of networks trainable variables wrt. loss.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "        scope: str, the name of the network of interest.\n",
    "    \"\"\"\n",
    "    # Add summaries for TensorBoard.\n",
    "    with summary_file_writer.as_default():\n",
    "        with tf.summary.record_if(\n",
    "            global_step % params[\"save_summary_steps\"] == 0\n",
    "        ):\n",
    "            for v, g in zip(variables, gradients):\n",
    "                tf.summary.histogram(\n",
    "                    name=\"{}_variables/{}\".format(scope, v.name[:-2]),\n",
    "                    data=v,\n",
    "                    step=global_step\n",
    "                )\n",
    "                if tf.is_tensor(x=g):\n",
    "                    tf.summary.histogram(\n",
    "                        name=\"{}_gradients/{}\".format(scope, v.name[:-2]),\n",
    "                        data=g,\n",
    "                        step=global_step\n",
    "                    )\n",
    "            summary_file_writer.flush()\n",
    "\n",
    "\n",
    "def get_select_loss_variables_and_gradients(\n",
    "    global_batch_size,\n",
    "    real_images,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    global_step,\n",
    "    summary_file_writer,\n",
    "    params,\n",
    "    scope\n",
    "):\n",
    "    \"\"\"Gets selected network's loss, variables, and gradients.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        real_images: tensor, real images of shape\n",
    "            [batch_size, height * width * depth].\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "        scope: str, the name of the network of interest.\n",
    "\n",
    "    Returns:\n",
    "        Selected network's loss, variables, and gradients.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "        # Get fake logits from generator.\n",
    "        fake_logits, generator_loss = train_and_eval.generator_loss_phase(\n",
    "            global_batch_size,\n",
    "            generator,\n",
    "            discriminator,\n",
    "            params,\n",
    "            global_step,\n",
    "            summary_file_writer,\n",
    "            mode=\"TRAIN\",\n",
    "            training=(scope == \"generator\")\n",
    "        )\n",
    "\n",
    "        # Get discriminator loss.\n",
    "        _, discriminator_loss = train_and_eval.discriminator_loss_phase(\n",
    "            global_batch_size,\n",
    "            discriminator,\n",
    "            real_images,\n",
    "            fake_logits,\n",
    "            params,\n",
    "            global_step,\n",
    "            summary_file_writer,\n",
    "            mode=\"TRAIN\",\n",
    "            training=(scope == \"discriminator\")\n",
    "        )\n",
    "\n",
    "    # Create empty dicts to hold loss, variables, gradients.\n",
    "    loss_dict = {}\n",
    "    vars_dict = {}\n",
    "    grads_dict = {}\n",
    "\n",
    "    # Loop over generator and discriminator.\n",
    "    for (loss, network, gradient_tape, scope) in zip(\n",
    "        [generator_loss, discriminator_loss],\n",
    "        [generator, discriminator],\n",
    "        [gen_tape, dis_tape],\n",
    "        [\"generator\", \"discriminator\"]\n",
    "    ):\n",
    "        # Get variables and gradients from generator wrt. loss.\n",
    "        variables, gradients = get_variables_and_gradients(\n",
    "            loss, network, gradient_tape, params, scope\n",
    "        )\n",
    "\n",
    "        # Add loss, variables, and gradients to dictionaries.\n",
    "        loss_dict[scope] = loss\n",
    "        vars_dict[scope] = variables\n",
    "        grads_dict[scope] = gradients\n",
    "\n",
    "        # Create variable and gradient histogram summaries.\n",
    "        create_variable_and_gradient_histogram_summaries(\n",
    "            variables,\n",
    "            gradients,\n",
    "            params,\n",
    "            global_step,\n",
    "            summary_file_writer,\n",
    "            scope\n",
    "        )\n",
    "\n",
    "    return loss_dict[scope], vars_dict[scope], grads_dict[scope]\n",
    "\n",
    "\n",
    "def train_network(variables, gradients, optimizer):\n",
    "    \"\"\"Trains network variables using gradients with optimizer.\n",
    "\n",
    "    Args:\n",
    "        variables: list, network's trainable variables.\n",
    "        gradients: list, gradients of networks trainable variables wrt. loss.\n",
    "        optimizer: instance of `Optimizer`.\n",
    "    \"\"\"\n",
    "    # Zip together gradients and variables.\n",
    "    grads_and_vars = zip(gradients, variables)\n",
    "\n",
    "    # Applying gradients to variables using optimizer.\n",
    "    optimizer.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "\n",
    "\n",
    "def train_discriminator(\n",
    "    global_batch_size,\n",
    "    features,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    discriminator_optimizer,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer\n",
    "):\n",
    "    \"\"\"Trains discriminator network.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        features: dict, feature tensors from input function.\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        discriminator_optimizer: instance of `Optimizer`, discriminator's\n",
    "            optimizer.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "\n",
    "    Returns:\n",
    "        Discriminator loss tensor.\n",
    "    \"\"\"\n",
    "    # Extract real images from features dictionary.\n",
    "    real_images = tf.reshape(\n",
    "        tensor=features[\"image\"],\n",
    "        shape=[-1, params[\"height\"] * params[\"width\"] * params[\"depth\"]]\n",
    "    )\n",
    "\n",
    "    # Get gradients for training by running inputs through networks.\n",
    "    if global_step % params[\"save_summary_steps\"] == 0:\n",
    "        # More computation, but needed for ALL histogram summaries.\n",
    "        loss, variables, gradients = (\n",
    "            get_select_loss_variables_and_gradients(\n",
    "                global_batch_size,\n",
    "                real_images,\n",
    "                generator,\n",
    "                discriminator,\n",
    "                global_step,\n",
    "                summary_file_writer,\n",
    "                params,\n",
    "                scope=\"discriminator\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # More efficient computation.\n",
    "        loss, variables, gradients = (\n",
    "            get_discriminator_loss_variables_and_gradients(\n",
    "                global_batch_size,\n",
    "                real_images,\n",
    "                generator,\n",
    "                discriminator,\n",
    "                global_step,\n",
    "                summary_file_writer,\n",
    "                params\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Train discriminator network.\n",
    "    train_network(variables, gradients, optimizer=discriminator_optimizer)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_generator(\n",
    "    global_batch_size,\n",
    "    features,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    generator_optimizer,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer\n",
    "):\n",
    "    \"\"\"Trains generator network.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        features: dict, feature tensors from input function.\n",
    "        generator: instance of `Generator`.\n",
    "        discriminator: instance of `Discriminator`.\n",
    "        generator_optimizer: instance of `Optimizer`, generator's\n",
    "            optimizer.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "\n",
    "    Returns:\n",
    "        Generator loss tensor.\n",
    "    \"\"\"\n",
    "    # Get gradients for training by running inputs through networks.\n",
    "    if global_step % params[\"save_summary_steps\"] == 0:\n",
    "        # Extract real images from features dictionary.\n",
    "        real_images = tf.reshape(\n",
    "            tensor=features[\"image\"],\n",
    "            shape=[-1, params[\"height\"] * params[\"width\"] * params[\"depth\"]]\n",
    "        )\n",
    "\n",
    "        # More computation, but needed for ALL histogram summaries.\n",
    "        loss, variables, gradients = (\n",
    "            get_select_loss_variables_and_gradients(\n",
    "                global_batch_size,\n",
    "                real_images,\n",
    "                generator,\n",
    "                discriminator,\n",
    "                global_step,\n",
    "                summary_file_writer,\n",
    "                params,\n",
    "                scope=\"generator\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # More efficient computation.\n",
    "        loss, variables, gradients = (\n",
    "            get_generator_loss_variables_and_gradients(\n",
    "                global_batch_size,\n",
    "                generator,\n",
    "                discriminator,\n",
    "                global_step,\n",
    "                summary_file_writer,\n",
    "                params\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Train generator network.\n",
    "    train_network(variables, gradients, optimizer=generator_optimizer)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    global_batch_size,\n",
    "    features,\n",
    "    network_dict,\n",
    "    optimizer_dict,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer\n",
    "):\n",
    "    \"\"\"Perform one train step.\n",
    "\n",
    "    Args:\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        features: dict, feature tensors from input function.\n",
    "        network_dict: dict, dictionary of network objects.\n",
    "        optimizer_dict: dict, dictionary of optimizer objects.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "\n",
    "    Returns:\n",
    "        Loss tensor for chosen network.\n",
    "    \"\"\"\n",
    "    # Determine if it is time to train generator or discriminator.\n",
    "    cycle_step = global_step % (\n",
    "        params[\"discriminator_train_steps\"] + params[\"generator_train_steps\"]\n",
    "    )\n",
    "\n",
    "    # Conditionally choose to train generator or discriminator subgraph.\n",
    "    if cycle_step < params[\"discriminator_train_steps\"]:\n",
    "        loss = train_discriminator(\n",
    "            global_batch_size=global_batch_size,\n",
    "            features=features,\n",
    "            generator=network_dict[\"generator\"],\n",
    "            discriminator=network_dict[\"discriminator\"],\n",
    "            discriminator_optimizer=optimizer_dict[\"discriminator\"],\n",
    "            params=params,\n",
    "            global_step=global_step,\n",
    "            summary_file_writer=summary_file_writer\n",
    "        )\n",
    "    else:\n",
    "        loss = train_generator(\n",
    "            global_batch_size=global_batch_size,\n",
    "            features=features,\n",
    "            generator=network_dict[\"generator\"],\n",
    "            discriminator=network_dict[\"discriminator\"],\n",
    "            generator_optimizer=optimizer_dict[\"generator\"],\n",
    "            params=params,\n",
    "            global_step=global_step,\n",
    "            summary_file_writer=summary_file_writer\n",
    "        )\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla_gan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/vanilla_gan.py\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import discriminators\n",
    "from . import generators\n",
    "\n",
    "\n",
    "def instantiate_network_objects(params):\n",
    "    \"\"\"Instantiates generator and discriminator with parameters.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of instance of `Generator` and instance of `Discriminator`.\n",
    "    \"\"\"\n",
    "    # Instantiate generator.\n",
    "    generator = generators.Generator(\n",
    "        input_shape=(params[\"latent_size\"]),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "            l1=params[\"generator_l1_regularization_scale\"],\n",
    "            l2=params[\"generator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        name=\"generator\",\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Instantiate discriminator.\n",
    "    discriminator = discriminators.Discriminator(\n",
    "        input_shape=(\n",
    "            params[\"height\"] * params[\"width\"] * params[\"depth\"]\n",
    "        ),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "            l1=params[\"discriminator_l1_regularization_scale\"],\n",
    "            l2=params[\"discriminator_l2_regularization_scale\"]\n",
    "        ),\n",
    "        bias_regularizer=None,\n",
    "        name=\"discriminator\",\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    return {\"generator\": generator, \"discriminator\": discriminator}\n",
    "\n",
    "\n",
    "def instantiate_optimizer(params, scope):\n",
    "    \"\"\"Instantiates optimizer with parameters.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "        scope: str, the name of the network of interest.\n",
    "\n",
    "    Returns:\n",
    "        Instance of `Optimizer`.\n",
    "    \"\"\"\n",
    "    # Create optimizer map.\n",
    "    optimizers = {\n",
    "        \"Adadelta\": tf.keras.optimizers.Adadelta,\n",
    "        \"Adagrad\": tf.keras.optimizers.Adagrad,\n",
    "        \"Adam\": tf.keras.optimizers.Adam,\n",
    "        \"Adamax\": tf.keras.optimizers.Adamax,\n",
    "        \"Ftrl\": tf.keras.optimizers.Ftrl,\n",
    "        \"Nadam\": tf.keras.optimizers.Nadam,\n",
    "        \"RMSprop\": tf.keras.optimizers.RMSprop,\n",
    "        \"SGD\": tf.keras.optimizers.SGD\n",
    "    }\n",
    "\n",
    "    # Get optimizer and instantiate it.\n",
    "    if params[\"{}_optimizer\".format(scope)] == \"Adam\":\n",
    "        optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "            learning_rate=params[\"{}_learning_rate\".format(scope)],\n",
    "            beta_1=params[\"{}_adam_beta1\".format(scope)],\n",
    "            beta_2=params[\"{}_adam_beta2\".format(scope)],\n",
    "            epsilon=params[\"{}_adam_epsilon\".format(scope)],\n",
    "            name=\"{}_{}_optimizer\".format(\n",
    "                scope, params[\"{}_optimizer\".format(scope)].lower()\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optimizers[params[\"{}_optimizer\".format(scope)]](\n",
    "            learning_rate=params[\"{}_learning_rate\".format(scope)],\n",
    "            name=\"{}_{}_optimizer\".format(\n",
    "                scope, params[\"{}_optimizer\".format(scope)].lower()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def vanilla_gan_model(params):\n",
    "    \"\"\"Vanilla GAN custom Estimator model function.\n",
    "\n",
    "    Args:\n",
    "        params: dict, user passed parameters.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of network objects, dictionary of models objects, and\n",
    "            dictionary of optimizer objects.\n",
    "    \"\"\"\n",
    "    # Instantiate generator and discriminator objects.\n",
    "    network_dict = instantiate_network_objects(params)\n",
    "\n",
    "    # Instantiate generator optimizer.\n",
    "    generator_optimizer = instantiate_optimizer(params, scope=\"generator\")\n",
    "\n",
    "    # Instantiate discriminator optimizer.\n",
    "    discriminator_optimizer = instantiate_optimizer(\n",
    "        params, scope=\"discriminator\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        network_dict,\n",
    "        {\n",
    "            \"generator\": generator_optimizer,\n",
    "            \"discriminator\": discriminator_optimizer\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import input\n",
    "from . import vanilla_gan\n",
    "from . import train\n",
    "\n",
    "\n",
    "def distributed_train_step(\n",
    "    strategy,\n",
    "    global_batch_size,\n",
    "    features,\n",
    "    network_dict,\n",
    "    optimizer_dict,\n",
    "    params,\n",
    "    global_step,\n",
    "    summary_file_writer\n",
    "):\n",
    "    \"\"\"Perform one distributed train step.\n",
    "\n",
    "    Args:\n",
    "        strategy: instance of `tf.distribute.Strategy`.\n",
    "        global_batch_size: int, global batch size for distribution.\n",
    "        features: dict, feature tensors from input function.\n",
    "        network_dict: dict, dictionary of network objects.\n",
    "        optimizer_dict: dict, dictionary of optimizer objects.\n",
    "        params: dict, user passed parameters.\n",
    "        global_step: int, current global step for training.\n",
    "        summary_file_writer: summary file writer.\n",
    "\n",
    "    Returns:\n",
    "        Reduced loss tensor for chosen network across replicas.\n",
    "    \"\"\"\n",
    "    if params[\"tf_version\"] > 2.1:\n",
    "        per_replica_losses = strategy.run(\n",
    "            fn=train.train_step,\n",
    "            kwargs={\n",
    "                \"global_batch_size\": global_batch_size,\n",
    "                \"features\": features,\n",
    "                \"network_dict\": network_dict,\n",
    "                \"optimizer_dict\": optimizer_dict,\n",
    "                \"params\": params,\n",
    "                \"global_step\": global_step,\n",
    "                \"summary_file_writer\": summary_file_writer\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        per_replica_losses = strategy.experimental_run_v2(\n",
    "            fn=train.train_step,\n",
    "            kwargs={\n",
    "                \"global_batch_size\": global_batch_size,\n",
    "                \"features\": features,\n",
    "                \"network_dict\": network_dict,\n",
    "                \"optimizer_dict\": optimizer_dict,\n",
    "                \"params\": params,\n",
    "                \"global_step\": global_step,\n",
    "                \"summary_file_writer\": summary_file_writer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return strategy.reduce(\n",
    "        reduce_op=tf.distribute.ReduceOp.SUM,\n",
    "        value=per_replica_losses,\n",
    "        axis=None\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates Keras model.\n",
    "\n",
    "    Args:\n",
    "        args: dict, user passed parameters.\n",
    "    \"\"\"\n",
    "    # If the list of devices is not specified in the\n",
    "    # `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(\"Number of devices = {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Get input datasets. Batch size will be split evenly between replicas.\n",
    "    train_dataset = input.read_dataset(\n",
    "        filename=args[\"train_file_pattern\"],\n",
    "        batch_size=args[\"train_batch_size\"] * strategy.num_replicas_in_sync,\n",
    "        params=args,\n",
    "        training=True\n",
    "    )()\n",
    "\n",
    "    eval_dataset = input.read_dataset(\n",
    "        filename=args[\"eval_file_pattern\"],\n",
    "        batch_size=args[\"eval_batch_size\"] * strategy.num_replicas_in_sync,\n",
    "        params=args,\n",
    "        training=False\n",
    "    )()\n",
    "    if args[\"eval_steps\"]:\n",
    "        eval_dataset = eval_dataset.take(count=args[\"eval_steps\"])\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Create distributed datasets.\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            dataset=train_dataset\n",
    "        )\n",
    "        eval_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            dataset=eval_dataset\n",
    "        )\n",
    "\n",
    "        # Create iterators of distributed datasets.\n",
    "        train_dist_iter = iter(train_dist_dataset)\n",
    "        eval_dist_iter = iter(eval_dist_dataset)\n",
    "\n",
    "        steps_per_epoch = args[\"train_dataset_length\"] // args[\"train_batch_size\"]\n",
    "\n",
    "        # Instantiate model objects.\n",
    "        network_dict, optimizer_dict = vanilla_gan.vanilla_gan_model(params=args)\n",
    "\n",
    "        # Create checkpoint instance.\n",
    "        checkpoint_dir = os.path.join(args[\"output_dir\"], \"checkpoints\")\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "        checkpoint = tf.train.Checkpoint(\n",
    "            generator_model=network_dict[\"generator\"].get_model(),\n",
    "            discriminator_model=network_dict[\"discriminator\"].get_model(),\n",
    "            generator_optimizer=optimizer_dict[\"generator\"],\n",
    "            discriminator_optimizer=optimizer_dict[\"discriminator\"]\n",
    "        )\n",
    "\n",
    "        # Create checkpoint manager.\n",
    "        checkpoint_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=checkpoint,\n",
    "            directory=checkpoint_dir,\n",
    "            max_to_keep=args[\"keep_checkpoint_max\"]\n",
    "        )\n",
    "\n",
    "        # Restore any prior checkpoints.\n",
    "        status = checkpoint.restore(\n",
    "            save_path=checkpoint_manager.latest_checkpoint\n",
    "        )\n",
    "\n",
    "        # Create summary file writer.\n",
    "        summary_file_writer = tf.summary.create_file_writer(\n",
    "            logdir=os.path.join(args[\"output_dir\"], \"summaries\"),\n",
    "            name=\"summary_file_writer\"\n",
    "        )\n",
    "\n",
    "        # Loop over datasets to perform training.\n",
    "        global_step = 0\n",
    "        for epoch in range(args[\"num_epochs\"]):\n",
    "            for epoch_step in range(steps_per_epoch):\n",
    "                features, labels = next(train_dist_iter)\n",
    "\n",
    "                loss = distributed_train_step(\n",
    "                    strategy=strategy,\n",
    "                    global_batch_size=(\n",
    "                        args[\"train_batch_size\"] * strategy.num_replicas_in_sync\n",
    "                    ),\n",
    "                    features=features,\n",
    "                    network_dict=network_dict,\n",
    "                    optimizer_dict=optimizer_dict,\n",
    "                    params=args,\n",
    "                    global_step=global_step,\n",
    "                    summary_file_writer=summary_file_writer\n",
    "                )\n",
    "\n",
    "                if global_step % args[\"log_step_count_steps\"] == 0:\n",
    "                    print(\n",
    "                        \"epoch = {}, global_step = {}, epoch_step = {}, loss = {}\".format(\n",
    "                            epoch, global_step, epoch_step, loss\n",
    "                        )\n",
    "                    )\n",
    "                global_step += 1\n",
    "\n",
    "            # Checkpoint model every so many steps.\n",
    "            if global_step % args[\"save_checkpoints_steps\"] == 0:\n",
    "                checkpoint_manager.save(checkpoint_number=global_step)\n",
    "\n",
    "        # Write final checkpoint.\n",
    "        checkpoint_manager.save(checkpoint_number=global_step)\n",
    "\n",
    "        # Export SavedModel for serving.\n",
    "        export_path = os.path.join(\n",
    "            args[\"output_dir\"],\n",
    "            \"export\",\n",
    "            datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        )\n",
    "\n",
    "        # Signature will be serving_default.\n",
    "        tf.saved_model.save(\n",
    "            obj=network_dict[\"generator\"].get_model(), export_dir=export_path\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vanilla_gan_module/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "def convert_string_to_bool(string):\n",
    "    \"\"\"Converts string to bool.\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "    Returns:\n",
    "        Boolean conversion of string.\n",
    "    \"\"\"\n",
    "    return False if string.lower() == \"false\" else True\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_float(string):\n",
    "    \"\"\"Converts string to None or float.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or float conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else float(string)\n",
    "\n",
    "\n",
    "def convert_string_to_none_or_int(string):\n",
    "    \"\"\"Converts string to None or int.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "\n",
    "    Returns:\n",
    "        None or int conversion of string.\n",
    "    \"\"\"\n",
    "    return None if string.lower() == \"none\" else int(string)\n",
    "\n",
    "\n",
    "def convert_string_to_list_of_ints(string, sep):\n",
    "    \"\"\"Converts string to list of ints.\n",
    "\n",
    "    Args:\n",
    "        string: str, string to convert.\n",
    "        sep: str, separator string.\n",
    "\n",
    "    Returns:\n",
    "        List of ints conversion of string.\n",
    "    \"\"\"\n",
    "    return [int(x) for x in string.split(sep)]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # File arguments.\n",
    "    parser.add_argument(\n",
    "        \"--train_file_pattern\",\n",
    "        help=\"GCS location to read training data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_file_pattern\",\n",
    "        help=\"GCS location to read evaluation data.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models.\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud.\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "\n",
    "    # Training parameters.\n",
    "    parser.add_argument(\n",
    "        \"--tf_version\",\n",
    "        help=\"Version of TensorFlow\",\n",
    "        type=float,\n",
    "        default=2.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train for.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_dataset_length\",\n",
    "        help=\"Number of examples in one epoch of training set\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Number of examples in training batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_step_count_steps\",\n",
    "        help=\"How many steps to train before writing steps and loss to log.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_summary_steps\",\n",
    "        help=\"How many steps to train before saving a summary.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_checkpoints_steps\",\n",
    "        help=\"How many steps to train before saving a checkpoint.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_checkpoint_max\",\n",
    "        help=\"Max number of checkpoints to keep.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_fn_autotune\",\n",
    "        help=\"Whether to autotune input function performance.\",\n",
    "        type=str,\n",
    "        default=\"True\"\n",
    "    )\n",
    "\n",
    "    # Eval parameters.\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        help=\"Number of examples in evaluation batch.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Number of steps to evaluate for.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "\n",
    "    # Image parameters.\n",
    "    parser.add_argument(\n",
    "        \"--height\",\n",
    "        help=\"Height of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--width\",\n",
    "        help=\"Width of image.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--depth\",\n",
    "        help=\"Depth of image.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "\n",
    "    # Generator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--latent_size\",\n",
    "        help=\"The latent size of the noise vector.\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_hidden_units\",\n",
    "        help=\"Hidden layer sizes to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"2,4,8\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of generator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_final_activation\",\n",
    "        help=\"The final activation function of generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for generator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_optimizer\",\n",
    "        help=\"Name of optimizer to use for generator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for generator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for generator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generator_train_steps\",\n",
    "        help=\"Number of steps to train generator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "\n",
    "    # Discriminator parameters.\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_hidden_units\",\n",
    "        help=\"Hidden layer sizes to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"2,4,8\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_leaky_relu_alpha\",\n",
    "        help=\"The amount of leakyness of discriminator's leaky relus.\",\n",
    "        type=float,\n",
    "        default=0.2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l1_regularization_scale\",\n",
    "        help=\"Scale factor for L1 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_l2_regularization_scale\",\n",
    "        help=\"Scale factor for L2 regularization for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_optimizer\",\n",
    "        help=\"Name of optimizer to use for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"Adam\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_learning_rate\",\n",
    "        help=\"How quickly we train our model by scaling the gradient for discriminator.\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta1\",\n",
    "        help=\"Adam optimizer's beta1 hyperparameter for first moment.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_beta2\",\n",
    "        help=\"Adam optimizer's beta2 hyperparameter for second moment.\",\n",
    "        type=float,\n",
    "        default=0.999\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_adam_epsilon\",\n",
    "        help=\"Adam optimizer's epsilon hyperparameter for numerical stability.\",\n",
    "        type=float,\n",
    "        default=1e-8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_clip_gradients\",\n",
    "        help=\"Global clipping to prevent gradient norm to exceed this value for discriminator.\",\n",
    "        type=str,\n",
    "        default=\"None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--discriminator_train_steps\",\n",
    "        help=\"Number of steps to train discriminator for per cycle.\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--label_smoothing\",\n",
    "        help=\"Multiplier when making real labels instead of all ones.\",\n",
    "        type=float,\n",
    "        default=0.9\n",
    "    )\n",
    "\n",
    "    # Parse all arguments.\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service.\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Fix input_fn_autotune.\n",
    "    arguments[\"input_fn_autotune\"] = convert_string_to_bool(\n",
    "        string=arguments[\"input_fn_autotune\"]\n",
    "    )\n",
    "\n",
    "    # Fix eval steps.\n",
    "    arguments[\"eval_steps\"] = convert_string_to_none_or_int(\n",
    "        string=arguments[\"eval_steps\"])\n",
    "\n",
    "    # Fix hidden_units.\n",
    "    arguments[\"generator_hidden_units\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"generator_hidden_units\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_hidden_units\"] = convert_string_to_list_of_ints(\n",
    "        string=arguments[\"discriminator_hidden_units\"], sep=\",\"\n",
    "    )\n",
    "\n",
    "    # Fix clip_gradients.\n",
    "    arguments[\"generator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"generator_clip_gradients\"]\n",
    "    )\n",
    "\n",
    "    arguments[\"discriminator_clip_gradients\"] = convert_string_to_none_or_float(\n",
    "        string=arguments[\"discriminator_clip_gradients\"]\n",
    "    )\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning.\n",
    "    # This code can be removed if you are not using hyperparameter tuning.\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\n",
    "                \"TF_CONFIG\", \"{}\"\n",
    "            )\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\"))\n",
    "\n",
    "    # Run the training job.\n",
    "    model.train_and_evaluate(arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
